[
  {
    "objectID": "llm-example.html",
    "href": "llm-example.html",
    "title": "Automotive Efficiency Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(haven)\n\n# Load the auto dataset (assuming Stata format)\n# If you have a different source, adjust accordingly\nauto &lt;- read_dta(\"http://www.stata-press.com/data/r15/auto.dta\")\n\n# Split 'make' into 'manufacturer' and 'model'\nauto_processed &lt;- auto %&gt;%\n  mutate(\n    manufacturer = str_extract(make, \"^\\\\S+\"),\n    model = str_trim(str_remove(make, \"^\\\\S+\"))\n  ) %&gt;%\n  # Categorize weight into two categories (lighter and heavier)\n  mutate(\n    weight_category = cut(\n      weight,\n      breaks = quantile(weight, probs = c(0, 0.5, 1)),\n      labels = c(\"Lighter Vehicles\", \"Heavier Vehicles\"),\n      include.lowest = TRUE\n    )\n  )\n\n\n# Define colorblind-friendly palette (Wong palette) for all visualizations\naccessible_colors &lt;- c(\n  regular = \"#56B4E9\",      # Sky blue\n  annotated = \"#E69F00\"     # Orange\n)"
  },
  {
    "objectID": "llm-example.html#data-preparation",
    "href": "llm-example.html#data-preparation",
    "title": "Automotive Efficiency Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(haven)\n\n# Load the auto dataset (assuming Stata format)\n# If you have a different source, adjust accordingly\nauto &lt;- read_dta(\"http://www.stata-press.com/data/r15/auto.dta\")\n\n# Split 'make' into 'manufacturer' and 'model'\nauto_processed &lt;- auto %&gt;%\n  mutate(\n    manufacturer = str_extract(make, \"^\\\\S+\"),\n    model = str_trim(str_remove(make, \"^\\\\S+\"))\n  ) %&gt;%\n  # Categorize weight into two categories (lighter and heavier)\n  mutate(\n    weight_category = cut(\n      weight,\n      breaks = quantile(weight, probs = c(0, 0.5, 1)),\n      labels = c(\"Lighter Vehicles\", \"Heavier Vehicles\"),\n      include.lowest = TRUE\n    )\n  )\n\n\n# Define colorblind-friendly palette (Wong palette) for all visualizations\naccessible_colors &lt;- c(\n  regular = \"#56B4E9\",      # Sky blue\n  annotated = \"#E69F00\"     # Orange\n)"
  },
  {
    "objectID": "llm-example.html#understanding-spread-and-location",
    "href": "llm-example.html#understanding-spread-and-location",
    "title": "Automotive Efficiency Analysis",
    "section": "Understanding Spread and Location",
    "text": "Understanding Spread and Location\nUnderstanding both the location (where the center of your data sits) and spread (how much variability exists) is crucial because two datasets can have identical averages but tell completely different stories. For example, if two car models both average 25 mpg, but one ranges from 24-26 mpg while the other ranges from 15-35 mpg, that difference in consistency matters greatly for real-world decision making—the first is predictable while the second is unreliable.\n\n# Example 1: Comparing MPG spread across weight categories\nmpg_comparison &lt;- auto_processed %&gt;%\n  group_by(weight_category) %&gt;%\n  summarise(\n    n = n(),\n    mean_mpg = round(mean(mpg), 1),\n    sd_mpg = round(sd(mpg), 1),\n    min_mpg = min(mpg),\n    max_mpg = max(mpg),\n    range_mpg = max_mpg - min_mpg\n  )\n\nknitr::kable(mpg_comparison,\n             col.names = c(\"Weight Category\", \"Count\", \"Mean MPG\", \"SD\", \"Min\", \"Max\", \"Range\"),\n             caption = \"Location (mean) and Spread (SD, range) of MPG by Weight Category\")\n\n\nLocation (mean) and Spread (SD, range) of MPG by Weight Category\n\n\nWeight Category\nCount\nMean MPG\nSD\nMin\nMax\nRange\n\n\n\n\nLighter Vehicles\n37\n25.1\n5.3\n17\n41\n24\n\n\nHeavier Vehicles\n37\n17.5\n3.1\n12\n28\n16\n\n\n\n\n# Example 2: Price variability by manufacturer\nprice_by_mfr &lt;- auto_processed %&gt;%\n  group_by(manufacturer) %&gt;%\n  filter(n() &gt;= 3) %&gt;%  # Only manufacturers with 3+ cars\n  summarise(\n    n = n(),\n    mean_price = round(mean(price), 0),\n    sd_price = round(sd(price), 0)\n  ) %&gt;%\n  arrange(desc(mean_price)) %&gt;%\n  head(8)\n\nknitr::kable(price_by_mfr,\n             col.names = c(\"Manufacturer\", \"Count\", \"Mean Price ($)\", \"SD Price ($)\"),\n             caption = \"Price variability: Some manufacturers (high SD) offer diverse model ranges, others (low SD) target specific market segments\")\n\n\nPrice variability: Some manufacturers (high SD) offer diverse model ranges, others (low SD) target specific market segments\n\n\nManufacturer\nCount\nMean Price (\\()| SD Price (\\))\n\n\n\n\n\nCad.\n3\n13930\n2314\n\n\nLinc.\n3\n12852\n1175\n\n\nBuick\n7\n6075\n2258\n\n\nOlds\n7\n6051\n2486\n\n\nVW\n4\n6021\n1166\n\n\nDatsun\n4\n6006\n1573\n\n\nToyota\n3\n5122\n1193\n\n\nDodge\n4\n5056\n1236\n\n\n\n\n\nNotice how lighter vehicles have both higher average efficiency (location) AND greater variability (spread). Similarly, some manufacturers show high price variability, indicating they serve multiple market segments, while others cluster tightly around a specific price point."
  },
  {
    "objectID": "llm-example.html#exploratory-data-analysis-examining-distributions",
    "href": "llm-example.html#exploratory-data-analysis-examining-distributions",
    "title": "Automotive Efficiency Analysis",
    "section": "Exploratory Data Analysis: Examining Distributions",
    "text": "Exploratory Data Analysis: Examining Distributions\nBefore diving into the main analysis, let’s examine the distributions of our key variables using standard statistical visualizations.\n\nDistribution Plots\n\n# Create histograms with density overlays for key variables\np1 &lt;- ggplot(auto_processed, aes(x = mpg)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 15, fill = accessible_colors[\"regular\"], alpha = 0.7) +\n  geom_density(color = accessible_colors[\"annotated\"], linewidth = 1.2) +\n  labs(title = \"Distribution of Fuel Efficiency (MPG)\",\n       x = \"Miles Per Gallon\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(auto_processed, aes(x = weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 15, fill = accessible_colors[\"regular\"], alpha = 0.7) +\n  geom_density(color = accessible_colors[\"annotated\"], linewidth = 1.2) +\n  labs(title = \"Distribution of Vehicle Weight\",\n       x = \"Weight (lbs)\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np3 &lt;- ggplot(auto_processed, aes(x = price)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 15, fill = accessible_colors[\"regular\"], alpha = 0.7) +\n  geom_density(color = accessible_colors[\"annotated\"], linewidth = 1.2) +\n  labs(title = \"Distribution of Vehicle Price\",\n       x = \"Price ($)\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine plots\nlibrary(patchwork)\np1 / p2 / p3\n\n\n\n\n\n\n\n\nWhat this tells us:\n\nMPG distribution appears somewhat bimodal (two peaks), suggesting distinct groups of vehicles with different efficiency characteristics—likely reflecting the lighter vs. heavier vehicle split\nWeight distribution shows relatively even spread with slight right skew (more very heavy vehicles than very light ones)\nPrice distribution is right-skewed with a long tail, indicating most cars cluster at lower price points with a few expensive outliers\n\n\n\nBox Plots: Comparing Distributions Across Categories\n\nb1 &lt;- ggplot(auto_processed, aes(x = weight_category, y = mpg, fill = weight_category)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_manual(values = c(accessible_colors[\"regular\"], accessible_colors[\"annotated\"])) +\n  labs(title = \"MPG by Weight Category\",\n       x = \"Weight Category\", y = \"Miles Per Gallon\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"),\n        legend.position = \"none\")\n\nb2 &lt;- ggplot(auto_processed, aes(x = weight_category, y = price, fill = weight_category)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_manual(values = c(accessible_colors[\"regular\"], accessible_colors[\"annotated\"])) +\n  labs(title = \"Price by Weight Category\",\n       x = \"Weight Category\", y = \"Price ($)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"),\n        legend.position = \"none\")\n\nb1 + b2\n\n\n\n\n\n\n\n\nWhat this tells us:\n\nMPG box plots show clear separation: lighter vehicles have substantially higher median efficiency and wider spread (more variability in performance)\nPrice box plots reveal heavier vehicles cost more on average, but both categories have outliers representing luxury or specialty models\n\n\n\nCorrelation Analysis\n\n# Calculate correlation matrix\ncor_matrix &lt;- auto_processed %&gt;%\n  select(mpg, weight, price) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  round(3)\n\nknitr::kable(cor_matrix,\n             caption = \"Correlation Matrix: MPG, Weight, and Price\")\n\n\nCorrelation Matrix: MPG, Weight, and Price\n\n\n\nmpg\nweight\nprice\n\n\n\n\nmpg\n1.000\n-0.807\n-0.469\n\n\nweight\n-0.807\n1.000\n0.539\n\n\nprice\n-0.469\n0.539\n1.000\n\n\n\n\n\n\n# Create individual scatterplots for key relationships\ns1 &lt;- ggplot(auto_processed, aes(x = weight, y = mpg)) +\n  geom_point(alpha = 0.6, size = 2.5, color = accessible_colors[\"regular\"]) +\n  geom_smooth(method = \"lm\", se = TRUE, color = accessible_colors[\"annotated\"], linewidth = 1.2) +\n  labs(title = \"MPG vs Weight\",\n       subtitle = paste(\"Correlation:\", round(cor(auto_processed$weight, auto_processed$mpg), 3)),\n       x = \"Weight (lbs)\", y = \"MPG\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\ns2 &lt;- ggplot(auto_processed, aes(x = price, y = mpg)) +\n  geom_point(alpha = 0.6, size = 2.5, color = accessible_colors[\"regular\"]) +\n  geom_smooth(method = \"lm\", se = TRUE, color = accessible_colors[\"annotated\"], linewidth = 1.2) +\n  labs(title = \"MPG vs Price\",\n       subtitle = paste(\"Correlation:\", round(cor(auto_processed$price, auto_processed$mpg), 3)),\n       x = \"Price ($)\", y = \"MPG\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\ns3 &lt;- ggplot(auto_processed, aes(x = weight, y = price)) +\n  geom_point(alpha = 0.6, size = 2.5, color = accessible_colors[\"regular\"]) +\n  geom_smooth(method = \"lm\", se = TRUE, color = accessible_colors[\"annotated\"], linewidth = 1.2) +\n  labs(title = \"Price vs Weight\",\n       subtitle = paste(\"Correlation:\", round(cor(auto_processed$weight, auto_processed$price), 3)),\n       x = \"Weight (lbs)\", y = \"Price ($)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\nlibrary(patchwork)\ns1 / s2 / s3\n\n\n\n\n\n\n\n\nWhat this tells us:\n\nStrong negative correlation between weight and MPG (r ≈ -0.80): heavier vehicles consistently get worse fuel economy\nModerate negative correlation between price and MPG: more expensive vehicles tend to be less efficient (likely because luxury/performance vehicles prioritize power over efficiency)\nModerate positive correlation between weight and price: heavier vehicles cost more, reflecting size, materials, and features\n\n\n\nQ-Q Plot: Checking Normality\n\nq1 &lt;- ggplot(auto_processed, aes(sample = mpg)) +\n  stat_qq(color = accessible_colors[\"regular\"], size = 2) +\n  stat_qq_line(color = accessible_colors[\"annotated\"], linewidth = 1) +\n  labs(title = \"Q-Q Plot: MPG\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\nq2 &lt;- ggplot(auto_processed, aes(sample = weight)) +\n  stat_qq(color = accessible_colors[\"regular\"], size = 2) +\n  stat_qq_line(color = accessible_colors[\"annotated\"], linewidth = 1) +\n  labs(title = \"Q-Q Plot: Weight\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\nq3 &lt;- ggplot(auto_processed, aes(sample = price)) +\n  stat_qq(color = accessible_colors[\"regular\"], size = 2) +\n  stat_qq_line(color = accessible_colors[\"annotated\"], linewidth = 1) +\n  labs(title = \"Q-Q Plot: Price\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\nq1 + q2 + q3\n\n\n\n\n\n\n\n\nWhat this tells us:\n\nMPG: Reasonably normal with some deviation at extremes (high-efficiency outliers)\nWeight: Close to normal distribution\nPrice: Clear departure from normality at upper tail—those luxury outliers create right skew, meaning parametric tests assuming normality should be used cautiously with price data\n\n\n\nImplications for Analysis\nThese exploratory plots reveal several important insights:\n\nNatural groupings exist: The bimodal MPG distribution and clear separation in box plots justify our decision to analyze lighter and heavier vehicles separately\nStrong weight-efficiency relationship: The correlation analysis confirms weight is the dominant factor affecting fuel efficiency, making it the primary focus of our analysis\nNon-linear effects likely: The scatterplots show the relationship between variables may not be perfectly linear, suggesting different efficiency-weight trade-offs in different vehicle classes\nOutliers matter: Several vehicles deviate substantially from patterns, representing either engineering achievements (high-efficiency outliers) or specialized markets (luxury price outliers)"
  },
  {
    "objectID": "llm-example.html#efficiency-vs-weight-analysis",
    "href": "llm-example.html#efficiency-vs-weight-analysis",
    "title": "Automotive Efficiency Analysis",
    "section": "Efficiency vs Weight Analysis",
    "text": "Efficiency vs Weight Analysis\n\n# Identify cars to annotate in each category\nannotations &lt;- auto_processed %&gt;%\n  group_by(weight_category) %&gt;%\n  arrange(mpg) %&gt;%\n  mutate(\n    rank_mpg = row_number(),\n    n = n()\n  ) %&gt;%\n  filter(\n    rank_mpg == 1 |                    # least efficient\n    rank_mpg == round(n/2) |           # average (median position)\n    rank_mpg == n                      # most efficient\n  ) %&gt;%\n  mutate(\n    label = paste0(manufacturer, \" \", model, \"\\n\", mpg, \" mpg\")\n  ) %&gt;%\n  ungroup()\n\n\nLighter Vehicles\n\n# Filter data for lighter vehicles\nlighter_data &lt;- auto_processed %&gt;%\n  filter(weight_category == \"Lighter Vehicles\")\n\nlighter_annotations &lt;- annotations %&gt;%\n  filter(weight_category == \"Lighter Vehicles\")\n\n# Create plot for lighter vehicles\nggplot(lighter_data, aes(x = weight, y = mpg)) +\n  geom_point(alpha = 0.7, size = 3.5, color = accessible_colors[\"regular\"]) +\n  geom_point(\n    data = lighter_annotations,\n    aes(x = weight, y = mpg),\n    color = accessible_colors[\"annotated\"],\n    size = 6,\n    shape = 18\n  ) +\n  geom_text(\n    data = lighter_annotations,\n    aes(x = weight, y = mpg, label = label),\n    hjust = -0.1,\n    vjust = 0,\n    size = 4,\n    color = \"#000000\",\n    fontface = \"bold\",\n    check_overlap = FALSE\n  ) +\n  labs(\n    title = \"Fuel Efficiency vs Weight: Lighter Vehicles\",\n    x = \"Weight (lbs)\",\n    y = \"Efficiency (MPG)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nHeavier Vehicles\n\n# Filter data for heavier vehicles\nheavier_data &lt;- auto_processed %&gt;%\n  filter(weight_category == \"Heavier Vehicles\")\n\nheavier_annotations &lt;- annotations %&gt;%\n  filter(weight_category == \"Heavier Vehicles\")\n\n# Create plot for heavier vehicles\nggplot(heavier_data, aes(x = weight, y = mpg)) +\n  geom_point(alpha = 0.7, size = 3.5, color = accessible_colors[\"regular\"]) +\n  geom_point(\n    data = heavier_annotations,\n    aes(x = weight, y = mpg),\n    color = accessible_colors[\"annotated\"],\n    size = 6,\n    shape = 18\n  ) +\n  geom_text(\n    data = heavier_annotations,\n    aes(x = weight, y = mpg, label = label),\n    hjust = -0.1,\n    vjust = 0,\n    size = 4,\n    color = \"#000000\",\n    fontface = \"bold\",\n    check_overlap = FALSE\n  ) +\n  labs(\n    title = \"Fuel Efficiency vs Weight: Heavier Vehicles\",\n    x = \"Weight (lbs)\",\n    y = \"Efficiency (MPG)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )"
  },
  {
    "objectID": "llm-example.html#summary",
    "href": "llm-example.html#summary",
    "title": "Automotive Efficiency Analysis",
    "section": "Summary",
    "text": "Summary\nThis analysis categorizes vehicles into two weight classes (Lighter and Heavier Vehicles) based on the median weight and examines the relationship between weight and fuel efficiency. The visualization uses a colorblind-friendly palette (Wong palette) with distinct shapes for enhanced accessibility. Each plot highlights:\n\nThe least efficient vehicle (lowest MPG)\nA representative average vehicle (median MPG)\nThe most efficient vehicle (highest MPG)\n\n\n# Summary statistics by weight category\nauto_processed %&gt;%\n  group_by(weight_category) %&gt;%\n  summarise(\n    n = n(),\n    avg_mpg = round(mean(mpg), 1),\n    min_mpg = min(mpg),\n    max_mpg = max(mpg),\n    avg_weight = round(mean(weight), 0)\n  ) %&gt;%\n  knitr::kable(col.names = c(\"Weight Category\", \"Count\", \"Avg MPG\", \"Min MPG\", \"Max MPG\", \"Avg Weight\"))\n\n\n\n\nWeight Category\nCount\nAvg MPG\nMin MPG\nMax MPG\nAvg Weight\n\n\n\n\nLighter Vehicles\n37\n25.1\n17\n41\n2359\n\n\nHeavier Vehicles\n37\n17.5\n12\n28\n3680"
  },
  {
    "objectID": "llm-example.html#addressing-non-normality-power-transformations-on-price",
    "href": "llm-example.html#addressing-non-normality-power-transformations-on-price",
    "title": "Automotive Efficiency Analysis",
    "section": "Addressing Non-Normality: Power Transformations on Price",
    "text": "Addressing Non-Normality: Power Transformations on Price\nAs we saw in the Q-Q plots, price deviates substantially from a normal distribution due to its right skew. Power transformations can help reshape skewed distributions to be more symmetric and closer to normal.\n\nWhat Are Power Transformations?\nPower transformations are part of the Box-Cox family and follow the form:\n\nλ = 2: Square (y²) - increases right skew, rarely useful for already skewed data\nλ = 1: No transformation (y) - original data\nλ = 0.5: Square root (√y) - mild transformation, reduces moderate skew\nλ = 0: Natural log (ln(y)) - moderate transformation, commonly used for right-skewed data\nλ = -0.5: Inverse square root (1/√y) - stronger transformation\nλ = -1: Inverse (1/y) - strongest transformation, reverses order\n\nThe goal is to find a transformation that makes the distribution more symmetric and approximately normal, which can improve the performance of statistical methods that assume normality.\n\n# Create transformed versions of price\nauto_transformed &lt;- auto_processed %&gt;%\n  mutate(\n    price_squared = price^2,\n    price_original = price,\n    price_sqrt = sqrt(price),\n    price_log = log(price),\n    price_inv_sqrt = 1/sqrt(price),\n    price_inverse = 1/price\n  )\n\n# Reshape data for easier plotting\nprice_long &lt;- auto_transformed %&gt;%\n  select(starts_with(\"price_\")) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"transformation\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    transformation = factor(\n      transformation,\n      levels = c(\"price_squared\", \"price_original\", \"price_sqrt\",\n                 \"price_log\", \"price_inv_sqrt\", \"price_inverse\"),\n      labels = c(\"λ = 2 (Square)\", \"λ = 1 (Original)\", \"λ = 0.5 (Square Root)\",\n                 \"λ = 0 (Log)\", \"λ = -0.5 (Inverse √)\", \"λ = -1 (Inverse)\")\n    )\n  )\n\n\n\nComparing Distributions Across Transformations\n\n# Create histograms for each transformation\nggplot(price_long, aes(x = value)) +\n  geom_histogram(bins = 20, fill = accessible_colors[\"regular\"], alpha = 0.7, color = \"white\") +\n  geom_density(aes(y = after_stat(density) * (max(..count..) / max(..density..))),\n               color = accessible_colors[\"annotated\"], linewidth = 1.2) +\n  facet_wrap(~transformation, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Price Distributions Under Different Power Transformations\",\n    x = \"Transformed Price Value\",\n    y = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    strip.text = element_text(size = 12, face = \"bold\"),\n    strip.background = element_rect(fill = \"#F0F0F0\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\nQ-Q Plots: Assessing Normality of Transformations\n\n# Create Q-Q plots for each transformation\nggplot(price_long, aes(sample = value)) +\n  stat_qq(color = accessible_colors[\"regular\"], size = 2) +\n  stat_qq_line(color = accessible_colors[\"annotated\"], linewidth = 1) +\n  facet_wrap(~transformation, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Q-Q Plots: Assessing Normality of Transformed Price Data\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    strip.text = element_text(size = 12, face = \"bold\"),\n    strip.background = element_rect(fill = \"#F0F0F0\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\nStatistical Assessment: Which Transformation is Best?\nTo objectively determine the “best” transformation, we need multiple metrics. A transformation is closer to normal when:\n\nShapiro-Wilk p-value is higher (ideally &gt; 0.05, meaning we cannot reject normality)\nSkewness is closer to 0 (perfectly symmetric distribution)\nKurtosis is closer to 3 (normal distribution has kurtosis = 3)\nShapiro-Wilk W statistic is closer to 1 (perfect normality = 1)\n\n\n# Calculate comprehensive normality metrics for each transformation\nnormality_metrics &lt;- price_long %&gt;%\n  group_by(transformation) %&gt;%\n  summarise(\n    n = n(),\n    mean = round(mean(value), 2),\n    sd = round(sd(value), 2),\n    skewness = round(moments::skewness(value), 3),\n    kurtosis = round(moments::kurtosis(value), 3),\n    shapiro_W = round(shapiro.test(value)$statistic, 4),\n    shapiro_p = round(shapiro.test(value)$p.value, 4)\n  ) %&gt;%\n  mutate(\n    # Calculate deviation from ideal normality metrics\n    skew_deviation = abs(skewness - 0),  # Ideal = 0\n    kurtosis_deviation = abs(kurtosis - 3),  # Ideal = 3\n    W_deviation = abs(1 - shapiro_W),  # Ideal = 1\n    # Overall normality score (lower is better)\n    normality_score = skew_deviation + kurtosis_deviation + W_deviation\n  ) %&gt;%\n  arrange(normality_score)\n\n# Display full comparison\nknitr::kable(\n  normality_metrics %&gt;% select(-skew_deviation, -kurtosis_deviation, -W_deviation),\n  digits = 4,\n  col.names = c(\"Transformation\", \"N\", \"Mean\", \"SD\", \"Skewness\", \"Kurtosis\", \"W Stat\", \"p-value\", \"Score\"),\n  caption = \"Normality Metrics for All Transformations (lower score = more normal)\"\n)\n\n\nNormality Metrics for All Transformations (lower score = more normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformation\nN\nMean\nSD\nSkewness\nKurtosis\nW Stat\np-value\nScore\n\n\n\n\nλ = -1 (Inverse)\n74\n0.00\n0.00\n-0.456\n2.345\n0.9527\n0.0076\n1.1583\n\n\nλ = -0.5 (Inverse √)\n74\n0.01\n0.00\n-0.760\n2.647\n0.9227\n0.0002\n1.1903\n\n\nλ = 0 (Log)\n74\n8.64\n0.39\n1.063\n3.162\n0.8794\n0.0000\n1.3456\n\n\nλ = 0.5 (Square Root)\n74\n76.75\n16.68\n1.362\n3.886\n0.8260\n0.0000\n2.4220\n\n\nλ = 1 (Original)\n74\n6165.26\n2949.50\n1.653\n4.819\n0.7670\n0.0000\n3.7050\n\n\nλ = 2 (Square)\n74\n46592355.69\n51448974.19\n2.224\n7.347\n0.6490\n0.0000\n6.9220\n\n\n\n\n\nReading the table:\n\nSkewness: Values near 0 indicate symmetry. Negative = left-skewed, Positive = right-skewed\nKurtosis: Values near 3 indicate normal “tailedness”. &lt;3 = thin tails, &gt;3 = heavy tails\nShapiro W: Values near 1 indicate normality\np-value: Values &gt;0.05 suggest we cannot reject normality (good!)\nScore: Combined deviation from ideal normality (lower is better)\n\n\n# Visualize the comparison\nmetric_comparison &lt;- normality_metrics %&gt;%\n  select(transformation, skewness, kurtosis, shapiro_W, shapiro_p) %&gt;%\n  pivot_longer(\n    cols = -transformation,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    metric = factor(metric,\n                   levels = c(\"skewness\", \"kurtosis\", \"shapiro_W\", \"shapiro_p\"),\n                   labels = c(\"Skewness\\n(target: 0)\",\n                            \"Kurtosis\\n(target: 3)\",\n                            \"Shapiro W\\n(target: 1)\",\n                            \"p-value\\n(target: &gt;0.05)\"))\n  )\n\nggplot(metric_comparison, aes(x = transformation, y = value, fill = transformation)) +\n  geom_col(alpha = 0.7) +\n  geom_hline(data = data.frame(\n    metric = factor(c(\"Skewness\\n(target: 0)\", \"Kurtosis\\n(target: 3)\",\n                     \"Shapiro W\\n(target: 1)\", \"p-value\\n(target: &gt;0.05)\"),\n                   levels = c(\"Skewness\\n(target: 0)\", \"Kurtosis\\n(target: 3)\",\n                            \"Shapiro W\\n(target: 1)\", \"p-value\\n(target: &gt;0.05)\")),\n    target = c(0, 3, 1, 0.05)\n  ), aes(yintercept = target),\n  linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  facet_wrap(~metric, scales = \"free_y\", nrow = 1) +\n  scale_fill_manual(values = rep(c(accessible_colors[\"regular\"], accessible_colors[\"annotated\"]), 3)) +\n  labs(\n    title = \"Comparison of Normality Metrics Across Transformations\",\n    subtitle = \"Red dashed line shows ideal value for normality\",\n    x = \"Transformation\",\n    y = \"Metric Value\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.text.x = element_blank(),\n    legend.position = \"bottom\",\n    strip.text = element_text(face = \"bold\")\n  ) +\n  guides(fill = guide_legend(title = \"Transformation\", nrow = 2))\n\n\n\n\n\n\n\n\n\n# Identify the winner\nbest_transformation &lt;- normality_metrics %&gt;%\n  slice(1) %&gt;%\n  pull(transformation)\n\nbest_metrics &lt;- normality_metrics %&gt;%\n  slice(1)\n\n\n\nThe Verdict\nBased on the combined normality score, the λ = -1 (Inverse) performs best. Here’s why:\n\nSkewness: -0.456 (very close to 0 = symmetric)\nKurtosis: 2.345 (close to 3 = normal tailedness)\nShapiro W: 0.9527 (close to 1 = normal)\np-value: 0.0076 (Still rejects normality, but best available)\n\nThis transformation minimizes the total deviation from ideal normality characteristics across all metrics.\n\n\nWhy This Matters\nThe objective comparison above shows which transformation best normalizes the data using multiple statistical criteria, not just visual inspection. The transformation with the lowest normality score achieves the best balance across:\n\nSymmetry (skewness near 0)\nAppropriate tail behavior (kurtosis near 3)\nOverall normality (Shapiro-Wilk W near 1, p-value high)\n\nPractical Implications:\n\nFor parametric statistical tests (t-tests, ANOVA, linear regression): Use the winning transformation to meet normality assumptions\nFor interpretation: The winning transformation also has practical meaning:\n\nLog transformation: Changes represent proportional/percentage changes rather than absolute changes\nSquare root: Reduces influence of extreme values while maintaining order\nInverse: Emphasizes lower values, de-emphasizes higher values\n\nTrade-offs: Sometimes the “most normal” transformation may not be the most interpretable. Consider both statistical properties and practical meaning when choosing a transformation for analysis.\n\nWhen to skip transformation:\n\nIf using non-parametric tests (no normality assumption needed)\nIf sample size is very large (Central Limit Theorem helps)\nIf the non-normality doesn’t substantially affect your conclusions"
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html",
    "href": "examples/prams_2_ggplot_concepts.html",
    "title": "ggplot2 Fundamentals Example",
    "section": "",
    "text": "# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"ggplot2\", \"forcats\", \"kableExtra\", \"readr\", \"ggtext\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(ggtext)  # Add ggtext for markdown support"
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html#research-question",
    "href": "examples/prams_2_ggplot_concepts.html#research-question",
    "title": "ggplot2 Fundamentals Example",
    "section": "Research Question",
    "text": "Research Question\nWe had substantial missing data on depression/anxiety questions. For this reason, let’s focus on the relationship between location and binge drinking."
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html#workflow-with-ggplot2",
    "href": "examples/prams_2_ggplot_concepts.html#workflow-with-ggplot2",
    "title": "ggplot2 Fundamentals Example",
    "section": "Workflow with ggplot2:",
    "text": "Workflow with ggplot2:\n\nStart with data\nPick an aesthetic mapping\nChoose a geometric object\nAdd statistical transformations\nAdjust finer details: scales, coordinate systems, faceting, etc.\n\nBuild this up, layer by layer."
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html#step-1-data",
    "href": "examples/prams_2_ggplot_concepts.html#step-1-data",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 1: Data",
    "text": "Step 1: Data\nLet’s load the data.\n\ndf_final &lt;- readRDS(here::here(\"data\", \"processed\", \"cdc_prams_df_final.rds\"))\n\ndf_final |&gt; \n  glimpse()\n\nRows: 1,222\nColumns: 8\n$ location_abbr                        &lt;fct&gt; AR, AR, AR, AR, AR, AR, AR, AR, A…\n$ subgroup_cat                         &lt;chr&gt; \"Adequacy of Prenatal care\", \"Ade…\n$ subgroup                             &lt;chr&gt; \"ADEQUATE PNC\", \"INADEQUATE PNC\",…\n$ depression_within_3_months_birth     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ anxiety_within_3_months_birth        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ binge_drinking_within_3_months_birth &lt;dbl&gt; 24.6, 12.6, 22.2, 36.0, 21.5, 24.…\n$ alcohol_use_within_3_months_birth    &lt;dbl&gt; 53.5, 28.7, 40.9, 54.1, 45.8, 49.…\n$ location                             &lt;fct&gt; Arkansas, Arkansas, Arkansas, Ark…\n\n\nLet’s filter out missing data now to avoid warnings later.\n\ndf_binge_location &lt;- df_final |&gt;\n  filter(!is.na(binge_drinking_within_3_months_birth)) |&gt; \n  select(location_abbr, subgroup_cat, subgroup, location, binge_drinking_within_3_months_birth)\n\nThis isn’t actually enough. Remember, the data has sub groupings:\n\ndf_binge_location |&gt;\n  select(subgroup_cat, subgroup) |&gt;\n  distinct() |&gt; \n  kable()\n\n\n\n\nsubgroup_cat\nsubgroup\n\n\n\n\nAdequacy of Prenatal care\nADEQUATE PNC\n\n\nAdequacy of Prenatal care\nINADEQUATE PNC\n\n\nAdequacy of Prenatal care\nINTERMEDIATE PNC\n\n\nAdequacy of Prenatal care\nUNKNOWN PNC\n\n\nBirth Weight\nLBW (&lt;=2500g)\n\n\nBirth Weight\nNBW (&gt;2500g)\n\n\nIncome (years 2004 and beyond)\n$10,000 to $24,999\n\n\nIncome (years 2004 and beyond)\n$25,000 to $49,999\n\n\nIncome (years 2004 and beyond)\n$50,000 or more\n\n\nIncome (years 2004 and beyond)\nLess than $10,000\n\n\nMarital Status\nMARRIED\n\n\nMarital Status\nOTHER\n\n\nMaternal Age (3 Levels)\n20-29 yrs\n\n\nMaternal Age (3 Levels)\n30+ yrs\n\n\nMaternal Age (3 Levels)\n&lt;20 yrs\n\n\nMaternal Age (4 Levels)\n20-24 yrs\n\n\nMaternal Age (4 Levels)\n25-34 yrs\n\n\nMaternal Age (4 Levels)\n35+ yrs\n\n\nMaternal Age (4 Levels)\n&lt;20 yrs\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 18 - 24\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 25 - 29\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 30 - 44\n\n\nMaternal Age - 18 to 44 years in groupings\nAge &lt; 18\n\n\nMaternal Age - 18 to 44 years only\nAge 18 - 44\n\n\nMaternal Education\n12 yrs\n\n\nMaternal Education\n&lt; 12 yrs\n\n\nMaternal Education\n&gt;12 yrs\n\n\nMaternal Race/Ethnicity\nBlack, non-Hispanic\n\n\nMaternal Race/Ethnicity\nHispanic\n\n\nMaternal Race/Ethnicity\nWhite, non-Hispanic\n\n\nMedicaid Recipient\nMedicaid\n\n\nMedicaid Recipient\nNon-Medicaid\n\n\nMother Hispanic\nHispanic\n\n\nMother Hispanic\nNon-Hispanic\n\n\nNone\nNone\n\n\nNumber of Previous Live Births\n0\n\n\nNumber of Previous Live Births\n1 or more\n\n\nOn WIC during Pregnancy\nNon-WIC\n\n\nOn WIC during Pregnancy\nWIC\n\n\nPregnancy Intendedness\nIntended\n\n\nPregnancy Intendedness\nUnintended\n\n\nSmoked 3 months before Pregnancy\nNon-Smoker\n\n\nSmoked 3 months before Pregnancy\nSmoker\n\n\nSmoked last 3 months of Pregnancy\nNon-Smoker\n\n\nSmoked last 3 months of Pregnancy\nSmoker\n\n\nMaternal Race/Ethnicity\nOther non-Hispanic\n\n\n\n\n\nFor now, let’s just filter on the “None” subgroup.\n\ndf_binge_location &lt;- df_binge_location |&gt;\n  filter(subgroup_cat == \"None\")"
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html#step-2.-aesthetic-mappings",
    "href": "examples/prams_2_ggplot_concepts.html#step-2.-aesthetic-mappings",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 2. Aesthetic Mappings",
    "text": "Step 2. Aesthetic Mappings\n\nWhat is your X (independent) variable?\nWhat is your Y (dependent) variable?\n\nHere:\n\nx = location\ny = binge_drinking_within_3_months_birth\n\n\np1 &lt;- df_binge_location |&gt;\n    ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) \n\np1\n\n\n\n\n\n\n\n\nWe can see already we’re likely going to want to do a coordinate flip, but let’s save that for later.\nNote how we save our plot as an object, p1.\nIt’s often nice to build plots up, step by step, with p{n} where n is the step number."
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html#step-3.-geometric-object",
    "href": "examples/prams_2_ggplot_concepts.html#step-3.-geometric-object",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 3. Geometric Object",
    "text": "Step 3. Geometric Object\n\nOur x is a categorical variable\nOur y is a continuous variable\n\nIt makes sense to use a bar chart.\n\np2 &lt;- p1 + geom_bar(stat = \"identity\")\n\np2"
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html#step-4.-statistical-transformations",
    "href": "examples/prams_2_ggplot_concepts.html#step-4.-statistical-transformations",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 4. Statistical Transformations",
    "text": "Step 4. Statistical Transformations\n\nThe public 2011 PRAMS data we are using is aggregated at the location level.\nFor this reason, we don’t need to do any statistical transformations.\n\nYou can imagine data where we have data at the individual level, and we want to aggregate it to the location level, we’d need to use an aggregation function. I’ll show this below with simulated data."
  },
  {
    "objectID": "examples/prams_2_ggplot_concepts.html#step-5.-adjust-fine-details",
    "href": "examples/prams_2_ggplot_concepts.html#step-5.-adjust-fine-details",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 5. Adjust Fine Details",
    "text": "Step 5. Adjust Fine Details\n\nCoordinate Flip\nFirst, let’s flip the coordinates.\n\np3 &lt;- p2 + coord_flip()\n\np3\n\n\n\n\n\n\n\n\nThat looks a lot better.\n\n\nFixing Labels\nRight now, the labels are reverse alphabetical. Let’s fix that.\n\np4 &lt;- p3 + \n  scale_x_discrete(limits = rev(levels(df_binge_location$location)))\n\np4\n\n\n\n\n\n\n\n\nWe might also consider sorting by the rate of binge drinking.\n\np5 &lt;- df_binge_location |&gt;\n  mutate(location = fct_reorder(location, binge_drinking_within_3_months_birth)) |&gt;\n  ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()\n\np5\n\n\n\n\n\n\n\n\nAh – something of an insight!\n\n\nLabels\nOur labels are not great. We actually do not need the location names, but we need to label the x-axis and give it a title.\n\np6 &lt;- p5 +\n  labs(\n    x = NULL,\n    y = \"Percent\",\n    title = \"Binge Drinking Before Pregnancy\",\n    subtitle = \"Percentage of mothers who reported binging drinking in the 3 months before pregnancy\"\n)\n\np6\n\n\n\n\n\n\n\n\nYou can consider where to put the necessary description. Possible options:\n\ntitle\nsubtitle\ncaption\nx-axis or y-axis labels\n\nAbove, I opted for the subtitle.\n\n\nTheme\nI find the minimal theme to be a good starting point.\n\np7 &lt;- p6 +\n  theme_minimal()\n\np7\n\n\n\n\n\n\n\n\n\n\nGridlines\nHere, the horizontal gridlines are not helpful. (Remember, we used coord_flip() above, so we need to “flip” the gridlines.)\n\np8 &lt;- p7 +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\np8\n\n\n\n\n\n\n\n\nLet’s make the title bold:\n\np9 &lt;- p8 +\n  theme(plot.title = element_text(face = \"bold\"))\n\np9\n\n\n\n\n\n\n\n\n\n\nEmphasizing Certain Values\nImagine we were interested in comparing New York with everyone else. We have two New York bars.\nWe could reduce the opacity of the bars, color the New York bars, and then annotate the specific values.\nLet’s start by removing the fill.\nLet’s also reset our entire plot so we can see all the steps we took\n\np_ny &lt;- df_binge_location |&gt;\n  mutate(location = fct_reorder(location, binge_drinking_within_3_months_birth)) |&gt;\n  ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    aes(fill = location %in% c(\"New York (excluding NYC)\", \"New York City\"))\n  ) +\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"lightgray\")) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"none\" # Remove the legend since we don't need it\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percent\",\n    title = \"Binge Drinking Before Pregnancy\",\n    subtitle = \"Percentage of mothers who reported binging drinking in the 3 months before pregnancy\"\n  ) +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\np_ny\n\n\n\n\n\n\n\n\n\n\nConvert to Lollipop Chart\nI feel the gray is too overwhelming and the red is too bright. We can convert it to a so-called lollipop chart. The information is the same, but for many bars this can look cleaner.\n\np_ny2 &lt;- df_binge_location |&gt;\n    mutate(\n      location = fct_reorder(location, binge_drinking_within_3_months_birth),\n      is_ny = location %in% c(\"New York (excluding NYC)\", \"New York City\")\n    ) |&gt;\n    ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) +\n    geom_segment(aes(xend = location, yend = 0,\n                    color = is_ny),\n                linewidth = ifelse(df_binge_location$location %in% c(\"New York (excluding NYC)\", \"New York City\"), 1.5, 0.8)) +\n    geom_point(aes(color = is_ny),\n              size = ifelse(df_binge_location$location %in% c(\"New York (excluding NYC)\", \"New York City\"), 4.5, 2.2)) +\n    scale_color_manual(\n      values = c(\"TRUE\" = \"#E63946\", \"FALSE\" = \"grey\")  # Darker red\n    ) +\n    scale_x_discrete(\n      labels = function(x) ifelse(x %in% c(\"New York (excluding NYC)\", \"New York City\"), \n                                 paste0(\"**\", x, \"**\"), \n                                 x)\n    ) +\n    coord_flip() +\n    theme_minimal() +\n    theme(\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.position = \"none\",  # Remove legend\n      axis.text.y = element_markdown(size = 11)  # Use markdown for labels\n    ) +\n    labs(\n      x = NULL,\n      y = \"Percent\",\n      title = \"Binge Drinking Before Pregnancy in New York vs. Other States\",\n      subtitle = \"Percentage of mothers who reported binging drinking in the 3 months before pregnancy\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\"))\n\n\np_ny2\n\n\n\n\n\n\n\n\n\n\nAdd Text Annotations\n\np_ny3 &lt;- p_ny2 +\n  geom_text(data = df_binge_location |&gt;\n              filter(location %in% c(\"New York (excluding NYC)\", \"New York City\")) |&gt;\n              mutate(location = fct_reorder(location, binge_drinking_within_3_months_birth)),\n            aes(x = location, y = binge_drinking_within_3_months_birth,\n                label = paste0(round(binge_drinking_within_3_months_birth, 1), \"%\")),\n            hjust = -0.5,\n            vjust = 0.5,\n            size = 4,\n            fontface = \"bold\")\n\np_ny3\n\n\n\n\n\n\n\n\nThis isn’t perfect, but I think it’s a vast improvement and conveys the message well.\nWe can also force the scale to 100%; this can sometimes help viewers realize the actual rates.\n\n\nForce Scale to 100%\n\np_ny4 &lt;- p_ny3 +\n  scale_y_continuous(limits = c(0, 100))\n\np_ny4\n\n\n\n\n\n\n\n\n\n\nWhite Space Adjustments\nAnother final touch: I do not think there is enough vertical space between the title/subtitle and the lines. Likewise, the x-axis labels are a bit too close to the bars.\n\np_ny5 &lt;- p_ny4 +\n  theme(\n    plot.subtitle = element_text(margin = margin(b = 12)),\n    axis.title.x = element_text(margin = margin(t = 10))\n  )\n\np_ny5"
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html",
    "href": "examples/ggplot_themes_and_staying_dry.html",
    "title": "ggplot Themes and Staying DRY",
    "section": "",
    "text": "You may have noticed that as we create more and more notebooks and plots, we repeat a lot of code.\nYou might also notice that a lot of styling decisions in ggplot require being repeated across multiple plots.\nIn this example we will give you some techniques for making your code more DRY."
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#staying-dry-dont-repeat-yourself",
    "href": "examples/ggplot_themes_and_staying_dry.html#staying-dry-dont-repeat-yourself",
    "title": "ggplot Themes and Staying DRY",
    "section": "",
    "text": "You may have noticed that as we create more and more notebooks and plots, we repeat a lot of code.\nYou might also notice that a lot of styling decisions in ggplot require being repeated across multiple plots.\nIn this example we will give you some techniques for making your code more DRY."
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#scaffolding",
    "href": "examples/ggplot_themes_and_staying_dry.html#scaffolding",
    "title": "ggplot Themes and Staying DRY",
    "section": "Scaffolding",
    "text": "Scaffolding\n\nWhen you notice that you are repeating a lot of code, a nice and easy pattern is to create a “scaffold” file (e.g,. `scaffold.R) that contains the code you are repeating\nIn this file you could include your theme.R file, but also common library imports, functions, and so on.\nAll you then need to do is put something like source(\"scaffold.R\") in your setup block."
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#theme-file",
    "href": "examples/ggplot_themes_and_staying_dry.html#theme-file",
    "title": "ggplot Themes and Staying DRY",
    "section": "Theme File",
    "text": "Theme File\n\nI have created a scaffold.R file and a theme.R file in the examples directory\nThe scaffold.R file loads libraries and sources the theme.R file\nRun it in a set up block, like below:\n\n\nsource(here::here(\"examples\", \"scaffold.R\"))\n\n\nNow you don’t need to repeat yourself at the top of every new notebook."
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#using-the-theme",
    "href": "examples/ggplot_themes_and_staying_dry.html#using-the-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Using The Theme",
    "text": "Using The Theme\n\nThis file has a theme_jhu() functions\nIt extends theme_minimal() and adds a few customizations that accord with the https://brand.jhu.edu/visual-identity/colors/"
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#make-a-plot",
    "href": "examples/ggplot_themes_and_staying_dry.html#make-a-plot",
    "title": "ggplot Themes and Staying DRY",
    "section": "Make A Plot",
    "text": "Make A Plot\nBelow we’ll make a simple plot and quickly encounter our usual issues with styling.\n\np &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Fuel Efficiency by Engine Size\",\n    caption = \"Source: mpg dataset\",\n    x = \"Engine Size (liters)\",\n    y = \"Fuel Efficiency (mpg)\"\n  )\n\np"
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#using-a-theme",
    "href": "examples/ggplot_themes_and_staying_dry.html#using-a-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Using A Theme",
    "text": "Using A Theme\n\nIn our theme.R file we have a function called theme_jhu() that looks like this\nNotice how annoying it would be to include this in every plot.\n\n\ntheme_jhu &lt;- function() {\n  theme_minimal(base_family = \"Tahoma\") +\n    theme(\n      plot.title = element_text(\n        family = \"Georgia\", size = 18, hjust = 0.5, face = \"bold\",\n        margin = margin(b = 10)\n      ),\n      plot.subtitle = element_text(\n        hjust = 0.5, family = \"Tahoma\", size = 12,\n        margin = margin(t = 5, b = 15)\n      ),\n      plot.caption = element_text(\n        family = \"Tahoma\", size = 10, color = colors$Gray3,\n        margin = margin(t = 15)\n      ),\n      axis.text.y = element_text(\n        family = \"Tahoma\", size = 9, color = colors$Gray5\n      ),\n      axis.text.x = element_text(\n        family = \"Tahoma\", size = 9, color = colors$Gray5\n      ),\n      axis.title.y = element_text(\n        family = \"Tahoma\", size = 11,\n        margin = margin(r = 15),\n        face = \"bold\"\n      ),\n      axis.title.x = element_text(\n        family = \"Tahoma\", size = 11,\n        margin = margin(t = 15),\n        face = \"bold\"\n      ),\n      axis.line.x = element_line(color = colors$Gray1, linewidth = 0.5),\n      axis.line.y = element_line(color = colors$Gray1, linewidth = 0.5),\n      panel.grid.major.x = element_line(\n        color = colors$Gray1, linewidth = 0.5, linetype = \"solid\"\n      ),\n      panel.grid.minor = element_blank()\n    ) +\n    update_geom_defaults(\"smooth\", list(\n      color = colors$Blue,\n      fill = colors$Gray2\n    ))\n}"
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#applying-the-theme",
    "href": "examples/ggplot_themes_and_staying_dry.html#applying-the-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Applying The Theme",
    "text": "Applying The Theme\n\nAll we need to do to apply the theme is add + theme_jhu() to the end of the plot code\nThis will apply the theme to the plot\nNotice how we now have JHU fonts, better spacing, etc.\n\n\np + theme_jhu()"
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#types-of-plots",
    "href": "examples/ggplot_themes_and_staying_dry.html#types-of-plots",
    "title": "ggplot Themes and Staying DRY",
    "section": "Types of Plots",
    "text": "Types of Plots\n\nSometimes you want to apply a theme to a specific type of plot\nIn theme.R we have a function called theme_jhu_bar() that looks like this\n\n\ntheme_jhu_bar &lt;- function() {\n  theme_jhu() +\n    update_geom_defaults(\"bar\", list(fill = colors$HopkinsBlue)) +\n    theme(\n      axis.line.x = element_blank(),\n      axis.line.y = element_blank(),\n      axis.ticks = element_line(color = colors$Gray1, linewidth = 0.25),\n      axis.ticks.length = unit(0.2, \"cm\"),\n      panel.border = element_blank(),\n      panel.grid.major.x = element_blank(),\n      panel.grid.minor = element_blank()\n    )\n}"
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#make-a-bar-plot",
    "href": "examples/ggplot_themes_and_staying_dry.html#make-a-bar-plot",
    "title": "ggplot Themes and Staying DRY",
    "section": "Make A Bar Plot",
    "text": "Make A Bar Plot\n\nLet’s make a bar plot\n\n\np_bar &lt;- ggplot(mpg, aes(x = class)) +\n  geom_bar() + \n  labs(\n    title = \"Fuel Efficiency by Engine Size\",\n    caption = \"Source: mpg dataset\",\n    x = \"Class of Vehicle\"\n  )\n\np_bar"
  },
  {
    "objectID": "examples/ggplot_themes_and_staying_dry.html#applying-the-bar-theme",
    "href": "examples/ggplot_themes_and_staying_dry.html#applying-the-bar-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Applying The Bar Theme",
    "text": "Applying The Bar Theme\nAll we need to do to apply the bar theme is add + theme_jhu_bar() to the end of the plot code\n\np_bar + theme_jhu_bar()"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html",
    "href": "examples/cleveland_univariate_data.html",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#introduction",
    "href": "examples/cleveland_univariate_data.html#introduction",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#simulated-example-data",
    "href": "examples/cleveland_univariate_data.html#simulated-example-data",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Simulated Example Data",
    "text": "Simulated Example Data\nWe’ll use a simulated healthcare dataset to explore Cleveland’s concepts. Imagine we’re examining patient recovery times (in days) from a medical procedure across different hospitals.\n\nset.seed(1234)\n\n# Hospital A: Normal distribution, mean recovery time\nhospital_a &lt;- rnorm(200, mean = 14, sd = 3)\n\n# Hospital B: Normal distribution, longer recovery\nhospital_b &lt;- rnorm(200, mean = 18, sd = 3)\n\n# Hospital C: Skewed distribution with some very long recoveries\nhospital_c &lt;- c(\n  rnorm(180, mean = 14, sd = 2.5),\n  runif(20, min = 25, max = 40)  # Some outliers\n)\n\n# Hospital D: Bimodal distribution (two distinct patient groups)\nhospital_d &lt;- c(\n  rnorm(100, mean = 10, sd = 2),\n  rnorm(100, mean = 20, sd = 2)\n)\n\n# Combine into data frame\nrecovery_data &lt;- data.frame(\n  hospital = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 200),\n  recovery_days = c(hospital_a, hospital_b, hospital_c, hospital_d)\n)"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#location-measuring-central-tendency",
    "href": "examples/cleveland_univariate_data.html#location-measuring-central-tendency",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Location: Measuring Central Tendency",
    "text": "Location: Measuring Central Tendency\nCleveland emphasized understanding the “location” of data. Different measures of location tell different stories.\n\n# Calculate measures of location for each hospital\nlocation_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    mean = mean(recovery_days),\n    median = median(recovery_days),\n    q25 = quantile(recovery_days, 0.25),\n    q75 = quantile(recovery_days, 0.75),\n    n = n()\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nlocation_summary |&gt;\n  knitr::kable(caption = \"Measures of Location by Hospital\")\n\n\nMeasures of Location by Hospital\n\n\nhospital\nmean\nmedian\nq25\nq75\nn\n\n\n\n\nA\n13.8\n13.5\n11.7\n15.7\n200\n\n\nB\n18.2\n18.4\n16.3\n20.1\n200\n\n\nC\n15.6\n14.3\n12.0\n16.3\n200\n\n\nD\n14.9\n13.8\n9.8\n19.9\n200\n\n\n\n\n\n\nKey Insights:\n\nHospital A & B: Mean ≈ Median (symmetric distributions)\nHospital C: Mean &gt; Median (right-skewed due to outliers)\nHospital D: Bimodal pattern obscures single “center”"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "href": "examples/cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Visual Comparison: Mean vs. Median",
    "text": "Visual Comparison: Mean vs. Median\n\nggplot(recovery_data, aes(x = recovery_days, y = hospital)) +\n  geom_boxplot(outlier.alpha = 0.3, fill = \"lightblue\") +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = mean),\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 0.8\n  ) +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = median),\n    color = \"darkblue\",\n    linetype = \"solid\",\n    linewidth = 0.8\n  ) +\n  labs(\n    title = \"Recovery Times by Hospital\",\n    subtitle = \"Solid line = median (resistant), Dashed line = mean (sensitive to outliers)\",\n    x = \"Recovery Days\",\n    y = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nKey insight: The median is a resistant measure. It is not affected by extreme values. The mean is sensitive to outliers. Hospital C shows outliers pulling the mean upward."
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#spread-measuring-variability",
    "href": "examples/cleveland_univariate_data.html#spread-measuring-variability",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread: Measuring Variability",
    "text": "Spread: Measuring Variability\nUnderstanding how data varies around the center is equally important as knowing where the center is.\n\nspread_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    sd = sd(recovery_days),\n    iqr = IQR(recovery_days),\n    range = max(recovery_days) - min(recovery_days),\n    cv = sd / mean(recovery_days) * 100\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nspread_summary |&gt;\n  knitr::kable(\n    caption = \"Measures of Spread by Hospital\",\n    col.names = c(\"Hospital\", \"Std Dev\", \"IQR\", \"Range\", \"CV (%)\")\n  )\n\n\nMeasures of Spread by Hospital\n\n\nHospital\nStd Dev\nIQR\nRange\nCV (%)\n\n\n\n\nA\n3.1\n4.0\n17.7\n22.1\n\n\nB\n3.0\n3.8\n18.9\n16.6\n\n\nC\n6.1\n4.2\n32.1\n39.2\n\n\nD\n5.6\n10.1\n19.7\n37.2\n\n\n\n\n\n\nKey Insights:\n\nIQR (Interquartile Range): Resistant to outliers, focuses on middle 50% of data\nStandard Deviation: Sensitive to outliers (see Hospital C)\nCoefficient of Variation: Standardized measure of spread relative to mean"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#understanding-quantiles",
    "href": "examples/cleveland_univariate_data.html#understanding-quantiles",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Understanding Quantiles",
    "text": "Understanding Quantiles\nA quantile answers the question: “What value do X% of my observations fall below?”\nThe pth percentile is the value where p percent of the data falls below it. For example:\n\n25th percentile (p25): 25% of observations are below this value\n50th percentile (p50): 50% of observations are below this value (this is the median)\n75th percentile (p75): 75% of observations are below this value\n\n\nHow to Calculate Quantiles\nStep 1: Sort your data from smallest to largest\nStep 2: Find the position\nFor the pth percentile with n observations:\n\\[\\text{Position} = p \\times (n + 1)\\]\nStep 3: Get the value\n\nIf the position is a whole number (like 25), use that observation\nIf the position is fractional (like 25.25), interpolate between the two nearest values\n\n\n\nQuantiles Are…\n\nDistribution-free\nRobust to outliers\nEasy to compare: Comparing quantiles across groups reveals differences in location, spread, and shape."
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#spread-location-plots",
    "href": "examples/cleveland_univariate_data.html#spread-location-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread-Location Plots",
    "text": "Spread-Location Plots\nCleveland emphasized visualizing the relationship between location (central tendency) and spread (dispersion). This helps identify whether variability is constant or changes with the level of the data. This is fundamental to many modeling assumptions (e.g., for pooled t-tests, homoskedasticity).\n\n# Create meaningful groups by binning recovery times\n# This gives us many more points to assess the relationship\nspread_location_data &lt;- recovery_data |&gt;\n  mutate(\n    recovery_bin = cut(recovery_days,\n                      breaks = seq(0, 50, by = 2.5),\n                      include.lowest = TRUE)\n  ) |&gt;\n  group_by(recovery_bin) |&gt;\n  filter(n() &gt;= 5) |&gt;  # Only keep bins with at least 5 observations\n  summarize(\n    median = median(recovery_days),\n    iqr = IQR(recovery_days),\n    mad = mad(recovery_days),\n    n = n()\n  ) |&gt;\n  filter(!is.na(median))\n\n# Create spread-location plot\nggplot(spread_location_data, aes(x = median, y = iqr)) +\n  geom_point(size = 3, color = \"steelblue\", alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Spread-Location Relationship\",\n    subtitle = \"Does variability increase with central tendency?\",\n    x = \"Location (Median Recovery Days)\",\n    y = \"Spread (IQR)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation: The plot shows the relationship between location and spread across recovery time bins. If the red line slopes upward (spread increases with location), this indicates *monotone spread or nonconstant variance. This violates assumptions of many statistical tests (e.g., pooled t-tests, OLS) and suggests a transformation (like log or square root) might stabilize the variance."
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#mean-difference-plot",
    "href": "examples/cleveland_univariate_data.html#mean-difference-plot",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Mean-Difference Plot",
    "text": "Mean-Difference Plot\nWhen comparing two hospitals or measurement methods, Cleveland recommended plotting the difference vs. the average. This reveals whether differences are constant or depend on the magnitude.\n\n# Create comparison data with proper matching\n# Hospital B: constant offset (same SD as A, just shifted)\n# Hospital C: monotone spread (variance increases with magnitude)\nset.seed(789)\n\n# For Hospital B: pair each observation with a random one from Hospital A\nhospital_b_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"B\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital B vs A\"\n  )\n\n# For Hospital C: pair observations - this has outliers so will show monotone spread\nhospital_c_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital C vs A\"\n  )\n\ncomparison_pairs &lt;- bind_rows(hospital_b_pairs, hospital_c_pairs)\n\nggplot(comparison_pairs, aes(x = mean_value, y = difference)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\", linewidth = 1) +\n  facet_wrap(~ comparison, ncol = 2) +\n  labs(\n    title = \"Mean-Difference Plots: Comparing Hospitals to Hospital A\",\n    subtitle = \"Red line shows average difference; horizontal = constant spread\",\n    x = \"Average of Two Measurements\",\n    y = \"Difference (Hospital - Hospital A)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nHospital B vs A: Relatively flat red line around +4 days means constant spread (simple location shift)\nHospital C vs A: Upward-sloping red line → monotone spread (variability increases at higher values due to outliers)\n\nThis type of plot is essential in method comparison studies and reveals patterns that simple correlation or regression would miss."
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#power-transformations-for-stabilizing-variance",
    "href": "examples/cleveland_univariate_data.html#power-transformations-for-stabilizing-variance",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Power Transformations for Stabilizing Variance",
    "text": "Power Transformations for Stabilizing Variance\nWhen spread-location plots reveal monotone spread (variance changes with level), Cleveland advocated for power transformations to stabilize variance and make distributions more symmetric.\n\nThe Power Transformation Family\nThe power transformation family takes the form:\n\\[y^{\\lambda} \\text{ where } \\lambda \\text{ is the power}\\]\nCommon transformations:\n\n\n\nλ\nTransformation\nName\nWhen to Use\n\n\n\n\n2\n\\(y^2\\)\nSquare\nCompress right tail\n\n\n1\n\\(y\\)\nIdentity\nNo transformation needed\n\n\n0.5\n\\(\\sqrt{y}\\)\nSquare root\nModerate right skew, count data\n\n\n0\n\\(\\log(y)\\)\nLog*\nStrong right skew, multiplicative processes\n\n\n-0.5\n\\(-1/\\sqrt{y}\\)\nInverse square root\nVery strong right skew\n\n\n-1\n\\(-1/y\\)\nInverse\nExtreme right skew\n\n\n\n\n\nWhen to Transform\n\nMonotone spread: Variance increases with the mean\nStrong skewness: Long tails distort analyses\nMultiplicative relationships: Percent changes, growth rates\nMeeting model assumptions: Many tests assume constant variance and normality\n\n\n\nExample: Log Transformation for Hospital C\nHospital C has outliers creating right skew and monotone spread. Let’s see if log transformation helps.\n\n# Create comparison of original vs log-transformed data for Hospital C\nhospital_c_data &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  mutate(\n    log_recovery = log(recovery_days),\n    sqrt_recovery = sqrt(recovery_days)\n  )\n\n# Create histograms\np1 &lt;- ggplot(hospital_c_data, aes(x = recovery_days)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Original Scale\",\n    x = \"Recovery Days\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np2 &lt;- ggplot(hospital_c_data, aes(x = log_recovery)) +\n  geom_histogram(bins = 30, fill = \"coral\", color = \"white\") +\n  labs(\n    title = \"Log-Transformed\",\n    x = \"Log(Recovery Days)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np3 &lt;- ggplot(hospital_c_data, aes(x = sqrt_recovery)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", color = \"white\") +\n  labs(\n    title = \"Square Root Transformed\",\n    x = \"√(Recovery Days)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n# Combine plots\n(p1 / p2 / p3) +\n  plot_annotation(\n    title = \"Effect of Power Transformations on Hospital C Data\",\n    subtitle = \"Log and √ transformations reduce right skew and stabilize variance\"\n  )\n\n\n\n\n\n\n\n\n\n\nAssessing the Transformation\n\n# Compare summary statistics before and after transformation\ntransformation_summary &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"SD\", \"IQR\", \"Skewness\"),\n  Original = c(\n    mean(hospital_c_data$recovery_days),\n    median(hospital_c_data$recovery_days),\n    sd(hospital_c_data$recovery_days),\n    IQR(hospital_c_data$recovery_days),\n    moments::skewness(hospital_c_data$recovery_days)\n  ),\n  Log = c(\n    mean(hospital_c_data$log_recovery),\n    median(hospital_c_data$log_recovery),\n    sd(hospital_c_data$log_recovery),\n    IQR(hospital_c_data$log_recovery),\n    moments::skewness(hospital_c_data$log_recovery)\n  ),\n  Sqrt = c(\n    mean(hospital_c_data$sqrt_recovery),\n    median(hospital_c_data$sqrt_recovery),\n    sd(hospital_c_data$sqrt_recovery),\n    IQR(hospital_c_data$sqrt_recovery),\n    moments::skewness(hospital_c_data$sqrt_recovery)\n  )\n) |&gt;\n  mutate(across(where(is.numeric), ~round(., 2)))\n\ntransformation_summary |&gt;\n  knitr::kable(caption = \"Comparing Transformations for Hospital C\")\n\n\nComparing Transformations for Hospital C\n\n\nMeasure\nOriginal\nLog\nSqrt\n\n\n\n\nMean\n15.59\n2.69\n3.89\n\n\nMedian\n14.29\n2.66\n3.78\n\n\nSD\n6.11\n0.32\n0.68\n\n\nIQR\n4.23\n0.30\n0.56\n\n\nSkewness\n2.12\n1.07\n1.64\n\n\n\n\n\nLog scale: Differences become ratios (additive turns to multiplicative); logs are typically preferred over other transformations because of this interpretive enefit\n\n\nChoosing the Right Transformation\nCleveland suggested using data-driven approaches:\n\nVisual inspection: Try several transformations, plot results\nSymmetry: Choose transformation that makes distribution most symmetric\nConstant variance: Choose transformation that stabilizes spread-location relationship\n\nThe goal is not mathematical purity but getting an intimate undrstanding of your data."
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#comparing-distributions-q-q-plots",
    "href": "examples/cleveland_univariate_data.html#comparing-distributions-q-q-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Comparing Distributions: Q-Q Plots",
    "text": "Comparing Distributions: Q-Q Plots\nCleveland emphasized quantile-quantile (Q-Q) plots for comparing distributions. They plot quantiles of one distribution against another.\n\n# Compare each hospital to Hospital A\ncomparison_data &lt;- recovery_data |&gt;\n  filter(hospital != \"A\") |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    quantile_rank = rank(recovery_days) / (n() + 1)\n  ) |&gt;\n  ungroup()\n\n# Get corresponding quantiles from Hospital A\nhospital_a_quantiles &lt;- function(p) {\n  quantile(recovery_data$recovery_days[recovery_data$hospital == \"A\"], p)\n}\n\ncomparison_data &lt;- comparison_data |&gt;\n  mutate(\n    hospital_a_value = sapply(quantile_rank, hospital_a_quantiles)\n  )\n\nggplot(comparison_data, aes(x = hospital_a_value, y = recovery_days)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ hospital, ncol = 3) +\n  labs(\n    title = \"Q-Q Plots: Comparing Hospitals to Hospital A\",\n    subtitle = \"Points on red line = identical distributions\",\n    x = \"Hospital A Quantiles\",\n    y = \"Other Hospital Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nInterpreting Q-Q Plots:\n\nHospital B: Points above line → consistently longer recovery times\nHospital C: Curves up at high quantiles → longer tail (outliers)\nHospital D: S-shaped curve → different distribution shape (bimodal vs. unimodal)"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#q-q-plots-for-assessing-normality",
    "href": "examples/cleveland_univariate_data.html#q-q-plots-for-assessing-normality",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Q-Q Plots for Assessing Normality",
    "text": "Q-Q Plots for Assessing Normality\nA particularly important use of Q-Q plots is assessing whether data follows a normal distribution. This is done by plotting sample quantiles against theoretical normal quantiles.\n\nWhy This Matters\nMany statistical methods assume normality: - t-tests - ANOVA - Linear regression (assumes normal residuals) - Confidence intervals based on standard errors\nCleveland emphasized visual assessment over formal tests because: 1. Visual patterns reveal the type of departure (skew, outliers, heavy tails) 2. Sample size affects formal tests (large n makes trivial departures “significant”) 3. We can see if departures matter for our specific purpose\n\n\nHow It Works\nNormal Q-Q Plot: - X-axis: Theoretical quantiles from standard normal distribution - Y-axis: Observed quantiles from your data - If data is normal: Points fall on a straight line - If not normal: Systematic deviations reveal the problem\n\n# Create Q-Q plots for each hospital\nlibrary(patchwork)\n\n# Hospital A - approximately normal\np_a &lt;- ggplot(recovery_data |&gt; filter(hospital == \"A\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital A: Approximately Normal\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital B - normal but shifted\np_b &lt;- ggplot(recovery_data |&gt; filter(hospital == \"B\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital B: Normal (Different Mean)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital C - right-skewed with outliers\np_c &lt;- ggplot(recovery_data |&gt; filter(hospital == \"C\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital C: Right-Skewed + Outliers\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital D - bimodal\np_d &lt;- ggplot(recovery_data |&gt; filter(hospital == \"D\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital D: Bimodal (Non-Normal)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine\n(p_a + p_b) / (p_c + p_d) +\n  plot_annotation(\n    title = \"Normal Q-Q Plots: Diagnosing Departures from Normality\"\n  )\n\n\n\n\n\n\n\n\n\n\nReading the Patterns\n1. Points on the line (Hospitals A & B): - Data follows normal distribution - Parametric methods are safe to use\n2. Curve at upper end (Hospital C): - Right tail heavier than normal → right skew - Upper quantiles deviate upward from line - Common with positive data (income, recovery times) - Solution: Log or square root transformation\n3. Curve at both ends (Hospital C outliers): - Points at extremes deviate from line - Indicates outliers beyond what normal distribution would produce - Action: Investigate outliers, consider robust methods\n4. S-shaped curve (Hospital D): - Points curve below line at low end, above at high end - Distribution has shorter tails than normal (or is bimodal) - Indicates: Fundamentally different distribution shape - Action: Don’t assume normality, use distribution-free methods\n5. Points below line at low end, above at high end: - Distribution has heavier tails than normal - More extreme values than normal distribution predicts - Common in financial data, measurement errors\n\n\nBenefits of Q-Q Plots Over Formal Tests\nShapiro-Wilk test and similar tests have limitations:\n\n\n\n\n\n\n\nFormal Tests\nQ-Q Plots\n\n\n\n\nBinary answer (reject/fail to reject)\nShows what and how much departure\n\n\nVery sensitive with large n\nAssess practical significance\n\n\nDon’t reveal type of departure\nSee if it’s skew, outliers, heavy tails, etc.\n\n\nNo guidance on fix\nSuggests transformation strategy\n\n\nArbitrary α threshold\nProfessional judgment based on context\n\n\n\nExample: With n = 10,000, Shapiro-Wilk might reject normality for trivial departures that don’t affect t-test validity. Q-Q plot shows if departure matters for your analysis.\n\n\nPractical Workflow\nCleveland’s approach to checking normality:\n\nStart with histogram: Get overall shape sense\nCreate Q-Q plot: Diagnose specific departures\nAssess practical impact:\n\nSlight skew with large n? Probably fine.\nStrong bimodality? Serious problem.\nOutliers? Investigate before proceeding.\n\nConsider alternatives:\n\nTransform to achieve normality\nUse robust methods (median, bootstrap)\nUse distribution-free tests (Mann-Whitney, Kruskal-Wallis)\n\n\n\n\nWhen Normality Doesn’t Matter\nWith large samples, Central Limit Theorem means: - Means are approximately normal even if data isn’t - t-tests, ANOVA are robust to moderate departures - More concerned about outliers than mild skewness\nFocus Q-Q plot assessment on: - Small samples (n &lt; 30): Normality matters more - Extreme violations: Bimodality, severe skew, major outliers - Residual diagnostics: In regression, check if residuals are normal\n\n\nExample: Hospital C After Log Transformation\n\n# Compare original vs log-transformed Hospital C\nhospital_c_data &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  mutate(log_recovery = log(recovery_days))\n\np_orig &lt;- ggplot(hospital_c_data, aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Scale\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles (Days)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np_log &lt;- ggplot(hospital_c_data, aes(sample = log_recovery)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Log-Transformed\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles (Log Days)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np_orig + p_log +\n  plot_annotation(\n    title = \"Log Transformation Improves Normality\",\n    subtitle = \"Points align better with line after transformation\"\n  )\n\n\n\n\n\n\n\n\nKey observation: Log transformation brings the upper tail back toward the line, making distribution more symmetric and closer to normal.\n\n\nCleveland’s Message\n\n“The goal is not to prove normality, but to understand your data’s shape and choose appropriate methods.”\n\nQ-Q plots serve exploration and diagnosis, not hypothesis testing. Use them to: - Understand what you’re working with - Decide if assumptions are reasonable - Choose between parametric and robust methods - Transform when it aids interpretation"
  },
  {
    "objectID": "examples/cleveland_univariate_data.html#references",
    "href": "examples/cleveland_univariate_data.html#references",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "References",
    "text": "References\nCleveland, W. S. (1993). Visualizing Data. Hobart Press."
  },
  {
    "objectID": "examples/applications_6_visualizing_correlations_and_models.html",
    "href": "examples/applications_6_visualizing_correlations_and_models.html",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"broom.mixed\",\n  \"kableExtra\",\n  \"lme4\",\n  \"readr\",\n  \"emmeans\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}\n\nsource(here::here(\"examples\", \"colors.R\"))\nset.seed(123)"
  },
  {
    "objectID": "examples/applications_6_visualizing_correlations_and_models.html#looking-at-data-correlations",
    "href": "examples/applications_6_visualizing_correlations_and_models.html#looking-at-data-correlations",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Looking At Data: Correlations",
    "text": "Looking At Data: Correlations\nCorrelations are indisposable for understanding the relationship between two variables, but they can be misleading as we show below.\nThis is adapted directly from Jan Vanhove.\n\nplot_r &lt;- function(df, showSmoother = TRUE, smoother = \"lm\") {\n  p &lt;- ggplot(df, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7)\n  \n  if(showSmoother) {\n    p &lt;- p +\n    geom_smooth(\n      formula = y ~ x,\n      method = smoother,\n      color = colors$green$`500`,\n      fill = colors$slate$`300`,\n      alpha = 0.3,\n    )\n  }\n\n  p &lt;- p +\n    facet_wrap(~title, scales = \"free\", ncol = 2) +\n    theme_minimal(base_size = 12) +\n    theme(\n      strip.text = element_text(face = \"bold\", size = 14),\n      axis.title = element_blank(),\n      plot.title = element_text(size = 24, face = \"bold\", hjust = 0.5),\n      panel.grid.minor = element_blank(),\n      axis.text = element_text(size = 10),\n      panel.spacing = unit(2, \"lines\")\n    )\n  \n  p\n}\n\ncorr_r &lt;- function(r = 0.6, n = 50) {\n  \n  compute.y &lt;- function(x, y, r) {\n    theta &lt;- acos(r)\n    X &lt;- cbind(x, y)\n    Xctr &lt;- scale(X, center = TRUE, scale = FALSE)    # Centered variables (mean 0)\n    Id &lt;- diag(n)                                     # Identity matrix\n    Q &lt;- qr.Q(qr(Xctr[, 1, drop = FALSE]))            # QR decomposition\n    P &lt;- tcrossprod(Q)                                # Projection onto space defined by x1\n    x2o &lt;- (Id - P) %*% Xctr[, 2]                     # x2ctr made orthogonal to x1ctr\n    Xc2 &lt;- cbind(Xctr[, 1], x2o)\n    Y &lt;- Xc2 %*% diag(1 / sqrt(colSums(Xc2 ^ 2)))\n    y &lt;- Y[, 2] + (1 / tan(theta)) * Y[, 1]\n    return(y)\n  }\n  \n  cases &lt;- list(\n    list(id = 1, title = \"(1) Normal x, normal residuals\", x = rnorm(n), y = rnorm(n)),\n    list(id = 2, title = \"(2) Uniform x, normal residuals\", x = runif(n, 0, 1), y = rnorm(n)),\n    list(id = 3, title = \"(3) +-skewed x, normal residuals\", x = rlnorm(n, 5), y = rnorm(n)),\n    list(id = 4, title = \"(4) --skewed x, normal residuals\", x = rlnorm(n, 5) * -1 + 5000, y = rnorm(n)),\n    list(id = 5, title = \"(5) Normal x, +-skewed residuals\", x = rnorm(n), y = rlnorm(n, 5)),\n    list(id = 6, title = \"(6) Normal x, --skewed residuals\", x = rnorm(n), y = -rlnorm(n, 5)),\n    list(id = 7, title = \"(7) Increasing spread\", \n         x = sort(rnorm(n)) + abs(min(rnorm(n))), \n         y = rnorm(n, 0, sqrt(abs(10 * sort(rnorm(n)))))),\n    list(id = 8, title = \"(8) Decreasing spread\", \n         x = sort(rnorm(n)) + abs(min(rnorm(n))), \n         y = rnorm(n, 0, sqrt(pmax(0.1, abs(10 * max(sort(rnorm(n))) - 10 * sort(rnorm(n))))))),\n    list(id = 9, title = \"(9) Quadratic trend\", x = rnorm(n), y = rnorm(n) ^ 2),\n    list(id = 10, title = \"(10) Sinusoid relationship\", x = runif(n, -2 * pi, 2 * pi), y = sin(runif(n, -2 * pi, 2 * pi))),\n    list(id = 11, title = \"(11) A single positive outlier\", x = c(rnorm(n - 1), 10), y = c(rnorm(n - 1), 15)),\n    list(id = 12, title = \"(12) A single negative outlier\", x = c(rnorm(n - 1), 10), y = c(rnorm(n - 1), -15)),\n    list(id = 13, title = \"(13) Bimodal residuals\", x = rnorm(n), y = c(rnorm(floor(n / 2), mean = -3), rnorm(ceiling(n / 2), 3))),\n    list(id = 14, title = \"(14) Two groups\", \n         x = c(rnorm(floor(n / 2), -3), rnorm(ceiling(n / 2), 3)), \n         y = c(rnorm(floor(n / 2), mean = 3), rnorm(ceiling(n / 2), mean = -3))),\n    list(id = 15, title = \"(15) Sampling at the extremes\", \n         x = c(rnorm(floor(n / 2)), rnorm(ceiling(n / 2), mean = 10)), \n         y = rnorm(n)),\n    list(id = 16, title = \"(16) Categorical data\", \n         x = sample(1:5, n, replace = TRUE), \n         y = sample(1:7, n, replace = TRUE))\n  )\n  \n  df &lt;- bind_rows(lapply(cases, function(case) {\n    id = case$id\n    x &lt;- case$x\n    y &lt;- compute.y(x, case$y, r)\n    data.frame(id = id, x = x, y = y, title = case$title)\n  }))\n  \n  df$title &lt;- factor(df$title, levels = paste0(\"(\", 1:16, \") \", c(\n    \"Normal x, normal residuals\",\n    \"Uniform x, normal residuals\",\n    \"+-skewed x, normal residuals\",\n    \"--skewed x, normal residuals\",\n    \"Normal x, +-skewed residuals\",\n    \"Normal x, --skewed residuals\",\n    \"Increasing spread\",\n    \"Decreasing spread\",\n    \"Quadratic trend\",\n    \"Sinusoid relationship\",\n    \"A single positive outlier\",\n    \"A single negative outlier\",\n    \"Bimodal residuals\",\n    \"Two groups\",\n    \"Sampling at the extremes\",\n    \"Categorical data\"\n  )))\n  \n\n  return(df)\n}\n\n\ndata &lt;- corr_r(r=0.3, n=100)\n\nanalysis_data &lt;- data |&gt; filter(id == 1)\nmodel &lt;- lm(y ~ x, data = analysis_data)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = analysis_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.19848 -0.07113 -0.00910  0.06042  0.34241 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.00313    0.01015  -0.308  0.75846   \nx            0.03463    0.01112   3.113  0.00243 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.101 on 98 degrees of freedom\nMultiple R-squared:   0.09, Adjusted R-squared:  0.08071 \nF-statistic: 9.692 on 1 and 98 DF,  p-value: 0.002426\n\ncor(analysis_data$x, analysis_data$y)\n\n[1] 0.3\n\nplot_r(data, showSmoother=FALSE)\n\n\n\n\n\n\n\nplot_r(data, showSmoother=TRUE)"
  },
  {
    "objectID": "examples/applications_6_visualizing_correlations_and_models.html#visualizing-model-outputs",
    "href": "examples/applications_6_visualizing_correlations_and_models.html#visualizing-model-outputs",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Visualizing Model Outputs",
    "text": "Visualizing Model Outputs\nVisualization of model outputs is often neglected, but it can be a powerful way both to undrestand and to communicate the results of a model. A number of packages exist to help with this, including broom.mixed, emmeans, and ggeffects.\nHere, we fit a logistic mixed-effects model using the glmer() function to estimate the odds of receiving comprehensive postnatal care. This model accounts for both fixed effects (individual-level predictors like race or insurance status) and a random intercept for provider, which captures unobserved heterogeneity across providers.\n\n# Load data\ndata &lt;- read_csv(here::here(\"data\", \"processed\", \"simulated_data.csv\"))\n\nRows: 50000 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): state, self_report_income, edu, race_ethnicity, insurance, job_type\ndbl (14): id, provider_id, received_comprehensive_postnatal_care, age, depen...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Sample 1/4 of the sites\nset.seed(123)\nunique_sites &lt;- unique(data$provider_id)\nreduced_sites &lt;- sample(unique_sites, length(unique_sites) * 0.10)\ndata &lt;- data[data$provider_id %in% reduced_sites, ]\n\n# Fit the model\nmodel &lt;- glmer(\n  received_comprehensive_postnatal_care ~ \n    race_ethnicity +\n    log(distance_to_provider) +\n    insurance +\n    multiple_gestation + \n    placenta_previa + \n    gest_hypertension + \n    preeclampsia +\n    (1 | provider_id),  \n  data = data, \n  family = binomial(link = \"logit\"),\n  control = glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 1e5))\n)\n\n# Create variable labels for better visualization\nvariable_labels &lt;- c(\n  \"race_ethnicityaian\" = \"AIAN\",\n  \"race_ethnicityasian\" = \"Asian\",\n  \"race_ethnicityblack\" = \"Black\",\n  \"race_ethnicityhispanic\" = \"Hispanic\",\n  \"race_ethnicitynhpi\" = \"NHPI\",\n  \"race_ethnicityother\" = \"Other\",\n  \"log(distance_to_provider)\" = \"Log(Distance to Provider)\",\n  \"insuranceprivate\" = \"Insurance: Private\",\n  \"insurancestate_provided\" = \"Insurance: State-Provided\",\n  \"multiple_gestation\" = \"Multiple Gestation\",\n  \"placenta_previa\" = \"Placenta Previa\",\n  \"gest_hypertension\" = \"Gestational Hypertension\",\n  \"preeclampsia\" = \"Preeclampsia\",\n  \"(Intercept)\" = \"Intercept\"\n)\n\nBelow, we extract the fixed-effect estimates using broom.mixed::tidy(), including 95% confidence intervals, and plot them as a coefficient plot (a forest plot for regression coefficients).\nEach point shows the estimated log-odds of receiving comprehensive care associated with a given covariate, holding other variables constant. The dashed vertical line at 0 represents no effect.\n\n# Get fixed effects with confidence intervals\nfixed_effects &lt;- tidy(model, conf.int = TRUE) |&gt;\n  filter(effect == \"fixed\") |&gt;\n  # Remove any NA values\n  filter(!is.na(estimate)) |&gt;\n  # Create a more readable term name\n  mutate(term = case_when(\n    term == \"(Intercept)\" ~ \"Intercept\",\n    term == \"age\" ~ \"Age\",\n    term == \"sexMale\" ~ \"Male Sex\",\n    term == \"raceBlack\" ~ \"Black Race\",\n    term == \"raceHispanic\" ~ \"Hispanic Race\",\n    term == \"raceOther\" ~ \"Other Race\",\n    term == \"insurancePrivate\" ~ \"Private Insurance\",\n    term == \"insuranceMedicaid\" ~ \"Medicaid\",\n    term == \"insuranceOther\" ~ \"Other Insurance\",\n    term == \"comorbidity_score\" ~ \"Comorbidity Score\",\n    term == \"emergency_admissionTRUE\" ~ \"Emergency Admission\",\n    term == \"weekend_admissionTRUE\" ~ \"Weekend Admission\",\n    term == \"night_admissionTRUE\" ~ \"Night Admission\",\n    TRUE ~ term\n  ))\n\n# Create the forest plot\nggplot(fixed_effects, aes(x = estimate, y = reorder(term, estimate))) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), \n                 height = 0.2, color = colors$slate$`300`) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = colors$red$`600`) +\n  labs(\n    title = \"Fixed Effects from GLMM\",\n    subtitle = \"Each point represents the estimated effect on log-odds of receiving comprehensive care\",\n    x = \"Effect Size (95% CI)\",\n    y = \"Predictor\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    axis.text.y = element_text(size = 10)\n  )"
  },
  {
    "objectID": "examples/applications_6_visualizing_correlations_and_models.html#predicted-probabilities-by-covariate-groups",
    "href": "examples/applications_6_visualizing_correlations_and_models.html#predicted-probabilities-by-covariate-groups",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Predicted Probabilities by Covariate Groups",
    "text": "Predicted Probabilities by Covariate Groups\nHere, we use the emmeans package to estimate marginal predicted probabilities for selected covariates. These represent the expected probability of receiving comprehensive care for each level of a covariate, averaging over the distribution of other covariates in the model. This approach helps isolate the relationship between each covariate and the outcome while controlling for confounding. We visualize these estimates and their 95% confidence intervals, grouped by domain (e.g., race/ethnicity, insurance, medical conditions).\n\n# Calculate predicted probabilities for each covariate group\nlibrary(emmeans)\n\n# Race/Ethnicity\nrace_probs &lt;- emmeans(model, ~ race_ethnicity, type = \"response\")\nrace_probs_df &lt;- as.data.frame(race_probs) |&gt;\n  mutate(\n    group = \"Race/Ethnicity\",\n    category = case_when(\n      race_ethnicity == \"aian\" ~ \"AIAN\",\n      race_ethnicity == \"asian\" ~ \"Asian\",\n      race_ethnicity == \"black\" ~ \"Black\",\n      race_ethnicity == \"hispanic\" ~ \"Hispanic\",\n      race_ethnicity == \"nhpi\" ~ \"NHPI\",\n      race_ethnicity == \"other\" ~ \"Other\",\n      race_ethnicity == \"white\" ~ \"White\"\n    )\n  )\n\n# Insurance\ninsurance_probs &lt;- emmeans(model, ~ insurance, type = \"response\")\ninsurance_probs_df &lt;- as.data.frame(insurance_probs) |&gt;\n  mutate(\n    group = \"Insurance\",\n    category = case_when(\n      insurance == \"medicaid\" ~ \"Medicaid\",\n      insurance == \"private\" ~ \"Private\",\n      insurance == \"state_provided\" ~ \"State-Provided\",\n      insurance == \"uninsured\" ~ \"Uninsured\",\n      TRUE ~ \"Other\"\n    )\n  )\n\n# Medical Conditions\nconditions_probs &lt;- emmeans(model, ~ multiple_gestation + placenta_previa + gest_hypertension + preeclampsia, type = \"response\")\nconditions_probs_df &lt;- as.data.frame(conditions_probs) |&gt;\n  mutate(\n    group = \"Medical Conditions\",\n    category = case_when(\n      multiple_gestation == 1 ~ \"Multiple Gestation\",\n      placenta_previa == 1 ~ \"Placenta Previa\",\n      gest_hypertension == 1 ~ \"Gestational Hypertension\",\n      preeclampsia == 1 ~ \"Preeclampsia\",\n      TRUE ~ \"No Medical Conditions\"\n    )\n  ) |&gt;\n  # Remove duplicates where multiple conditions are true\n  distinct(category, .keep_all = TRUE)\n\n# Combine all predictions\nall_probs &lt;- bind_rows(\n  race_probs_df |&gt; select(group, category, prob, asymp.LCL, asymp.UCL),\n  insurance_probs_df |&gt; select(group, category, prob, asymp.LCL, asymp.UCL),\n  conditions_probs_df |&gt; select(group, category, prob, asymp.LCL, asymp.UCL)\n)\n\n# Create faceted plot\nggplot(all_probs, aes(x = prob, y = reorder(category, prob))) +\n  geom_errorbarh(aes(xmin = asymp.LCL, xmax = asymp.UCL), \n                 height = 0.2, color = colors$slate$`300`) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  facet_wrap(~ group, scales = \"free_y\", ncol = 1) +\n  labs(\n    title = \"Predicted Probability of Receiving Comprehensive Care\",\n    subtitle = \"By Covariate Groups\",\n    x = \"Predicted Probability (95% CI)\",\n    y = \"Category\"\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    axis.text.y = element_text(size = 10),\n    strip.text = element_text(size = 12, face = \"bold\")\n  )"
  },
  {
    "objectID": "examples/applications_6_visualizing_correlations_and_models.html#provider-level-random-effects",
    "href": "examples/applications_6_visualizing_correlations_and_models.html#provider-level-random-effects",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Provider-Level Random Effects",
    "text": "Provider-Level Random Effects\nWe visualize the provider-level random intercepts, which represent the estimated deviation of each provider from the overall mean (intercept) after accounting for the fixed effects in the model. This helps reveal between-provider variability and can identify providers whose outcomes are systematically higher or lower than expected. The dashed vertical line at zero indicates the overall average; providers to the right have higher-than-average effects, and those to the left are lower.\n\n# Get random effects\nrandom_effects &lt;- ranef(model, condVar = TRUE)\nrandom_effects_data &lt;- as.data.frame(random_effects)\n\n# Create the forest plot of random effects\nggplot(random_effects_data, aes(x = condval, y = reorder(grp, condval))) +\n  geom_errorbarh(aes(xmin = condval - 1.96*sqrt(attr(random_effects$provider_id, \"postVar\")[1,1,]), \n                     xmax = condval + 1.96*sqrt(attr(random_effects$provider_id, \"postVar\")[1,1,])), \n                 height = 0.2, color = colors$slate$`300`) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = colors$red$`600`) +\n  labs(\n    title = \"Random Effects by Provider\",\n    subtitle = \"Each point represents the provider-specific deviation from the overall intercept\",\n    x = \"Provider-Specific Effect (95% CI)\",\n    y = \"Provider ID\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    axis.text.y = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\n# Print model summary\nsummary(model)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nreceived_comprehensive_postnatal_care ~ race_ethnicity + log(distance_to_provider) +  \n    insurance + multiple_gestation + placenta_previa + gest_hypertension +  \n    preeclampsia + (1 | provider_id)\n   Data: data\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 1e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   5679.3    5776.5   -2824.7    5649.3      4782 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.3504 -0.6684 -0.5330  1.1475  3.0147 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n provider_id (Intercept) 0.2053   0.4531  \nNumber of obs: 4797, groups:  provider_id, 50\n\nFixed effects:\n                          Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)               -1.19650    0.40041  -2.988  0.00281 **\nrace_ethnicityasian        0.36808    0.41330   0.891  0.37315   \nrace_ethnicityblack       -0.21309    0.40567  -0.525  0.59939   \nrace_ethnicityhispanic     0.11511    0.39780   0.289  0.77230   \nrace_ethnicitynhpi         0.07320    0.80323   0.091  0.92739   \nrace_ethnicityother        0.54133    0.52931   1.023  0.30645   \nrace_ethnicitywhite        0.23508    0.39261   0.599  0.54933   \nlog(distance_to_provider) -0.02094    0.02519  -0.831  0.40585   \ninsuranceprivate           0.18922    0.07774   2.434  0.01493 * \ninsurancestate_provided    0.15031    0.08768   1.714  0.08648 . \nmultiple_gestation         0.22298    0.20185   1.105  0.26930   \nplacenta_previa            0.10400    0.30208   0.344  0.73063   \ngest_hypertension          0.23839    0.14268   1.671  0.09477 . \npreeclampsia               0.30954    0.22275   1.390  0.16463   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCorrelation matrix not shown by default, as p = 14 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it"
  },
  {
    "objectID": "examples/applications_6_visualizing_correlations_and_models.html#interaction-effects",
    "href": "examples/applications_6_visualizing_correlations_and_models.html#interaction-effects",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Interaction Effects",
    "text": "Interaction Effects\nThe emmeans package can also be used to help visualize interaction effects. Let’s simulate some data with an interaction effect.\n\nInteraction Between Categorical and Continuous Variable\nIn this example, we simulate data to study how different exercise programs might be more or less effective depending on a person’s starting fitness level. We have:\n\nA categorical predictor: Type of exercise program\n\nHigh-Intensity Interval Training (HIIT): Short bursts of intense exercise followed by rest\nStrength Training: Weight lifting and resistance exercises\nCardio: Steady-state aerobic exercise like jogging or cycling\n\nA continuous predictor: Baseline fitness level (on a 1-10 scale)\nAn outcome: Weight loss in pounds\n\nThe key question is whether the effectiveness of each program depends on how fit someone is when they start. For example, HIIT might be more effective for people who are already somewhat fit, while Cardio might be better for beginners. The relationship between program type and weight loss changes depends on baseline fitness.\nEach program has:\n\nA baseline effectiveness (how much weight loss we expect at average fitness levels)\nA different rate of effectiveness based on baseline fitness (how much more or less effective it becomes as fitness changes)\nSome random variation to account for individual differences\n\n\n# Simulate data with an interaction effect\nset.seed(123)\nn &lt;- 500\n\n# Simulate data for program × fitness interaction\nix_data &lt;- data.frame(\n  # Categorical predictor: Exercise program type\n  program = sample(c(\"HIIT\", \"Strength Training\", \"Cardio\"), \n                  size = n, replace = TRUE),\n  # Continuous predictor: Baseline fitness level (1-10 scale)\n  baseline_fitness = runif(n, min = 1, max = 10),\n  # Add some noise\n  error = rnorm(n, 0, 2)\n) |&gt;\n  # Create interaction effect\n  mutate(\n    # Center the baseline fitness\n    baseline_fitness_centered = baseline_fitness - mean(baseline_fitness),\n    # Different slopes for different programs\n    fitness_effect = case_when(\n      program == \"HIIT\" ~ 0.8,\n      program == \"Strength Training\" ~ 0.5,\n      program == \"Cardio\" ~ 0.3\n    ),\n    # Different intercepts for different programs\n    program_effect = case_when(\n      program == \"HIIT\" ~ 8.0,\n      program == \"Strength Training\" ~ 6.0,\n      program == \"Cardio\" ~ 4.0\n    ),\n    # Calculate outcome: Weight loss in pounds\n    weight_loss = program_effect + (fitness_effect * baseline_fitness_centered) + error\n  )\n\nLet’s fit a model to the simulated data.\n\nmodel &lt;- lm(weight_loss ~ program * baseline_fitness_centered, data = ix_data)\nmodel |&gt; tidy() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.1573036\n0.1628360\n25.530615\n0.0000000\n\n\nprogramHIIT\n3.7323100\n0.2285624\n16.329498\n0.0000000\n\n\nprogramStrength Training\n1.9190550\n0.2283159\n8.405262\n0.0000000\n\n\nbaseline_fitness_centered\n0.2963606\n0.0643186\n4.607698\n0.0000052\n\n\nprogramHIIT:baseline_fitness_centered\n0.4923781\n0.0884914\n5.564134\n0.0000000\n\n\nprogramStrength Training:baseline_fitness_centered\n0.2588035\n0.0920493\n2.811576\n0.0051260\n\n\n\n\n\nWe get results back. We have “main effects” for the program type and the baseline fitness level, and an interaction effect between the two.\nCentering helps: The intercept represents the expected weight loss when baseline fitness is at its mean.\nThe emmeans package can help us interpret. Let’s get it to give us the predicted weight loss at the average fitness level.\n\n# Plot the interaction\nemmeans(model, ~ program | baseline_fitness_centered)\n\nbaseline_fitness_centered = 1.28e-16:\n program           emmean    SE  df lower.CL upper.CL\n Cardio              4.16 0.163 494     3.84     4.48\n HIIT                7.89 0.160 494     7.57     8.20\n Strength Training   6.08 0.160 494     5.76     6.39\n\nConfidence level used: 0.95 \n\n\nWe see that this lines up with our model results:\n\nCardio: 4.16 lbs (intercept)\nHIIT: 7.89 lbs (4.16 + 3.73 = 7.89)\nStrength Training: 6.08 lbs (4.16 + 1.92 = 6.08)\n\nBut what if we want to see the relationship between program type and weight loss at different baseline fitness levels? Let’s create a plot that shows how weight loss changes across the full range of baseline fitness for each program.\nWe can use emmip() (expected means interaction plot)to plot the predicted weight loss by program type at different baseline fitness levels. We add some text to the plot to show the slopes for each program.\n\n# Create baseline fitness values in the original (uncentered) scale\n\n\n# Get centered values based on the observed data range\ncentered_range &lt;- range(ix_data$baseline_fitness - mean(ix_data$baseline_fitness))\ncentered_vals &lt;- seq(centered_range[1], centered_range[2], length.out = 100)\n\n# Create the plot\nemmip(\n  model,\n  program ~ baseline_fitness_centered,\n  at = list(baseline_fitness_centered = centered_vals),\n  type = \"response\"\n) +\n  labs(\n    title = \"Predicted Weight Loss by Program and Baseline Fitness\",\n    x = \"Baseline Fitness Level (Centered)\",\n    y = \"Predicted Weight Loss (pounds)\",\n    color = \"Program Type\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(-5, 5, by = 1),\n    labels = seq(-5, 5, by = 1)\n  ) +\n  annotate(\"text\", x = 3.5, y = 4.5, label = \"Cardio slope: 0.30\", color = \"#F8766D\", hjust = 0) +\n  annotate(\"text\", x = 3.5, y = 10.5, label = \"HIIT slope: 0.79\", color = \"#00BA38\", hjust = 0) +\n  annotate(\"text\", x = 3.5, y = 7.5, label = \"Strength slope: 0.56\", color = \"#619CFF\", hjust = 0) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nI find this a lot easier to interpret than the table of results. “Seeing is believing.”"
  },
  {
    "objectID": "examples/applications_6_visualizing_correlations_and_models.html#more-complexity-categorical-by-continuous-by-continuous-interaction",
    "href": "examples/applications_6_visualizing_correlations_and_models.html#more-complexity-categorical-by-continuous-by-continuous-interaction",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "More Complexity: Categorical by Continuous by Continuous Interaction",
    "text": "More Complexity: Categorical by Continuous by Continuous Interaction\nLet’s simulate data to get a sense of a complex set of interactions.\nWe will create a simulated dataset that explores how lung function is influenced by age, air pollution exposure, and smoking status. Both age and pollution independently reduce lung function, but the negative effect of pollution becomes more pronounced as people get older, so pollution is more damaging to lung function in older adults than in younger ones. Moreover, smoking increases the effect of pollution on lung function.\n\nfev1 = forced expiratory volume in one second\nage = age, which can be 30-80\npm25 = particulate matter exposure\nis_smoker = whether the person is a smoker\n\n\nset.seed(123)\nn &lt;- 300\nage &lt;- runif(n, 30, 80)              # Age 30–80\npm25 &lt;- runif(n, 5, 35)              # PM2.5 range\ninteraction_effect &lt;- -0.01          # Base moderation: steeper slope with age\n\n# Add smoking status (binary)\n# More likely to have smoking history as age increases\nsmoking_prob &lt;- 0.2 + (age - 30)/100  # Probability increases with age\nis_smoker &lt;- rbinom(n, 1, smoking_prob)\n\n# Create age-dependent error term (more variation with age)\nage_error &lt;- rnorm(n, 0, 0.2 * (1 + (age - 30)/50))  # Error increases with age\n\n# Outcome: base lung function + age effect + pollution effect + \n# two-way interactions + three-way interaction + smoking effect\n# Please note how are simulated data looks like the model we are going to estimate.\n# Simulating data is a great way to undrestand how models work.\n# In fact, if you uncenter the below model, you will see that the coefficients are very close to what we define below.\nfev1 &lt;- 4.5 - # intercept\n        0.02*age - # age effect\n        0.03*pm25 + # pollution effect\n        interaction_effect * age * pm25 -  # age × pollution\n        0.5*is_smoker +                    # smoking main effect\n        0.02*age*is_smoker +               # age × smoking\n        0.01*pm25*is_smoker +              # pollution × smoking\n        -0.005*age*pm25*is_smoker +        # three-way interaction\n        age_error\n\n# Create data frame\ndata &lt;- data.frame(age, pm25, fev1, is_smoker)\ndata |&gt; head() |&gt; kable()\n\n\n\n\nage\npm25\nfev1\nis_smoker\n\n\n\n\n44.37888\n28.537258\n-9.5398588\n0\n\n\n69.41526\n5.282897\n-0.3395903\n0\n\n\n50.44885\n28.371976\n-11.5508085\n0\n\n\n74.15087\n26.881720\n-26.1678392\n1\n\n\n77.02336\n23.903956\n-23.7395757\n1\n\n\n32.27782\n19.427325\n-6.3510524\n1\n\n\n\n\n\nA starting point for an interaction analysis is to color code by group and look at the trends. Let’s create age bins and plot out the trends, faceting by smoker status.\nWe clearly see a slope difference by age. We also see that all the slopes of smokers appear to be steeper than the slopes of non-smokers.\n\ndata |&gt; \n  mutate(\n    age_group = cut(age, breaks = c(30, 40, 50, 60, 70, 80), \n                   labels = c(\"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70+\")),\n    smoking_status = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\")\n  ) |&gt; \n  ggplot(aes(x = pm25, y = fev1, color = age_group)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ smoking_status) +\n  labs(\n    title = \"Lung Function by Air Pollution and Age Group\",\n    subtitle = \"Faceted by Smoking Status\",\n    x = \"PM2.5 Exposure\",\n    y = \"FEV1 (L)\",\n    color = \"Age Group\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Show variation by age group and smoking status\ndata |&gt;\n  mutate(\n    age_group = cut(age, breaks = c(30, 40, 50, 60, 70, 80), \n                   labels = c(\"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70+\")),\n    smoking_status = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\")\n  ) |&gt;\n  group_by(age_group, smoking_status) |&gt;\n  summarize(\n    mean_fev1 = mean(fev1),\n    sd_fev1 = sd(fev1),\n    min_fev1 = min(fev1),\n    max_fev1 = max(fev1),\n    n = n()\n  ) |&gt;\n  kable()\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage_group\nsmoking_status\nmean_fev1\nsd_fev1\nmin_fev1\nmax_fev1\nn\n\n\n\n\n30-39\nNon-smoker\n-3.868944\n3.220115\n-9.858047\n0.9199290\n40\n\n\n30-39\nSmoker\n-7.520161\n4.991676\n-15.763336\n0.4955185\n13\n\n\n40-49\nNon-smoker\n-6.433924\n4.233292\n-13.192217\n0.5287203\n43\n\n\n40-49\nSmoker\n-11.018059\n6.030700\n-20.044162\n-0.5861045\n25\n\n\n50-59\nNon-smoker\n-8.334104\n5.279194\n-17.175684\n0.4095706\n45\n\n\n50-59\nSmoker\n-13.701037\n8.796267\n-27.544445\n-0.4953899\n17\n\n\n60-69\nNon-smoker\n-9.396839\n6.462022\n-21.041806\n-0.3395903\n28\n\n\n60-69\nSmoker\n-16.031041\n8.681144\n-30.743716\n-1.9684067\n33\n\n\n70+\nNon-smoker\n-11.475683\n7.039974\n-20.919413\n-0.8792988\n19\n\n\n70+\nSmoker\n-16.826588\n9.214377\n-33.283871\n-1.8593214\n37\n\n\n\n\n\nNow let’s center our variables and fit a model with the three-way interaction.\n\ndata &lt;- data |&gt; \n  mutate(\n    pm25_centered = pm25 - mean(pm25),\n    age_centered = age - mean(age)\n  )\n\nmodel &lt;- lm(fev1 ~ pm25_centered * age_centered * is_smoker, data = data)\nmodel |&gt; tidy() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-8.1597223\n0.0230908\n-353.37484\n0\n\n\npm25_centered\n-0.5730489\n0.0025919\n-221.09554\n0\n\n\nage_centered\n-0.2173991\n0.0017101\n-127.12552\n0\n\n\nis_smoker\n-4.7052941\n0.0363237\n-129.53798\n0\n\n\npm25_centered:age_centered\n-0.0099022\n0.0001941\n-51.02526\n0\n\n\npm25_centered:is_smoker\n-0.2686729\n0.0041309\n-65.04034\n0\n\n\nage_centered:is_smoker\n-0.0783907\n0.0025621\n-30.59665\n0\n\n\npm25_centered:age_centered:is_smoker\n-0.0048553\n0.0003050\n-15.91835\n0\n\n\n\n\n\nWe see that the interaction terms in our regression model are statistically significant, but interpreting them directly from the table is very difficult. This is especially true in models with two or more continuous predictors interacting, and even more so when one of them is also interacting with a binary group.\nTake the main effects: pm25_centered and age_centered. These coefficients only describe the effect of PM2.5 or age at the average level of the other variable. For example, the coefficient for PM2.5 tells us how pollution affects FEV1 only at the mean age in our sample. That’s not very useful unless you already know what the mean is, and even then, it’s just a narrow slice of the story.\nNow add in interaction terms like pm25 × age or pm25 × age × is_smoker, and things get more abstract. What does a coefficient of –0.01 for pm25 × age actually mean? Technically, it means the effect of PM2.5 on lung function gets more negative as age increases, but:\n\nHow much more?\nIs that change clinically meaningful?\nDoes the slope actually flip direction at some point?\nIs the effect stronger in smokers than non-smokers?\n\nYou could try to work this out by multiplying coefficients in your head, but if you’re like me, then you’re not smart enough. Better to use visualization to help.\nemmeans can help us by generating predicted values and estimated trends. It also allows us to probe the model and “un-center” the data for interpretation.\nFor example, we can explore how the slope of PM2.5 on FEV1 changes at different ages, and see how those slopes differ by smoking status. Using emtrends(), we can even extract the actual slope estimates by subgroup, turning abstract interactions into tangible comparisons.\n\n# Get centering values\nmean_age &lt;- mean(data$age)\nmean_pm25 &lt;- mean(data$pm25)\n\n# Define values for uncentered age and PM2.5\nage_vals &lt;- c(40, 50, 60, 70, 80)\npm25_vals &lt;- seq(5, 35, length.out = 100)\n\n# Centered versions to use in emmip\nage_centered_vals &lt;- age_vals - mean_age\npm25_centered_vals &lt;- pm25_vals - mean_pm25\n\n# 1. Get predicted values with emmip, grouped by smoking status\npred_data &lt;- emmip(\n  model,\n  age_centered ~ pm25_centered | is_smoker,\n  at = list(\n    age_centered = age_centered_vals,\n    pm25_centered = pm25_centered_vals,\n    is_smoker = c(0, 1)\n  ),\n  type = \"response\",\n  plotit = FALSE\n)\n\n# Uncenter for plotting\npred_data &lt;- pred_data |&gt;\n  mutate(\n    age = age_centered + mean_age,\n    pm25 = pm25_centered + mean_pm25,\n    smoker_label = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\")\n  )\n\n# 2. Get slopes of PM2.5 at each age × smoking status\nslopes &lt;- emtrends(\n  model,\n  ~ age_centered * is_smoker,\n  var = \"pm25_centered\",\n  at = list(age_centered = age_centered_vals, is_smoker = c(0, 1))\n) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    age = age_centered + mean_age,\n    smoker_label = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\"),\n    label = paste0(\"Slope: \", round(pm25_centered.trend, 3))\n  )\n\n# 3. Plot\nggplot(pred_data, aes(x = pm25, y = yvar, color = factor(age), group = interaction(age, smoker_label))) +\n  geom_line(linewidth = 1) +\n  geom_text(\n    data = pred_data |&gt;\n      group_by(age, smoker_label) |&gt;\n      slice_max(pm25) |&gt;\n      left_join(slopes, by = c(\"age\", \"smoker_label\")),\n    aes(x = pm25, y = yvar, label = label, color = factor(age)),\n    hjust = -0.1,\n    size = 3.5,\n    inherit.aes = FALSE,\n    show.legend = FALSE\n  ) +\n  facet_wrap(~ smoker_label) +\n  labs(\n    title = \"Predicted Lung Function by PM2.5 and Age, with Smoking Interaction\",\n    subtitle = \"Slopes of PM2.5 Effect Vary by Age and Smoking Status\",\n    x = \"PM2.5 (μg/m³)\",\n    y = \"Predicted FEV1\",\n    color = \"Age\"\n  ) +\n  xlim(min(pm25_vals), max(pm25_vals) + 5) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\",\n    strip.text = element_text(face = \"bold\")\n  )"
  },
  {
    "objectID": "examples/dag_sim_data.html",
    "href": "examples/dag_sim_data.html",
    "title": "Data Simulation",
    "section": "",
    "text": "To simulate data, we implement our causal model. We will start by simulating data for root nodes or exogenous nodes (i.e., those without arrows going into them)."
  },
  {
    "objectID": "examples/dag_sim_data.html#data-simulation",
    "href": "examples/dag_sim_data.html#data-simulation",
    "title": "Data Simulation",
    "section": "",
    "text": "To simulate data, we implement our causal model. We will start by simulating data for root nodes or exogenous nodes (i.e., those without arrows going into them)."
  },
  {
    "objectID": "examples/dag_sim_data.html#statesgeography",
    "href": "examples/dag_sim_data.html#statesgeography",
    "title": "Data Simulation",
    "section": "States/Geography",
    "text": "States/Geography\nUsing data from the Census and the Commonwealth Fund health grades, we’ll generate a data frame with some state-level data. This data will be used to generate state-level variables for patients, such as the political/economic conditions related to health. It will also be used to structure the distribution of race/ethnicity in the population per state.\n\n## Population Data from US Census 2020. Combined with health scores from Commonwealth Fund\n## Health Scorecard data.  The weights measure is based upon population data from the Census.\n## `pec` is a z-score of the rank of the state in terms of population and used as a \n## measure of state political & economic conditions with respect to health.\n## This data is for illustration purposes only.\nstate_population_data &lt;- read.csv(here::here(\"data/raw/states_census2020_ranks.csv\")) |&gt; \n  mutate(state_weight = population/sum(population)) |&gt; \n   mutate(\n    inverted_rank = max(rank) + 1 - rank,  # Invert ranks (lower is better)\n    pec = scale(inverted_rank, center = TRUE, scale = TRUE)[, 1]  # Z-score\n  ) |&gt; \n  select(-inverted_rank) \n\n\n# Load in data on race/ethnicity population from states. This is from the Census2020 file.\n# It uses their one-race only and separates  Hispanic from non-Hispanic. The weights\n# are just the proportion of the population of one category vs the sum of the population in\n# all race/ethnicity categories. \n# This data is meant for illustration purposes only.\nstate_data &lt;- readr::read_csv(here::here(\"data/raw/race_by_state_census2020.csv\")) |&gt; \n  pivot_longer(cols = -\"Label\",\n               names_to = \"State\",\n               values_to = \"Population\") |&gt;\n  pivot_wider(names_from = \"Label\", values_from = Population) |&gt;\n  mutate(across(-State, ~ as.numeric(gsub(\",\", \"\", .)))) |&gt;\n  rename(\n    state = State,\n    white = White,\n    black = Black,\n    hispanic = \"Hispanic or Latino\",\n    asian = Asian,\n    aian = \"American Indian and Alaska Native\",\n    nhpi = \"Native Hawaiian and Other Pacific Islander\",\n    other = Other,\n  ) |&gt; mutate(\n    sum = white + hispanic + black + asian + aian + other + nhpi,\n    white = white / sum,\n    black = black / sum,\n    hispanic = hispanic / sum,\n    asian = asian / sum,\n    aian = aian / sum,\n    other = other / sum,\n    nhpi = nhpi / sum,\n  ) |&gt;\n  rename(state_name = state) |&gt;\n  select(-sum) |&gt;\n  left_join(state_population_data |&gt; select(state_name, state, state_weight, pec)) |&gt;\n  filter(!is.na(state))\n\nRows: 7 Columns: 53\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Label\nnum (52): Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecti...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nJoining with `by = join_by(state_name)`\n\nstate_data |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate_name\nhispanic\nwhite\nblack\naian\nasian\nnhpi\nother\nstate\nstate_weight\npec\n\n\n\n\nAlabama\n0.0545590\n0.6552837\n0.2661672\n0.0047770\n0.0156866\n0.0005397\n0.0029868\nAL\n0.0152792\n-0.8744746\n\n\nAlaska\n0.0753049\n0.6374530\n0.0313332\n0.1644998\n0.0656696\n0.0188247\n0.0069147\nAK\n0.0022303\n-1.6144147\n\n\nArizona\n0.3184257\n0.5543550\n0.0460678\n0.0383359\n0.0361437\n0.0020804\n0.0045915\nAZ\n0.0217483\n-0.6726728\n\n\nArkansas\n0.0896697\n0.7204210\n0.1570623\n0.0071740\n0.0178783\n0.0049854\n0.0028093\nAR\n0.0141646\n-1.0762765\n\n\nCalifornia\n0.4109587\n0.3617622\n0.0559023\n0.0041172\n0.1577082\n0.0036446\n0.0059068\nCA\n0.1202389\n0.8072074\n\n\nColorado\n0.2291691\n0.6821550\n0.0401439\n0.0061253\n0.0354114\n0.0016334\n0.0053620\nCO\n0.0175583\n0.8744746\n\n\nConnecticut\n0.1797075\n0.6571469\n0.1040652\n0.0018464\n0.0491466\n0.0002808\n0.0078065\nCT\n0.0108044\n1.3453456\n\n\nDelaware\n0.1101354\n0.6123516\n0.2248964\n0.0026623\n0.0447744\n0.0003210\n0.0048589\nDE\n0.0030105\n0.6054055\n\n\nDistrict of Columbia\n0.1176439\n0.3965867\n0.4273339\n0.0019347\n0.0502863\n0.0005287\n0.0056858\nDC\n0.0020970\n-1.6816820\n\n\nFlorida\n0.2746181\n0.5350660\n0.1507300\n0.0020326\n0.0303492\n0.0005553\n0.0066486\nFL\n0.0654994\n-0.4036037\n\n\nGeorgia\n0.1088434\n0.5194994\n0.3175926\n0.0019740\n0.0460851\n0.0005911\n0.0054145\nGA\n0.0325758\n-0.6054055\n\n\nHawaii\n0.1194132\n0.2702167\n0.0188047\n0.0019950\n0.4569079\n0.1281214\n0.0045411\nHI\n0.0043058\n1.6144147\n\n\nIdaho\n0.1359265\n0.8235534\n0.0083944\n0.0107324\n0.0147823\n0.0019310\n0.0046801\nID\n0.0017543\n-1.2108110\n\n\nIllinois\n0.1885365\n0.6027553\n0.1432216\n0.0013358\n0.0602759\n0.0002387\n0.0036362\nIL\n0.0389639\n0.2018018\n\n\nIndiana\n0.0849962\n0.7854079\n0.0977733\n0.0019843\n0.0255592\n0.0004235\n0.0038556\nIN\n0.0206353\n-0.4708710\n\n\nIowa\n0.0700867\n0.8560874\n0.0419642\n0.0029461\n0.0243428\n0.0018188\n0.0027540\nIA\n0.0097022\n0.4036037\n\n\nKansas\n0.1371900\n0.7610919\n0.0585731\n0.0078602\n0.0305591\n0.0011169\n0.0036086\nKS\n0.0886338\n-0.9417419\n\n\nKentucky\n0.0479980\n0.8462734\n0.0826155\n0.0018658\n0.0170519\n0.0007995\n0.0033959\nKY\n0.0137026\n-0.2690691\n\n\nLouisiana\n0.0716511\n0.5768320\n0.3226409\n0.0057743\n0.0189566\n0.0003790\n0.0037662\nLA\n0.0055929\n-1.1435437\n\n\nMaine\n0.0203311\n0.9384758\n0.0191895\n0.0055723\n0.0127355\n0.0003110\n0.0033848\nME\n0.0041430\n1.2780783\n\n\nMaryland\n0.1235503\n0.4933212\n0.3039091\n0.0020410\n0.0707635\n0.0004360\n0.0059789\nMD\n0.0187854\n1.1435437\n\n\nMassachusetts\n0.1324579\n0.7086173\n0.0682005\n0.0014007\n0.0753398\n0.0002398\n0.0137441\nMA\n0.0209607\n1.6816820\n\n\nMichigan\n0.0585621\n0.7569665\n0.1409480\n0.0049186\n0.0344768\n0.0002701\n0.0038580\nMI\n0.0303708\n0.4708710\n\n\nMinnesota\n0.0631830\n0.7958892\n0.0718130\n0.0104280\n0.0543757\n0.0004791\n0.0038320\nMN\n0.0173539\n1.0090092\n\n\nMississippi\n0.0365622\n0.5695525\n0.3749352\n0.0048714\n0.0112255\n0.0003603\n0.0024928\nMS\n0.0022303\n-1.5471474\n\n\nMissouri\n0.0518324\n0.7976482\n0.1184822\n0.0040184\n0.0226024\n0.0015893\n0.0038270\nMO\n0.0187176\n-0.3363364\n\n\nMontana\n0.0439049\n0.8755114\n0.0049316\n0.0627426\n0.0078457\n0.0008150\n0.0042488\nMT\n0.0032972\n0.0000000\n\n\nNebraska\n0.1242621\n0.7860186\n0.0499796\n0.0079683\n0.0277197\n0.0006978\n0.0033539\nNE\n0.0059651\n0.1345346\n\n\nNevada\n0.3030463\n0.4853986\n0.0993841\n0.0079627\n0.0905442\n0.0078191\n0.0058451\nNV\n0.0094414\n-0.7399401\n\n\nNew Hampshire\n0.0449400\n0.9075440\n0.0141009\n0.0017378\n0.0269123\n0.0002933\n0.0044718\nNH\n0.0041892\n1.5471474\n\n\nNew Jersey\n0.2225201\n0.5351818\n0.1282448\n0.0012452\n0.1047746\n0.0002160\n0.0078175\nNJ\n0.0282486\n0.5381382\n\n\nNew Mexico\n0.4912203\n0.3756288\n0.0186271\n0.0916581\n0.0171357\n0.0007051\n0.0050249\nNM\n0.0120407\n-1.3453456\n\n\nNew York\n0.2026669\n0.5440805\n0.1416307\n0.0028186\n0.0983721\n0.0003130\n0.0101182\nNY\n0.0614777\n1.0762765\n\n\nNorth Carolina\n0.1114968\n0.6291678\n0.2100691\n0.0100559\n0.0338956\n0.0006957\n0.0046190\nNC\n0.0317470\n0.2690691\n\n\nNorth Dakota\n0.0446180\n0.8495205\n0.0349231\n0.0498767\n0.0174268\n0.0011605\n0.0024745\nND\n0.0023693\n-0.2018018\n\n\nOhio\n0.0461421\n0.7925496\n0.1289781\n0.0016772\n0.0262531\n0.0003977\n0.0040023\nOH\n0.0358831\n-0.0672673\n\n\nOklahoma\n0.1316157\n0.6713349\n0.0789927\n0.0869823\n0.0250031\n0.0022780\n0.0037934\nOK\n0.0054548\n-1.4126129\n\n\nOregon\n0.1479820\n0.7631278\n0.0197704\n0.0105671\n0.0482075\n0.0045738\n0.0057714\nOR\n0.0128858\n0.9417419\n\n\nPennsylvania\n0.0836252\n0.7611426\n0.1090696\n0.0011973\n0.0403679\n0.0002519\n0.0043454\nPA\n0.0389319\n0.7399401\n\n\nRhode Island\n0.1742378\n0.7214899\n0.0529944\n0.0033613\n0.0367103\n0.0003062\n0.0109001\nRI\n0.0033372\n1.4798801\n\n\nSouth Carolina\n0.0715863\n0.6448878\n0.2574703\n0.0033661\n0.0181369\n0.0006259\n0.0039267\nSC\n0.0155655\n-0.8072074\n\n\nSouth Dakota\n0.0454581\n0.8279207\n0.0204650\n0.0875287\n0.0156436\n0.0005785\n0.0024054\nSD\n0.0026964\n0.0672673\n\n\nTennessee\n0.0721600\n0.7379203\n0.1632035\n0.0023400\n0.0202243\n0.0005412\n0.0036107\nTN\n0.0210164\n-0.5381382\n\n\nTexas\n0.4048817\n0.4099377\n0.1218961\n0.0030229\n0.0552566\n0.0009858\n0.0040193\nTX\n0.0091583\n-1.0090092\n\n\nUtah\n0.1564222\n0.7823633\n0.0118026\n0.0091046\n0.0249489\n0.0113707\n0.0039877\nUT\n0.0099492\n-0.1345346\n\n\nVermont\n0.0252702\n0.9342703\n0.0140972\n0.0032370\n0.0186740\n0.0002771\n0.0041742\nVT\n0.0019569\n1.4126129\n\n\nVirginia\n0.1104663\n0.6148877\n0.1918305\n0.0023193\n0.0742252\n0.0007531\n0.0055180\nVA\n0.0262488\n0.3363364\n\n\nWashington\n0.1472322\n0.6837234\n0.0411681\n0.0126757\n0.1005067\n0.0086862\n0.0060078\nWA\n0.0237116\n1.2108110\n\n\nWest Virginia\n0.0202297\n0.9287010\n0.0376102\n0.0018512\n0.0086566\n0.0002492\n0.0027022\nWV\n0.0090055\n-1.4798801\n\n\nWisconsin\n0.0786102\n0.8144184\n0.0644130\n0.0085034\n0.0306270\n0.0003325\n0.0030954\nWI\n0.0179233\n0.6726728\n\n\nWyoming\n0.1067398\n0.8490302\n0.0085596\n0.0212970\n0.0091056\n0.0008840\n0.0043838\nWY\n0.0064396\n-1.2780783\n\n\n\n\nreadr::write_csv(state_data, here::here(\"data/processed/state_data.csv\"))"
  },
  {
    "objectID": "examples/dag_sim_data.html#providers",
    "href": "examples/dag_sim_data.html#providers",
    "title": "Data Simulation",
    "section": "Providers",
    "text": "Providers\nWe need to generate providers. We will have a provider for roughly every 100 patients. We will generate a provider quality measure which is mostly random, but determined partially by state political and economic considerations with respect to health.\nProviders will vary by state population. We below show a random sample of 50 providers.\n\nproviders &lt;- state_data |&gt;\n  slice(rep(1:n(), each = 1)) |&gt;\n  arrange(state) |&gt;\n  mutate(\n    id = 1:n(),\n  ) |&gt; \n  select(id, state, pec)\n\nremaining_providers &lt;- n_providers - nrow(providers)\n\nadditional_providers &lt;- data.frame(\n  id = (nrow(providers) + 1):n_providers,\n  state = sample(state_data$state, remaining_providers, replace = TRUE, prob = state_data$state_weight)\n) |&gt; \n  left_join(state_data |&gt; select(state, pec), by = \"state\")\n\nproviders &lt;- bind_rows(providers, additional_providers) |&gt; \n  mutate(\n    quality = gen_provider_quality(n_providers, pec)\n  ) |&gt; \n  arrange(id) |&gt; \n  select(id, state, quality)\n\nproviders |&gt; sample_n(50) |&gt;  kable()\n\n\n\n\nid\nstate\nquality\n\n\n\n\n336\nNC\n0.2049902\n\n\n422\nSC\n1.6171118\n\n\n175\nTX\n-0.3280715\n\n\n85\nMN\n1.4525175\n\n\n416\nCA\n-0.3319574\n\n\n177\nFL\n-0.6703678\n\n\n276\nPA\n-0.2349806\n\n\n196\nGA\n-0.8649781\n\n\n225\nOR\n2.2181021\n\n\n191\nTN\n1.2696197\n\n\n325\nFL\n0.6065428\n\n\n74\nCA\n0.9928161\n\n\n379\nTX\n0.4294213\n\n\n270\nCO\n2.2474936\n\n\n476\nKS\n-0.0453329\n\n\n471\nVA\n0.8498859\n\n\n453\nWY\n0.3249208\n\n\n209\nUT\n0.3000523\n\n\n357\nVA\n1.2152936\n\n\n472\nFL\n0.0193411\n\n\n151\nKS\n0.8789851\n\n\n11\nGA\n0.2884123\n\n\n160\nNY\n0.7065599\n\n\n359\nNY\n0.7777045\n\n\n397\nMI\n0.4066230\n\n\n184\nFL\n2.1986934\n\n\n328\nMO\n0.9025574\n\n\n267\nAZ\n0.5664791\n\n\n244\nCA\n-0.9034353\n\n\n228\nGA\n1.6925911\n\n\n121\nWY\n0.8632490\n\n\n114\nWA\n1.3993463\n\n\n199\nNY\n1.8635650\n\n\n233\nSC\n0.2138441\n\n\n162\nNY\n1.4839141\n\n\n341\nCA\n1.0951649\n\n\n344\nNC\n0.7918381\n\n\n146\nMA\n1.1284493\n\n\n474\nME\n-0.5828624\n\n\n213\nCA\n1.1362895\n\n\n373\nKY\n1.4916928\n\n\n423\nNM\n1.3937375\n\n\n310\nTN\n0.0456490\n\n\n190\nNM\n-0.9314094\n\n\n219\nTN\n0.3712131\n\n\n117\nTN\n1.7359103\n\n\n429\nNJ\n1.9262119\n\n\n173\nDE\n0.2570958\n\n\n243\nMA\n1.3939015\n\n\n139\nIN\n1.5585299"
  },
  {
    "objectID": "examples/dag_sim_data.html#patients",
    "href": "examples/dag_sim_data.html#patients",
    "title": "Data Simulation",
    "section": "Patients",
    "text": "Patients\nWe’ll start by generating a date frame with patients. We will generate a state for each patient based on the population of each state. We will randomly assign a race/ethnicity based upon state-level proportions. We will then assign that person to a provider in their state.\n\n# Start by creating a dataframe with patients having state IDs proportional to\n# the population of each state. Then row-wise generate race/ethnicity categories.\ndata &lt;- data.frame(state = sample(\n  state_data$state,\n  n,\n  replace = TRUE,\n  prob = state_data$state_weight\n)) |&gt;\n  merge(state_data |&gt; select(state, pec, re_cats), by = \"state\") |&gt;\n  rowwise() |&gt;\n  mutate(race = sample(re_cats, size = 1, prob = c_across(all_of(re_cats)))) |&gt;\n  ungroup() |&gt;\n  select(-re_cats) |&gt; \n  mutate(id = row_number())\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(re_cats)\n\n  # Now:\n  data %&gt;% select(all_of(re_cats))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\ndata |&gt; sample_n(10) |&gt; kable()\n\n\n\n\nstate\npec\nrace\nid\n\n\n\n\nNC\n0.2690691\nblack\n30667\n\n\nMN\n1.0090092\nwhite\n28755\n\n\nAZ\n-0.6726728\nwhite\n2385\n\n\nTN\n-0.5381382\nblack\n44247\n\n\nMD\n1.1435437\nwhite\n26228\n\n\nGA\n-0.6054055\nwhite\n13698\n\n\nPA\n0.7399401\nwhite\n41629\n\n\nMA\n1.6816820\nwhite\n24538\n\n\nOH\n-0.0672673\nwhite\n38792\n\n\nNC\n0.2690691\nwhite\n30120"
  },
  {
    "objectID": "examples/dag_sim_data.html#assign-patient-to-provider",
    "href": "examples/dag_sim_data.html#assign-patient-to-provider",
    "title": "Data Simulation",
    "section": "Assign Patient to Provider",
    "text": "Assign Patient to Provider\n\ndata &lt;- data |&gt; \n  rowwise() |&gt;\n  mutate(provider_id = sample(providers$id[providers$state == state], 1)) |&gt;\n  ungroup()  |&gt; \n  left_join(providers |&gt; select(id, quality), by = c(\"provider_id\" = \"id\")) |&gt; \n  select(id, provider_id, quality, state, pec, race) |&gt; \n  rename(\n    provider_quality = quality\n  )\n  \n\ndata |&gt; sample_n(10) |&gt; kable()\n\n\n\n\nid\nprovider_id\nprovider_quality\nstate\npec\nrace\n\n\n\n\n25426\n315\n0.8489427\nMD\n1.1435437\nblack\n\n\n28899\n113\n0.3040839\nMO\n-0.3363364\nwhite\n\n\n1470\n3\n0.3500858\nAR\n-1.0762765\nwhite\n\n\n23946\n7\n0.4108439\nLA\n-1.1435437\nwhite\n\n\n40109\n166\n0.6797305\nOR\n0.9417419\nwhite\n\n\n48028\n235\n1.8819894\nWA\n1.2108110\nhispanic\n\n\n19427\n345\n-0.0639433\nKS\n-0.9417419\nwhite\n\n\n13537\n329\n0.8603494\nGA\n-0.6054055\nwhite\n\n\n11346\n132\n-0.0797059\nFL\n-0.4036037\nwhite\n\n\n4111\n498\n1.3826098\nCA\n0.8072074\nblack"
  },
  {
    "objectID": "examples/dag_sim_data.html#exogenous-nodes",
    "href": "examples/dag_sim_data.html#exogenous-nodes",
    "title": "Data Simulation",
    "section": "Exogenous Nodes",
    "text": "Exogenous Nodes\nNow every patient has a state, a provider, a race/ethnicity, and some associated data by state.\nIn addition to race/ethnicity and state, the following variables have no prior causes in our model:\n\nAGE: Age of the patient.\nPCE: Parent community connections\nPED: Parent education\nPI: Parent intelligence\nPM: Parent motivation\nPSR: Parent resilience\n\nWe will generate n observations for each of these variables.\n\n## Next generate all the other exogenous variables\n# We need to assign a provider to each patient. This provider needs a quality\ndata &lt;- data |&gt;\n  mutate(\n    age = gen_mother_ages(n),\n    parent_income = gen_incomes(\n      n,\n      median_income = 60000,\n      sd = 25000,\n      min_income = 0,\n      max_income = 1000000\n    ),\n    parent_intelligence = rnorm(n, 1, 1),\n    parent_resilience = rnorm(n, 1, 1),\n    parent_motivation = rnorm(n, 1, 1),\n    parent_community_connections = rnorm(n, 0, 1),\n    parent_edu = gen_education(n, ed_cats, parent_intelligence, parent_resilience, parent_motivation, parent_community_connections, parent_income),\n  )\n\ndata |&gt; sample_n(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nprovider_id\nprovider_quality\nstate\npec\nrace\nage\nparent_income\nparent_intelligence\nparent_resilience\nparent_motivation\nparent_community_connections\nparent_edu\n\n\n\n\n7159\n499\n1.4485962\nCA\n0.8072074\nhispanic\n37\n83835\n0.8858408\n1.0123769\n1.4085381\n-0.5220990\ncollege\n\n\n5807\n402\n0.7758654\nCA\n0.8072074\nwhite\n28\n69278\n1.3889968\n-1.2610415\n1.0793617\n0.4408008\nsome_college\n\n\n31790\n24\n1.4961803\nND\n-0.2018018\nhispanic\n32\n77305\n0.9154918\n1.5764111\n-0.2299443\n0.1175259\nhs\n\n\n18204\n435\n-0.5896960\nIN\n-0.4708710\nwhite\n30\n58598\n1.7420333\n2.4338768\n0.2793851\n0.9653005\nhs\n\n\n11626\n325\n0.6065428\nFL\n-0.4036037\nwhite\n15\n37888\n0.3562121\n2.4555014\n1.9506465\n-0.5790530\ncollege\n\n\n8967\n340\n0.8389976\nCO\n0.8744746\nwhite\n29\n75478\n2.8821099\n0.4545521\n1.6305025\n0.6643257\npost_grad\n\n\n44865\n379\n0.4294213\nTX\n-1.0090092\nwhite\n20\n47858\n2.7452877\n0.6031012\n1.0796006\n-1.2616087\nsome_college\n\n\n45511\n288\n1.1893308\nUT\n-0.1345346\nwhite\n27\n55546\n0.9176252\n0.9465321\n0.9966425\n-0.0883087\nsome_college\n\n\n23474\n373\n1.4916928\nKY\n-0.2690691\nwhite\n32\n102351\n0.6764381\n-0.7451855\n0.5961781\n-2.2135001\nhs\n\n\n36296\n103\n3.0029529\nNY\n1.0762765\nhispanic\n29\n51285\n1.3534419\n0.5033919\n0.6455139\n-0.6679477\nhs"
  },
  {
    "objectID": "examples/dag_sim_data.html#working-our-way-up-from-the-root-nodes",
    "href": "examples/dag_sim_data.html#working-our-way-up-from-the-root-nodes",
    "title": "Data Simulation",
    "section": "Working our way up from the root nodes",
    "text": "Working our way up from the root nodes\nReligion is patterned by race but has no other determinants.\n\ndata &lt;- data |&gt; mutate(\n  religion = gen_religion(n, rel_cats, race),\n)\n\ndata |&gt; select(id, race, religion) |&gt; sample_n(10) |&gt; kable()\n\n\n\n\nid\nrace\nreligion\n\n\n\n\n21701\nwhite\nbuddhist\n\n\n41238\nwhite\nchristian\n\n\n4063\nhispanic\nchristian\n\n\n16353\nwhite\nbuddhist\n\n\n36146\nasian\nmuslim\n\n\n48167\nhispanic\nother\n\n\n24696\nhispanic\nchristian\n\n\n42616\nhispanic\nchristian\n\n\n28443\nwhite\nchristian\n\n\n14401\nwhite\nother\n\n\n\n\n\nThe following variables are determined by parents’ values:\n\nI: Intelligence\nSR: Resilience\nM: Motivation\nCE: Community connections\n\n\ndata &lt;- data |&gt; mutate(\n  intelligence = gen_correlated(parent_intelligence, target_r = 0.5),\n  resilience = gen_correlated(parent_resilience, target_r = 0.5),\n  motivation = gen_correlated(parent_motivation, target_r = 0.5),\n  community_connections = gen_correlated(parent_community_connections, target_r = 0.5),\n)\n\ndata |&gt; select(id, parent_intelligence, intelligence, parent_resilience, resilience, parent_motivation, motivation, parent_community_connections, community_connections) |&gt; sample_n(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nparent_intelligence\nintelligence\nparent_resilience\nresilience\nparent_motivation\nmotivation\nparent_community_connections\ncommunity_connections\n\n\n\n\n5832\n2.9798657\n0.2401072\n2.1595536\n-1.0990314\n2.5218719\n-0.7420373\n-2.0400209\n-1.2128006\n\n\n12339\n-1.2206366\n-0.6655558\n1.8248700\n-0.2171290\n1.9038107\n0.3451989\n-0.6456990\n-1.5888214\n\n\n46586\n0.2482648\n-0.9834769\n-0.3861422\n-1.5187399\n0.1030061\n-0.8032744\n0.5423317\n2.2537155\n\n\n39304\n1.7775938\n-0.0329765\n2.2116305\n1.5844718\n2.1305360\n1.1693009\n0.4023256\n2.4011625\n\n\n28315\n1.8078598\n1.1789372\n2.2625768\n0.5034031\n1.2105333\n-0.5901088\n0.7128334\n-0.3137916\n\n\n39186\n0.4624403\n0.1088728\n0.1491391\n-0.8964719\n0.3206928\n-0.4329646\n0.1327722\n0.1838676\n\n\n44811\n0.0854350\n-1.2365915\n0.2464217\n-0.0511363\n0.0787397\n-2.0609027\n-0.8757560\n-0.0142037\n\n\n35777\n0.3139661\n0.6974896\n1.2434126\n-1.0902090\n-0.2561044\n-1.0150626\n-0.5165110\n-0.6145779\n\n\n6068\n1.7037640\n-0.0315442\n0.6858329\n-2.1535645\n0.1734255\n0.2305217\n-0.7604174\n-1.4004243\n\n\n28250\n0.1783547\n0.2140070\n-0.0337409\n-1.6174525\n0.2229554\n-0.3362930\n1.5606381\n0.7522387\n\n\n\n\n\nThe following variables are determined by a mixture personality, geography, and parents’ values:\n\nEducation (varies by personal traits and parents’ class position)\nCultural orientation (namely, trust of institutions; varies by parents class position, religion, geography, and commu nity connections)\nJob type (varies by educational attainment and parental income)\nDependents (varies by income, job type, and age)\nInsurance (varies by job type, state conditions, age)\nDistance to provider (varies by state conditions)\n\n\ndata &lt;- data |&gt; mutate(\n  edu = gen_education(n, ed_cats, intelligence, resilience, motivation, community_connections, parent_income, parent_edu),\n  income = gen_incomes(\n    n,\n    median_income = 60000,\n    sd = 25000,\n    min_income = 0,\n    max_income = 1000000\n  ),\n  cultural_orientation = gen_cultural_orientation(n, parent_income, parent_edu, pec, religion, community_connections),\n  job_type = gen_job_type(n, job_type_cats, ed_cats, edu, parent_income),\n  dependents = gen_dependents(n, income, job_type, age),\n  insurance = gen_insurance(n, job_type_cats, job_type, pec, age),\n  distance_to_provider = gen_distance(n, pec),\n)\n\ndata |&gt; select(id, edu, income, cultural_orientation, job_type, dependents, insurance, distance_to_provider) |&gt; sample_n(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nedu\nincome\ncultural_orientation\njob_type\ndependents\ninsurance\ndistance_to_provider\n\n\n\n\n41485\nsome_college\n32837\n0.7009329\nunskilled\n3\nno_insurance\n12.138941\n\n\n40967\nsome_college\n54401\n0.6777706\nprofessional\n2\nprivate\n30.907752\n\n\n16425\nhs\n100805\n0.6029632\nunskilled\n2\nno_insurance\n9.886199\n\n\n14762\nsome_college\n81724\n0.4519062\nunskilled\n2\nno_insurance\n22.685236\n\n\n30698\nsome_college\n61147\n0.5758929\nunskilled\n3\nstate_provided\n19.303343\n\n\n2341\nhs\n64934\n0.2634316\nunemployed\n2\nno_insurance\n17.753388\n\n\n18707\nhs\n66055\n0.5644650\nunskilled\n2\nprivate\n15.492048\n\n\n38541\nhs\n53720\n0.2758859\nunskilled\n2\nno_insurance\n23.523255\n\n\n1962\nless_than_hs\n125211\n0.4950062\nunskilled\n2\nno_insurance\n3.515861\n\n\n39107\nhs\n125569\n0.5736693\nunskilled\n3\nno_insurance\n7.108544\n\n\n\n\n\nComorbidity is determined by age, SES, and other comorbidities.\n\nObesity (varies by state conditions, age)\nMultiple gestation (varies by age, obesity)\nDiabetes (varies by age, obesity, income)\nHeart disease (varies by age, obesity, diabetes)\nPlacenta previa (varies by multiple gestation)\nHypertension (varies by age, obesity)\nGestational hypertension (varies by hypertension, multiple gestation)\nPreeclampsia (varies by age, hypertension, gestational hypertension, multiple gestation)\n\n\ndata &lt;- data |&gt; mutate(\n  obesity = gen_obesity(n, income, edu, pec, age, target_prevalence=0.35),\n  multiple_gestation = gen_multiple_gestation(n, age, obesity, target_prevalence = 0.03),\n  diabetes = gen_diabetes(n, age, obesity, income, target_prevalence = 0.1),\n  heart_disease = gen_heart_disease(n, age, obesity, diabetes, target_prevalence = 0.15),\n  placenta_previa = gen_placenta_previa(n, age, multiple_gestation, target_prevalence = 0.01),\n  hypertension = gen_hypertension(n, age, obesity, target_prevalence = 0.2),\n  gest_hypertension = gen_gest_hypertension(n, hypertension, multiple_gestation, target_prevalence = 0.05),\n  preeclampsia = gen_preeclampsia(n, age, hypertension, gest_hypertension, multiple_gestation, target_prevalence = 0.02),\n)\n\ndata |&gt; select(id, obesity, multiple_gestation, diabetes, heart_disease, placenta_previa, hypertension, gest_hypertension, preeclampsia) |&gt; sample_n(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nobesity\nmultiple_gestation\ndiabetes\nheart_disease\nplacenta_previa\nhypertension\ngest_hypertension\npreeclampsia\n\n\n\n\n45418\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n31167\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n22614\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n10498\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n26491\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n38009\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n48549\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n43809\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n15703\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n7999\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "examples/dag_sim_data.html#immediate-causes-of-receipt-of-comprehensive-postnatal-care",
    "href": "examples/dag_sim_data.html#immediate-causes-of-receipt-of-comprehensive-postnatal-care",
    "title": "Data Simulation",
    "section": "Immediate causes of receipt of comprehensive postnatal care",
    "text": "Immediate causes of receipt of comprehensive postnatal care\nProvider quality is determined by state conditions and already calculated.\nPersonal capacity (to attend visits) is determined by:\n\ndependents\njob type\nincome\ndistance to provider\n\n\ndata &lt;- data |&gt; mutate(\n  personal_capacity = gen_personal_capacity(n, dependents, job_type, income, distance_to_provider)\n)\n\nNow generate the risk profile, which is a function of:\n\nprovider quality (negatively correlated)\nage\nobesity\nmultiple gestation\ndiabetes\nheart disease\nplacenta previa\nhypertension\ngestational hypertension\npreeclampsia\n\n\ndata &lt;- data |&gt; mutate(\n  risk_profile = gen_risk_profile(n, provider_quality = providers$quality[match(data$provider_id, providers$id)], \n                                 age, obesity, multiple_gestation, diabetes, heart_disease, placenta_previa, hypertension, gest_hypertension, preeclampsia)\n)\n\nNow generate risk aversion, which we see as a function of the negative conseuqences from getting really sick and how much people want to avoid them.\n\ninsurance (people without insurance will be more risk averse because of cost; positively correlated)\nprovider quality (people with better providers will be less risk averse since they trust the care they can get; negatively correlated)\nrisk profile (people with higher risk will be more risk averse since they will have more consequences if getting ill; positively correlated)\n\n\ndata &lt;- data |&gt; mutate(\n  risk_aversion = gen_risk_aversion(n, insurance, provider_quality = providers$quality[match(data$provider_id, providers$id)], risk_profile)\n)\n\nProvider trust is a function of:\n\nrace/ethnicity (racial minorities are less likely to trust providers; negatively correlated)\nprovider quality (people with better providers will be more likely to trust them; positively correlated)\ncultural orientation (people trusting institutions will be more likely to trust providers; positively correlated)\n\n\ndata &lt;- data |&gt; mutate(\n  provider_trust = gen_provider_trust(n, re_cats, race, provider_quality, cultural_orientation)\n)\n\nAnd finally, willingness to pay, which we see as a function of:\n\nprovider quality (people with better providers will be willing to pay more; positively correlated)\nincome (people with higher income will be willing to pay more; positively correlated)\ninsurance (people with insurance will be willing to pay more since they don’t have to cover it out of pocket; positively correlated)\nrisk aversion (people who are more risk averse will be willing to pay more; positively correlated)\ncultural orientation (people who trust institutions will be willing to pay more; positively correlated)\n\n\ndata &lt;- data |&gt; mutate(\n  willingness_to_pay = gen_willingness_to_pay(n, provider_quality = providers$quality[match(data$provider_id, providers$id)], income, insurance, risk_aversion, cultural_orientation)\n)\n\nFinally, we generate the outcome of interest, receipt of comprehensive postnatal care, which is a function of:\n\npersonal capacity (people with more capacity will be more likely to attend visits; positively correlated)\nwillingness to pay (people who are willing to pay more will be more likely to attend visits; positively correlated)\nprovider quality (people with better providers will be more likely to attend visits; positively correlated)\nprovider trust (people who trust their providers will be more likely to attend visits; positively correlated)\nrisk aversion (people who are more risk averse will be more likely to attend visits; negatively correlated)\nrisk profile (people with higher risk will be more likely to attend visits; positively correlated)\n\n\ndata &lt;- data |&gt; \n  mutate(\n    received_comprehensive_postnatal_care = gen_received_comprehensive_postnatal_care(n, personal_capacity, willingness_to_pay, provider_quality, provider_trust, risk_aversion, risk_profile)\n  )\n\nWe’re going to include an income variable, but it’s going to be censored and have measurement error.\n\nsri_labels &lt;- c(\"$0–$24,999\", \"$25,000–$49,999\", \"$50,000–$74,999\", \"$75,000–$99,999\", \"$100,000–$124,999\", \"$125,000–$149,999\", \"$150,000–$174,999\", \"$175,000+\")\n\ndata &lt;- data |&gt; mutate(\n  self_report_income = cut(\n      pmin(pmax(income + rnorm(n, mean = 0, sd = 5000), 0), 200000), # Add noise and cap\n      breaks = seq(0, 200000, by = 25000),\n      include.lowest = TRUE,\n      right = FALSE,\n      labels = sri_labels\n    )\n)\n\ntable(data$self_report_income)\n\n\n       $0–$24,999   $25,000–$49,999   $50,000–$74,999   $75,000–$99,999 \n              608             14589             21430              9720 \n$100,000–$124,999 $125,000–$149,999 $150,000–$174,999         $175,000+ \n             2747               697               165                44 \n\nanalysis_data &lt;- data |&gt; select(\n  id,\n  provider_id,\n  state,\n  received_comprehensive_postnatal_care,\n  self_report_income,\n  age,\n  edu,\n  race,\n  insurance,\n  job_type,\n  dependents,\n  distance_to_provider,\n  obesity,\n  multiple_gestation,\n  diabetes,\n  heart_disease,\n  placenta_previa,\n  hypertension,\n  gest_hypertension,\n  preeclampsia\n) |&gt;\n  mutate(\n    self_report_income = as.character(self_report_income),\n    job_type = as.character(job_type),\n    dependents = as.character(dependents),\n  ) |&gt; \n  rename(\n    race_ethnicity = race\n  )\n\nreadr::write_csv(analysis_data, here::here(\"data/processed/simulated_data.csv\"))"
  },
  {
    "objectID": "examples/applications_5_visualizing_time_trends.html",
    "href": "examples/applications_5_visualizing_time_trends.html",
    "title": "Application 5: Time Trends",
    "section": "",
    "text": "Time series plots are a common way to visualize data over time. They are useful for showing trends and patterns in data over time.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate time series data\nn_weeks &lt;- 104  # 2 years of weekly data\nintervention_week &lt;- 52  # Intervention at 1 year\n\n# Create base data\ntime_data &lt;- tibble(\n  week = 1:n_weeks,\n  date = as.Date(\"2023-01-01\") + weeks(week - 1),\n  # Base level (flat)\n  base_level = 100,\n  # Random noise\n  noise = rnorm(n_weeks, mean = 0, sd = 10),\n  # Intervention effect with decay\n  intervention = ifelse(\n    week &gt;= intervention_week,\n    30 * exp(-0.02 * (week - intervention_week)),  # Exponential decay\n    0\n  ),\n  # Combine components\n  value = base_level + noise + intervention\n)\n\n# Add gender effect for later use\ntime_data &lt;- time_data |&gt;\n  crossing(gender = c(\"Male\", \"Female\")) |&gt;\n  mutate(\n    # Add gender-specific effects\n    gender_effect = ifelse(gender == \"Female\", -15, 15),\n    # Add gender-specific intervention effects with decay\n    gender_intervention = ifelse(\n      week &gt;= intervention_week,\n      ifelse(\n        gender == \"Female\",\n        40 * exp(-0.02 * (week - intervention_week)),  # Larger effect for females\n        20 * exp(-0.02 * (week - intervention_week))\n      ),\n      0\n    ),\n    # Combine all effects\n    value_gender = value + gender_effect + gender_intervention\n  )\n\n\n# Create basic time series plot\np_time &lt;-\n\n\nggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add line\n  geom_line(\n    color = \"gray30\",\n    linewidth = 0.5\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"red\",\n    alpha = 0.5\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time\",\n    subtitle = \"Intervention implemented at 1 year (dashed line)\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  )\n\np_time"
  },
  {
    "objectID": "examples/applications_5_visualizing_time_trends.html#time-series-plots",
    "href": "examples/applications_5_visualizing_time_trends.html#time-series-plots",
    "title": "Application 5: Time Trends",
    "section": "",
    "text": "Time series plots are a common way to visualize data over time. They are useful for showing trends and patterns in data over time.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate time series data\nn_weeks &lt;- 104  # 2 years of weekly data\nintervention_week &lt;- 52  # Intervention at 1 year\n\n# Create base data\ntime_data &lt;- tibble(\n  week = 1:n_weeks,\n  date = as.Date(\"2023-01-01\") + weeks(week - 1),\n  # Base level (flat)\n  base_level = 100,\n  # Random noise\n  noise = rnorm(n_weeks, mean = 0, sd = 10),\n  # Intervention effect with decay\n  intervention = ifelse(\n    week &gt;= intervention_week,\n    30 * exp(-0.02 * (week - intervention_week)),  # Exponential decay\n    0\n  ),\n  # Combine components\n  value = base_level + noise + intervention\n)\n\n# Add gender effect for later use\ntime_data &lt;- time_data |&gt;\n  crossing(gender = c(\"Male\", \"Female\")) |&gt;\n  mutate(\n    # Add gender-specific effects\n    gender_effect = ifelse(gender == \"Female\", -15, 15),\n    # Add gender-specific intervention effects with decay\n    gender_intervention = ifelse(\n      week &gt;= intervention_week,\n      ifelse(\n        gender == \"Female\",\n        40 * exp(-0.02 * (week - intervention_week)),  # Larger effect for females\n        20 * exp(-0.02 * (week - intervention_week))\n      ),\n      0\n    ),\n    # Combine all effects\n    value_gender = value + gender_effect + gender_intervention\n  )\n\n\n# Create basic time series plot\np_time &lt;-\n\n\nggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add line\n  geom_line(\n    color = \"gray30\",\n    linewidth = 0.5\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"red\",\n    alpha = 0.5\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time\",\n    subtitle = \"Intervention implemented at 1 year (dashed line)\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  )\n\np_time"
  },
  {
    "objectID": "examples/applications_5_visualizing_time_trends.html#time-series-plots-with-smoothers",
    "href": "examples/applications_5_visualizing_time_trends.html#time-series-plots-with-smoothers",
    "title": "Application 5: Time Trends",
    "section": "Time Series Plots with Smoothers",
    "text": "Time Series Plots with Smoothers\nSmoothers are a common way to visualize trends in data over time. Below, we use a loess smoother to visualize the trend in the data.\n\n# Create time series plot with smoother\np_time_smooth &lt;- ggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add smoother\n  geom_smooth(\n    method = \"loess\",\n    span = 0.3,\n    color = \"blue\",\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"red\",\n    alpha = 0.5\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time with Smoother\",\n    subtitle = \"Intervention implemented at 1 year (dashed line)\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  )\n\np_time_smooth"
  },
  {
    "objectID": "examples/applications_5_visualizing_time_trends.html#interrupted-time-series-plots",
    "href": "examples/applications_5_visualizing_time_trends.html#interrupted-time-series-plots",
    "title": "Application 5: Time Trends",
    "section": "Interrupted Time Series Plots",
    "text": "Interrupted Time Series Plots\nInterrupted time series plots are a common way to visualize the effect of an intervention on a time series. Below, we fit a linear model to the data before and after the intervention and visualize the trend in the data before and after the intervention. We annotate the plot with the average weight before and after the intervention. This specific example a “regression discontinuity” design.\n\n# Calculate averages for annotation\navg_before &lt;- time_data |&gt;\n  filter(date &lt; as.Date(\"2024-01-01\")) |&gt;\n  summarise(avg = mean(value)) |&gt;\n  mutate(\n    x = as.Date(\"2023-07-01\"),\n    label = sprintf(\"Avg: %.0f min\", avg)\n  )\n\navg_after &lt;- time_data |&gt;\n  filter(date &gt;= as.Date(\"2024-01-01\")) |&gt;\n  summarise(avg = mean(value)) |&gt;\n  mutate(\n    x = as.Date(\"2024-07-01\"),\n    label = sprintf(\"Avg: %.0f min\", avg)\n  )\n\n# Create time series plot with separate linear models\np_time_lm &lt;- ggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add linear model for pre-intervention\n  geom_smooth(\n    data = filter(time_data, date &lt; as.Date(\"2024-01-01\")),\n    method = \"lm\",\n    color = \"blue\",\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add linear model for post-intervention\n  geom_smooth(\n    data = filter(time_data, date &gt;= as.Date(\"2024-01-01\")),\n    method = \"lm\",\n    color = \"red\",\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"black\",\n    alpha = 0.5\n  ) +\n  # Add arrow and annotation\n  annotate(\n    \"segment\",\n    x = as.Date(\"2023-11-15\"),\n    xend = as.Date(\"2024-01-01\"),\n    y = 150,\n    yend = 150,\n    arrow = arrow(length = unit(0.2, \"cm\"), ends = \"last\", type = \"closed\"),\n    color = \"red\"\n  ) +\n  annotate(\n    \"text\",\n    x = as.Date(\"2023-11-15\"),\n    y = 150,\n    label = \"Intervention\\nStarted\",\n    hjust = 1.1,\n    color = \"red\",\n    fontface = \"bold\"\n  ) +\n  # Add average annotations\n  geom_text(\n    data = avg_before,\n    aes(x = x, y = avg, label = label),\n    hjust = 0.5,\n    vjust = -6,\n    fontface = \"bold\",\n    color = \"black\"\n  ) +\n  geom_text(\n    data = avg_after,\n    aes(x = x, y = avg, label = label),\n    hjust = 0.5,\n    vjust = -6,\n    fontface = \"bold\",\n    color = \"black\"\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time\",\n    subtitle = \"Linear trends before and after intervention\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_y_continuous(\n    limits = c(0, 180)\n  )\n\np_time_lm"
  },
  {
    "objectID": "examples/applications_5_visualizing_time_trends.html#using-labels-shading-and-annotations",
    "href": "examples/applications_5_visualizing_time_trends.html#using-labels-shading-and-annotations",
    "title": "Application 5: Time Trends",
    "section": "Using Labels, Shading, and Annotations",
    "text": "Using Labels, Shading, and Annotations\nWe can use labels, shading, and annotations to add additional information to our plots.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate dates for 4 years\ndates &lt;- seq.Date(\n  from = as.Date(\"2020-01-01\"),\n  to = as.Date(\"2023-12-31\"),\n  by = \"day\"\n)\n\n# Generate person IDs and their base characteristics\nn_people &lt;- 300\nperson_data &lt;- tibble(\n  person_id = 1:n_people,\n  gender = sample(c(\"Male\", \"Female\"), n_people, replace = TRUE),\n  # Base weight varies by person (in pounds)\n  base_weight = case_when(\n    gender == \"Male\" ~ rnorm(n_people, mean = 176, sd = 15),  # Mean 176 lbs, SD 15\n    TRUE ~ rnorm(n_people, mean = 143, sd = 12)               # Mean 143 lbs, SD 12\n  ),\n  # Individual seasonal sensitivity\n  seasonal_sensitivity = rnorm(n_people, mean = 1, sd = 0.2)  # Some people more sensitive to seasons\n)\n\n# Create base data frame with all person-dates\nweight_data &lt;- crossing(\n  date = dates,\n  person_id = 1:n_people\n) |&gt;\n  left_join(person_data, by = \"person_id\") |&gt;\n  mutate(\n    month = month(date),\n    day_of_year = yday(date),\n    # Small seasonal effect (2-3 pounds total)\n    seasonal_effect = case_when(\n      month %in% c(12, 1, 2) ~ 2.5 * sin((day_of_year - 15) * 2 * pi / 365),  # Winter peak\n      month %in% c(3, 4, 5) ~ 1.5 * sin((day_of_year - 15) * 2 * pi / 365),   # Spring\n      month %in% c(6, 7, 8) ~ 0,                                              # Summer\n      TRUE ~ 2.0 * sin((day_of_year - 15) * 2 * pi / 365)                     # Fall\n    ),\n    # Apply individual sensitivity to seasonal effect\n    seasonal_effect = seasonal_effect * seasonal_sensitivity,\n    # Add very small random noise (0.2 lbs)\n    noise = rnorm(n(), 0, 0.2),\n    # Calculate final weight\n    weight = base_weight + seasonal_effect + noise\n  )\n\n# Calculate daily averages by gender\ndaily_averages &lt;- weight_data |&gt;\n  group_by(date, gender) |&gt;\n  summarise(\n    avg_weight = mean(weight),\n    .groups = \"drop\"\n  )\n\n# Create annotation data frame\nannotation_dates &lt;- c(\n  as.Date(\"2020-11-01\"), as.Date(\"2020-12-31\"),\n  as.Date(\"2021-11-01\"), as.Date(\"2021-12-31\"),\n  as.Date(\"2022-11-01\"), as.Date(\"2022-12-31\"),\n  as.Date(\"2023-11-01\"), as.Date(\"2023-12-31\")\n)\n\nannotations &lt;- daily_averages |&gt;\n  filter(date %in% annotation_dates) |&gt;\n  mutate(\n    label = sprintf(\"%.1f\", avg_weight),\n    text_x = case_when(\n      month(date) == 11 ~ date - 56,  # Dodge left for November\n      TRUE ~ date + 56                # Dodge right for December\n    ),\n    text_y = case_when(\n      gender == \"Female\" ~ 150,  # Fixed position for men (lowered from 155)\n      TRUE ~ 185               # Fixed position for women\n    )\n  )\n\n# Create time series plot\np_weight &lt;- ggplot(daily_averages, aes(x = date, y = avg_weight, color = gender)) +\n  # Add holiday season shading\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2020-11-01\"),\n    xmax = as.Date(\"2020-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2021-11-01\"),\n    xmax = as.Date(\"2021-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2022-11-01\"),\n    xmax = as.Date(\"2022-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2023-11-01\"),\n    xmax = as.Date(\"2023-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  # Add vertical lines for Nov 1 and Dec 31\n  geom_vline(\n    data = annotations,\n    aes(xintercept = date),\n    linewidth = 0.3,\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  # Add smoothed lines\n  geom_smooth(\n    method = \"loess\",\n    span = 0.1,\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add text labels\n  geom_text(\n    data = annotations,\n    aes(\n      x = text_x,\n      y = text_y,\n      label = label,\n      color = gender\n    ),\n    size = 3,\n    show.legend = FALSE\n  ) +\n  # Add labels\n  labs(\n    title = \"Average Weight Over Time by Gender\",\n    x = NULL,\n    y = \"Weight (lbs)\",\n    caption = \"Shaded area indicates holiday season (November 1 - December 31).\\nText annotations show average weights on November 1 and December 31.\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.title.position = \"plot\",\n    plot.caption = element_text(size = 8, hjust = 0, margin = margin(t = 10)),\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.title = element_blank()\n  ) +\n  # Extend y-axis range\n  scale_y_continuous(\n    limits = c(120, 220),\n    breaks = seq(120, 220, by = 20)\n  ) +\n  # Use different colors for genders\n  scale_color_manual(\n    values = c(\n      \"Male\" = \"red\",\n      \"Female\" = \"darkblue\"\n    ),\n    labels = c(\"Women\", \"Men\")\n  ) +\n  # Remove rect from legend and fix the \"a\" issue\n  guides(color = guide_legend(override.aes = list(shape = NA, fill = NA, linetype = 1, label = NULL, text = NULL)))\n\np_weight"
  },
  {
    "objectID": "examples/applications_5_visualizing_time_trends.html#sankey-diagrams",
    "href": "examples/applications_5_visualizing_time_trends.html#sankey-diagrams",
    "title": "Application 5: Time Trends",
    "section": "Sankey Diagrams",
    "text": "Sankey Diagrams\nSankey diagrams are a common way to visualize the flow of data. Below, we create a Sankey diagram to visualize simulated data of patients with hypertension who change treatments over time.\n\n# Create treatment pattern data\nset.seed(123)\n\n# Define possible treatments (common hypertension medications)\ntreatments &lt;- c(\"ACE Inhibitor\", \"ARB\", \"CCB\", \"No Treatment\")\n\n# Generate patient data with treatment changes\nn_patients &lt;- 1000\npatient_data &lt;- tibble(\n  patient_id = 1:n_patients,\n  initial_treatment = sample(treatments, n_patients, replace = TRUE,\n                           prob = c(0.4, 0.3, 0.2, 0.1)),  # ACE inhibitors most common first-line\n  # 60% of patients change treatment at least once\n  has_change = rbinom(n_patients, 1, 0.6),\n  # For those who change, determine second treatment\n  second_treatment = case_when(\n    has_change == 1 ~ sample(treatments, n_patients, replace = TRUE,\n                           prob = c(0.3, 0.3, 0.3, 0.1)),\n    TRUE ~ initial_treatment\n  ),\n  # 30% of patients who changed once change again\n  has_second_change = case_when(\n    has_change == 1 ~ rbinom(n_patients, 1, 0.3),\n    TRUE ~ 0\n  ),\n  # For those who change twice, determine third treatment\n  third_treatment = case_when(\n    has_second_change == 1 ~ sample(treatments, n_patients, replace = TRUE,\n                                  prob = c(0.2, 0.2, 0.2, 0.4)),\n    TRUE ~ second_treatment\n  )\n)\n\n# Create Sankey diagram data\n# Create nodes data frame\nnodes &lt;- data.frame(\n  name = c(\n    treatments,  # First column\n    treatments,  # Second column\n    treatments   # Third column\n  )\n)\n\n# Create links data frame\nlinks &lt;- patient_data |&gt;\n  count(initial_treatment, second_treatment, third_treatment) |&gt;\n  filter(n &gt; 0) |&gt;\n  mutate(\n    source = match(initial_treatment, nodes$name) - 1,\n    target = match(second_treatment, nodes$name) - 1 + length(treatments),\n    value = n\n  ) |&gt;\n  select(source, target, value)\n\n# Add second stage links\nlinks_second &lt;- patient_data |&gt;\n  count(second_treatment, third_treatment) |&gt;\n  filter(n &gt; 0) |&gt;\n  mutate(\n    source = match(second_treatment, nodes$name) - 1 + length(treatments),\n    target = match(third_treatment, nodes$name) - 1 + 2 * length(treatments),\n    value = n\n  ) |&gt;\n  select(source, target, value)\n\nlinks &lt;- bind_rows(links, links_second)\n\n# Create Sankey diagram\np_sankey &lt;- sankeyNetwork(\n  Links = links,\n  Nodes = nodes,\n  Source = \"source\",\n  Target = \"target\",\n  Value = \"value\",\n  NodeID = \"name\",\n  fontSize = 12,\n  nodeWidth = 30,\n  sinksRight = FALSE\n)\n\n# Add title using HTML\np_sankey &lt;- htmlwidgets::prependContent(\n  p_sankey,\n  htmltools::tags$h3(\"Hypertension Treatment Pattern Flow Over Time\")\n)\n\np_sankey\n\nHypertension Treatment Pattern Flow Over Time"
  },
  {
    "objectID": "examples/applications_5_visualizing_time_trends.html#sunburst-plots",
    "href": "examples/applications_5_visualizing_time_trends.html#sunburst-plots",
    "title": "Application 5: Time Trends",
    "section": "Sunburst Plots",
    "text": "Sunburst Plots\nSunburst plots are another common way to visualize the flow of data. Below, we create a Sunburst plot to visualize the distribution of patients with hypertension who change treatments over time.\nThese show the distribution of patients by treatment at each stage of the process, using color to indicate the treatment at each stage.\n\n# Create Sunburst plot data\n# Create properly formatted sunburst data with three levels\nsunburst_data &lt;- patient_data |&gt;\n  # Create paths for each treatment stage\n  mutate(\n    ids = paste0(\"initial_\", initial_treatment),\n    labels = initial_treatment,\n    parents = \"\",\n    values = 1,\n    level = \"initial\"\n  ) |&gt;\n  bind_rows(\n    patient_data |&gt;\n      filter(has_change == 1) |&gt;\n      mutate(\n        ids = paste0(\"second_\", initial_treatment, \"_\", second_treatment),\n        labels = second_treatment,\n        parents = paste0(\"initial_\", initial_treatment),\n        values = 1,\n        level = \"second\"\n      )\n  ) |&gt;\n  bind_rows(\n    patient_data |&gt;\n      filter(has_second_change == 1) |&gt;\n      mutate(\n        ids = paste0(\"third_\", initial_treatment, \"_\", second_treatment, \"_\", third_treatment),\n        labels = third_treatment,\n        parents = paste0(\"second_\", initial_treatment, \"_\", second_treatment),\n        values = 1,\n        level = \"third\"\n      )\n  ) |&gt;\n  group_by(ids, labels, parents, level) |&gt;\n  summarise(values = sum(values), .groups = \"drop\")\n\n# Create color scales for each level\ninitial_colors &lt;- viridis::viridis(4)\nsecond_colors &lt;- viridis::viridis(4, begin = 0.3, end = 0.7)\nthird_colors &lt;- viridis::viridis(4, begin = 0.6, end = 1)\n\n# Create Sunburst plot using plotly\np_sunburst &lt;- plot_ly(\n  sunburst_data,\n  ids = ~ids,\n  labels = ~labels,\n  parents = ~parents,\n  values = ~values,\n  type = \"sunburst\",\n  branchvalues = \"total\",\n  marker = list(\n    colors = case_when(\n      sunburst_data$level == \"initial\" ~ initial_colors[match(sunburst_data$labels, treatments)],\n      sunburst_data$level == \"second\" ~ second_colors[match(sunburst_data$labels, treatments)],\n      sunburst_data$level == \"third\" ~ third_colors[match(sunburst_data$labels, treatments)]\n    )\n  )\n) |&gt;\n  layout(\n    title = \"Hypertension Treatment Pattern Distribution\",\n    width = 800,\n    height = 800\n  )\n\n# Display plots\np_sunburst"
  },
  {
    "objectID": "examples/prams_1_data_prep.html",
    "href": "examples/prams_1_data_prep.html",
    "title": "Data Preparation Example",
    "section": "",
    "text": "We use CDC’s PRAMStat data for 2011."
  },
  {
    "objectID": "examples/prams_1_data_prep.html#data-source",
    "href": "examples/prams_1_data_prep.html#data-source",
    "title": "Data Preparation Example",
    "section": "",
    "text": "We use CDC’s PRAMStat data for 2011."
  },
  {
    "objectID": "examples/prams_1_data_prep.html#note",
    "href": "examples/prams_1_data_prep.html#note",
    "title": "Data Preparation Example",
    "section": "Note",
    "text": "Note\nIn this file, I use exposition to explain what I am doing and what I am thinking. In this context, this is an unknown data set we’re working with and I am trying to teach others.\nFor a project orreport, you would want to first read the data file documentation and codebook first to understand the data. Your commentary in the notebook should be targeted to the audience, conveying everything required for people to understand the steps you took and why you took them.\nThis data is aggregated at the location level and comes to us in “long” format."
  },
  {
    "objectID": "examples/prams_1_data_prep.html#loading-required-libraries",
    "href": "examples/prams_1_data_prep.html#loading-required-libraries",
    "title": "Data Preparation Example",
    "section": "Loading Required Libraries",
    "text": "Loading Required Libraries\nWe first need to load in the libraries we’ll use.\nWe will use:\n\nreadr to load in the data ( I prefer readr over the built-in read.csv function because it is faster and has better default settings for avoiding common issues with CSVs.)\ndplyr to clean data.\n\nkableExtra is used to make tables look nice.\nDT to make tables interactive and get nice pagination on long tables.\ntidyr to pivot the data to wide format.\n\n\n# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"DT\", \"kableExtra\", \"readr\", \"tidyr\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(DT)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(tidyr)\n\nTo hide this code from the output, I typically prepend the above code with.\n#| echo: false\n#| message: false\nHere, for illustration, I hide messages but not the code block."
  },
  {
    "objectID": "examples/prams_1_data_prep.html#read-and-explore-the-data",
    "href": "examples/prams_1_data_prep.html#read-and-explore-the-data",
    "title": "Data Preparation Example",
    "section": "Read and Explore the Data",
    "text": "Read and Explore the Data\nHere we read the data using the readr::read_csv function. Note that we use here::here to ensure the path to the data is correct relative to the project root.\nWe then use the glimpse function to take a look at the data. Glimpse tells us the number of rows and columns in the data, the names of the columns, the type of each column, and the first few rows of the data.\nI create a new object df_clean to store the cleaned data. For now, we’ll just copy the original data to this object. This is a good habit to get into, as it allows you to keep the original data for reference and to easily revert to it if needed.\n\n# Load the data using here::here to ensure correct path resolution\ndf &lt;- read_csv(here::here(\"data\", \"raw\", \"cdc_PRAMStat_Data_for_2011_20250610.csv\"))\n\n# Take a look at the data structure\ndf |&gt; glimpse()\n\nRows: 520,381\nColumns: 27\n$ Year                       &lt;dbl&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2…\n$ LocationAbbr               &lt;chr&gt; \"AR\", \"AR\", \"CO\", \"CO\", \"DE\", \"DE\", \"GA\", \"…\n$ LocationDesc               &lt;chr&gt; \"Arkansas\", \"Arkansas\", \"Colorado\", \"Colora…\n$ Class                      &lt;chr&gt; \"Prenatal Care\", \"Prenatal Care\", \"Prenatal…\n$ Topic                      &lt;chr&gt; \"Prenatal Care - Content\", \"Prenatal Care -…\n$ Question                   &lt;chr&gt; \"During any of your prenatal care visits  d…\n$ DataSource                 &lt;chr&gt; \"PRAMS\", \"PRAMS\", \"PRAMS\", \"PRAMS\", \"PRAMS\"…\n$ Response                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Data_Value_Unit            &lt;chr&gt; \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\"…\n$ Data_Value_Type            &lt;chr&gt; \"Percentage\", \"Percentage\", \"Percentage\", \"…\n$ Data_Value                 &lt;dbl&gt; 52.0, 52.0, 39.0, 35.0, 22.0, 19.0, 71.0, 3…\n$ Data_Value_Footnote_Symbol &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ Data_Value_Footnote        &lt;chr&gt; \"Missing includes not applicable, don't kno…\n$ Data_Value_Std_Err         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Low_Confidence_Limit       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ High_Confidence_Limit      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Sample_Size                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Break_Out                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Break_Out_Category         &lt;chr&gt; \"Maternal Age - 18 to 44 years in groupings…\n$ Geolocation                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ClassId                    &lt;chr&gt; \"CLA11\", \"CLA11\", \"CLA11\", \"CLA11\", \"CLA11\"…\n$ TopicId                    &lt;chr&gt; \"TOP19\", \"TOP19\", \"TOP19\", \"TOP19\", \"TOP19\"…\n$ QuestionId                 &lt;chr&gt; \"QUO333\", \"QUO67\", \"QUO333\", \"QUO67\", \"QUO3…\n$ LocationId                 &lt;dbl&gt; 5, 5, 8, 8, 10, 10, 13, 15, 15, 25, 25, 24,…\n$ BreakOutId                 &lt;chr&gt; \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\"…\n$ BreakOutCategoryid         &lt;chr&gt; \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\"…\n$ ResponseId                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\ndf_clean &lt;- df"
  },
  {
    "objectID": "examples/prams_1_data_prep.html#tabbing-out-data",
    "href": "examples/prams_1_data_prep.html#tabbing-out-data",
    "title": "Data Preparation Example",
    "section": "Tabbing Out Data",
    "text": "Tabbing Out Data\nAlways read the codebook first to understand the data. Nevertheless, it’s still helpful to tab out the data to get a sense of the variables and their values.\nLet’s first look at the topics availble\n\ntable(df$Topic) |&gt; kable()\n\n\n\n\nVar1\nFreq\n\n\n\n\nAbuse - Physical\n6799\n\n\nAlcohol Use\n17875\n\n\nAssisted Reproduction\n1964\n\n\nBreastfeeding\n8829\n\n\nContraception - Conception\n32460\n\n\nContraception - Postpartum\n26716\n\n\nDelivery - Method\n999\n\n\nDelivery - Payment\n14802\n\n\nHIV Test\n9975\n\n\nHospital Length of Stay\n9564\n\n\nHousehold Characteristics\n3469\n\n\nIncome\n8143\n\n\nInfant Health Care\n6377\n\n\nInjury Prevention\n1692\n\n\nInsurance Coverage\n2997\n\n\nMaternal Health Care\n2220\n\n\nMedicaid\n11770\n\n\nMental Health\n5994\n\n\nMorbidity - Infant\n5697\n\n\nMorbidity - Maternal\n35790\n\n\nMultivitamin Use\n12185\n\n\nObesity\n11517\n\n\nOral Health\n10543\n\n\nPreconception Health\n21977\n\n\nPreconception Morbidity\n7142\n\n\nPregnancy History\n12875\n\n\nPregnancy Intention\n14249\n\n\nPregnancy Outcome\n2928\n\n\nPregnancy Recognition\n2996\n\n\nPrenatal Care  Provider\n999\n\n\nPrenatal Care - Content\n36688\n\n\nPrenatal Care - Initiation\n8991\n\n\nPrenatal Care - Location\n1692\n\n\nPrenatal Care - Payment\n14846\n\n\nPrenatal Care - Visits\n7930\n\n\nSleep Behaviors\n19536\n\n\nSmoke Exposure\n2986\n\n\nStress\n56867\n\n\nTobacco Use\n56440\n\n\nWIC\n2862\n\n\n\n\n\nSuppose we’re intersted in the relationship between Alcohol Use and Mental Health.\nLet’s filter the data to include only those topics, and then look at the questions available.\n\ndf |&gt;\n  filter(Topic %in% c(\"Alcohol Use\", \"Mental Health\")) |&gt;\n  select(QuestionId, Topic, Question) |&gt;\n  distinct() |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\nQuestionId\nTopic\nQuestion\n\n\n\n\nQUO100\nAlcohol Use\nIndicator of drinking any alcohol during the past two years\n\n\nQUO271\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\n\n\nQUO8\nAlcohol Use\n(*PCH) Indicator of drinking alcohol during the three months before pregnancy\n\n\nQUO9\nAlcohol Use\nIndicator of whether mother reported having any alcoholic drinks during the last 3 months of pregnancy\n\n\nQUO99\nAlcohol Use\nChange in drinking between three months before pregnancy and last three months of pregnancy\n\n\nQUO133\nMental Health\nDuring the 3 months before you got pregnant with your new baby did you have depression?\n\n\nQUO219\nMental Health\n(*PCH) Indicator of whether mother reported frequent postpartum depressive symptoms (years 2009 - 2011)\n\n\nQUO232\nMental Health\nDuring the 3 months before you got pregnant with your new baby did you have anxiety?\n\n\nQUO97\nMental Health\nDid a doctor nurse or other health care worker talk with you about baby blues or postpartum depression during pregnancy or after your delivery?\n\n\nQUO133\nMental Health\nDuring the 3 months before you got pregnant with your new baby, did you have depression?\n\n\nQUO232\nMental Health\nDuring the 3 months before you got pregnant with your new baby, did you have anxiety?\n\n\nQUO97\nMental Health\nDid a doctor, nurse, or other health care worker talk with you about baby blues or postpartum depression during pregnancy or after your delivery?"
  },
  {
    "objectID": "examples/prams_1_data_prep.html#filtering-data",
    "href": "examples/prams_1_data_prep.html#filtering-data",
    "title": "Data Preparation Example",
    "section": "Filtering Data",
    "text": "Filtering Data\nSuppose we’re interested in the relationship between drinking in the first three months before pregnancy and reporting depression and anxiety. H\n\nquestion_ids_of_interest &lt;- c(\n  \"QUO271\", # Binge drinking in the first three months before pregnancy\n  \"QUO8\",   # Any alcohol use in the first three months before pregnancy\n  \"QUO133\", # Depression reported in the first three months before pregnancy\n  \"QUO232\"  # Anxiety reported in the first three months before pregnancy\n)\n\ndf_clean &lt;- df_clean |&gt;\n  filter(QuestionId %in% question_ids_of_interest)\n\ndf_clean |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nLocationAbbr\nLocationDesc\nClass\nTopic\nQuestion\nDataSource\nResponse\nData_Value_Unit\nData_Value_Type\nData_Value\nData_Value_Footnote_Symbol\nData_Value_Footnote\nData_Value_Std_Err\nLow_Confidence_Limit\nHigh_Confidence_Limit\nSample_Size\nBreak_Out\nBreak_Out_Category\nGeolocation\nClassId\nTopicId\nQuestionId\nLocationId\nBreakOutId\nBreakOutCategoryid\nResponseId\n\n\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNO\n%\nPercentage\n78.4\nNA\nNA\nNA\n76.7\n79.9\n7089\nLBW (&lt;=2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT1\nBOC1\nRES23\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nYES\n%\nPercentage\n21.6\nNA\nNA\nNA\n20.1\n23.3\n1832\nLBW (&lt;=2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT1\nBOC1\nRES40\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNO\n%\nPercentage\n76.8\nNA\nNA\nNA\n76.0\n77.5\n20588\nNBW (&gt;2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT2\nBOC1\nRES23\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nYES\n%\nPercentage\n23.2\nNA\nNA\nNA\n22.5\n24.0\n6099\nNBW (&gt;2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT2\nBOC1\nRES40\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNA\n%\nPercentage\n877.0\n1\nMissing includes not applicable, don’t know, not recorded, no responses, and legitimate skips\nNA\nNA\nNA\nNA\nNA\nOn WIC during Pregnancy\nNA\nCLA9\nTOP2\nQUO271\n59\nBOC10\nBOC10\nNA\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNO\n%\nPercentage\n74.5\nNA\nNA\nNA\n73.5\n75.4\n13716\nNon-WIC\nOn WIC during Pregnancy\nNA\nCLA9\nTOP2\nQUO271\n59\nWIC1\nBOC10\nRES23\n\n\n\n\n\nI personally prefer to remove values from the data set we do not need. We can always add them back in. By using select we also reorder the columns to make it easier to take in.\n\ndf_clean &lt;- df_clean |&gt;\n  select(\n    QuestionId,\n    Topic,\n    Question,\n    Response,\n    Data_Value,\n    Data_Value_Unit,\n    Data_Value_Type,\n    LocationAbbr,\n    LocationDesc,\n    Break_Out,\n    Break_Out_Category\n  )"
  },
  {
    "objectID": "examples/prams_1_data_prep.html#renaming-variables",
    "href": "examples/prams_1_data_prep.html#renaming-variables",
    "title": "Data Preparation Example",
    "section": "Renaming Variables",
    "text": "Renaming Variables\nI prefer to rename variables to follow a consistent style and naming convention (e.g., lowercase, snake_case, noun first). Here I also rename variables to be more descriptive, such as with subgroups. Some would prefer keeping the original names, but I prefer the ergonomics of this approach.\nWe can use the DT::datatable function to view the data in our document in an interactive, filterable way. You’ll want to be careful with inline this data, but with public data, this is a good way to share data (or a selection of it) with others.\n\ndf_clean &lt;- df_clean |&gt;\n  rename_with(tolower) |&gt;\n  rename(\n    question_id = questionid,\n    value = data_value,\n    unit = data_value_unit,\n    type = data_value_type,\n    location_abbr = locationabbr,\n    location = locationdesc,\n    subgroup = break_out,\n    subgroup_cat = break_out_category\n  )\n\nDT::datatable(df_clean)"
  },
  {
    "objectID": "examples/prams_1_data_prep.html#understanding-the-data",
    "href": "examples/prams_1_data_prep.html#understanding-the-data",
    "title": "Data Preparation Example",
    "section": "Understanding The Data",
    "text": "Understanding The Data\nFirst note that this data still needs work. Even without the codebook, we can seee:\n\nThe response/value has “yes,” “no”, and NA values.\nThe location_abbr has both aggregations and location values.\nThe meaning of each row depends on the subgroup and subgroup_cat variables.\n\nFirst, let’s get a sense of the locations in the data\n\ndf_clean |&gt; \n  group_by(location) |&gt;\n  summarise(\n    location_abbr = first(location_abbr),\n    n = n()\n  ) |&gt;\nkable()\n\n\n\n\nlocation\nlocation_abbr\nn\n\n\n\n\nArkansas\nAR\n210\n\n\nColorado\nCO\n216\n\n\nDelaware\nDE\n438\n\n\nGeorgia\nGA\n210\n\n\nHawaii\nHI\n438\n\n\nMaine\nME\n216\n\n\nMaryland\nMD\n438\n\n\nMassachusetts\nMA\n216\n\n\nMichigan\nMI\n438\n\n\nMinnesota\nMN\n438\n\n\nMissouri\nMO\n432\n\n\nNebraska\nNE\n210\n\n\nNew Jersey\nNJ\n216\n\n\nNew Mexico\nNM\n210\n\n\nNew York (excluding NYC)\nNY\n210\n\n\nNew York City\nYC\n216\n\n\nOklahoma\nOK\n216\n\n\nOregon\nOR\n210\n\n\nPRAMS Total\nPRAMS Total\n432\n\n\nPennsylvania\nPA\n210\n\n\nRhode Island\nRI\n216\n\n\nUtah\nUT\n438\n\n\nVermont\nVT\n216\n\n\nWashington\nWA\n216\n\n\nWest Virginia\nWV\n438\n\n\nWisconsin\nWI\n438\n\n\nWyoming\nWY\n438\n\n\n\n\n\nWe see that some non-standard choices are made here: New York and New York City are both listed as locations. Also, aggregations are included under PRAMS Total.\nLets get an overview of the subgroup categories:\n\ndf_clean |&gt; \n  group_by(subgroup_cat, subgroup) |&gt;\n  summarise(n = n()) |&gt;\n  kable()\n\n\n\n\nsubgroup_cat\nsubgroup\nn\n\n\n\n\nAdequacy of Prenatal care\nADEQUATE PNC\n152\n\n\nAdequacy of Prenatal care\nINADEQUATE PNC\n152\n\n\nAdequacy of Prenatal care\nINTERMEDIATE PNC\n152\n\n\nAdequacy of Prenatal care\nUNKNOWN PNC\n152\n\n\nAdequacy of Prenatal care\nNA\n49\n\n\nBirth Weight\nLBW (&lt;=2500g)\n152\n\n\nBirth Weight\nNBW (&gt;2500g)\n152\n\n\nBirth Weight\nNA\n67\n\n\nIncome (years 2004 and beyond)\n$10,000 to $24,999\n152\n\n\nIncome (years 2004 and beyond)\n$25,000 to $49,999\n152\n\n\nIncome (years 2004 and beyond)\n$50,000 or more\n152\n\n\nIncome (years 2004 and beyond)\nLess than $10,000\n152\n\n\nIncome (years 2004 and beyond)\nNA\n49\n\n\nMarital Status\nMARRIED\n152\n\n\nMarital Status\nOTHER\n152\n\n\nMarital Status\nNA\n67\n\n\nMaternal Age (3 Levels)\n20-29 yrs\n152\n\n\nMaternal Age (3 Levels)\n30+ yrs\n152\n\n\nMaternal Age (3 Levels)\n&lt;20 yrs\n152\n\n\nMaternal Age (3 Levels)\nNA\n49\n\n\nMaternal Age (4 Levels)\n20-24 yrs\n152\n\n\nMaternal Age (4 Levels)\n25-34 yrs\n152\n\n\nMaternal Age (4 Levels)\n35+ yrs\n152\n\n\nMaternal Age (4 Levels)\n&lt;20 yrs\n152\n\n\nMaternal Age (4 Levels)\nNA\n67\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 18 - 24\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 25 - 29\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 30 - 44\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 45+\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge &lt; 18\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nNA\n76\n\n\nMaternal Age - 18 to 44 years only\nAge 18 - 44\n152\n\n\nMaternal Age - 18 to 44 years only\nNA\n49\n\n\nMaternal Education\n12 yrs\n152\n\n\nMaternal Education\n&lt; 12 yrs\n152\n\n\nMaternal Education\n&gt;12 yrs\n152\n\n\nMaternal Education\nNA\n76\n\n\nMaternal Race/Ethnicity\nBlack, non-Hispanic\n152\n\n\nMaternal Race/Ethnicity\nHispanic\n152\n\n\nMaternal Race/Ethnicity\nOther non-Hispanic\n152\n\n\nMaternal Race/Ethnicity\nWhite, non-Hispanic\n152\n\n\nMaternal Race/Ethnicity\nNA\n67\n\n\nMedicaid Recipient\nMedicaid\n152\n\n\nMedicaid Recipient\nNon-Medicaid\n152\n\n\nMedicaid Recipient\nNA\n67\n\n\nMother Hispanic\nHispanic\n152\n\n\nMother Hispanic\nNon-Hispanic\n152\n\n\nMother Hispanic\nNA\n67\n\n\nNone\nNone\n152\n\n\nNumber of Previous Live Births\n0\n152\n\n\nNumber of Previous Live Births\n1 or more\n152\n\n\nNumber of Previous Live Births\nNA\n76\n\n\nOn WIC during Pregnancy\nNon-WIC\n152\n\n\nOn WIC during Pregnancy\nWIC\n152\n\n\nOn WIC during Pregnancy\nNA\n76\n\n\nPregnancy Intendedness\nIntended\n152\n\n\nPregnancy Intendedness\nUnintended\n152\n\n\nPregnancy Intendedness\nNA\n49\n\n\nSmoked 3 months before Pregnancy\nNon-Smoker\n152\n\n\nSmoked 3 months before Pregnancy\nSmoker\n152\n\n\nSmoked 3 months before Pregnancy\nNA\n76\n\n\nSmoked last 3 months of Pregnancy\nNon-Smoker\n152\n\n\nSmoked last 3 months of Pregnancy\nSmoker\n152\n\n\nSmoked last 3 months of Pregnancy\nNA\n49\n\n\n\n\n\nWe now have a sense of the structure of the entire data file.\nLet’s finally use the group_by and summarise functions to try narrow down groups to Ns of 1.\n\ndf_summary &lt;- df_clean |&gt;\n  group_by(question_id, location_abbr, subgroup_cat, subgroup, response) |&gt;\n  summarise(\n    n = n(),\n    question = first(question),\n    mean_value = mean(value)\n  )\n\nDT::datatable(df_summary)\n\n\n\n\n\nWe now know that the following data combine to form a “base” row:\n\nquestion_id\nlocation_abbr\nsubgroup_cat\nsubgroup\nresponse\n\nWith this in mind, we can now see that the following data combine to form a “base” row."
  },
  {
    "objectID": "examples/prams_1_data_prep.html#preparing-the-data-for-visualizaiton",
    "href": "examples/prams_1_data_prep.html#preparing-the-data-for-visualizaiton",
    "title": "Data Preparation Example",
    "section": "Preparing The Data For Visualizaiton",
    "text": "Preparing The Data For Visualizaiton\nYou’ll notice how the data is in a kind of long format: there are yes, no, and NA values for each question. We want to get all values per question per location/subgroup/subgroup_cat.\nSteps:\n\nFilter out the aggregate (non-location specific) values\nFilter out NAs on the subroup (break out) category: of course, you would need to identify WHY these are NAs and address this issue if there is a reason.\n\nWe can then use the tidyr package to pivot the data to wide format.\n\ndf_final &lt;- df_clean |&gt;\n  filter(location_abbr != \"PRAMS Total\") |&gt;\n  filter(!is.na(subgroup)) |&gt;\n  filter(response == \"YES\") |&gt;  # Only keep YES responses\n  pivot_wider(\n    id_cols = c(location_abbr, subgroup_cat, subgroup),\n    names_from = question_id,\n    values_from = value,\n    names_prefix = \"q_\"  # Add prefix to question ID columns\n  ) |&gt;\n  rename(\n    depression_within_3_months_birth = q_QUO133,\n    anxiety_within_3_months_birth = q_QUO232,\n    alcohol_use_within_3_months_birth = q_QUO8,\n    binge_drinking_within_3_months_birth = q_QUO271\n  ) |&gt; \n  arrange(location_abbr, subgroup_cat, subgroup)\n  \n\nDT::datatable(df_final)"
  },
  {
    "objectID": "examples/prams_1_data_prep.html#data-diagnostics",
    "href": "examples/prams_1_data_prep.html#data-diagnostics",
    "title": "Data Preparation Example",
    "section": "Data Diagnostics",
    "text": "Data Diagnostics\nLet’s do a quick analysis of missing values.\n\ndf_final |&gt; \n  summarise(\n    across(everything(), ~ 100 * sum(is.na(.)) / n())\n  ) |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation_abbr\nsubgroup_cat\nsubgroup\ndepression_within_3_months_birth\nanxiety_within_3_months_birth\nbinge_drinking_within_3_months_birth\nalcohol_use_within_3_months_birth\n\n\n\n\n0\n0\n0\n63.42062\n63.42062\n5.07365\n5.07365\n\n\n\n\n\nLet’s examine missing data across all measures, by location.\n\ndf_final |&gt;\n  group_by(location_abbr) |&gt;\n  summarise(\n    depression_missing = 100 * mean(is.na(depression_within_3_months_birth)),\n    anxiety_missing = 100 * mean(is.na(anxiety_within_3_months_birth)),\n    alcohol_missing = 100 * mean(is.na(alcohol_use_within_3_months_birth)),\n    binge_missing = 100 * mean(is.na(binge_drinking_within_3_months_birth))\n  ) |&gt;\n  arrange(location_abbr) |&gt;\n  kable(digits = 1)\n\n\n\n\n\n\n\n\n\n\n\nlocation_abbr\ndepression_missing\nanxiety_missing\nalcohol_missing\nbinge_missing\n\n\n\n\nAR\n100.0\n100.0\n4.3\n4.3\n\n\nCO\n100.0\n100.0\n2.1\n2.1\n\n\nDE\n4.3\n4.3\n4.3\n4.3\n\n\nGA\n100.0\n100.0\n2.1\n2.1\n\n\nHI\n4.3\n4.3\n4.3\n4.3\n\n\nMA\n100.0\n100.0\n4.3\n4.3\n\n\nMD\n4.3\n4.3\n4.3\n4.3\n\n\nME\n100.0\n100.0\n14.9\n14.9\n\n\nMI\n2.1\n2.1\n2.1\n2.1\n\n\nMN\n4.3\n4.3\n4.3\n4.3\n\n\nMO\n4.3\n4.3\n4.3\n4.3\n\n\nNE\n100.0\n100.0\n2.1\n2.1\n\n\nNJ\n100.0\n100.0\n6.4\n6.4\n\n\nNM\n100.0\n100.0\n4.3\n4.3\n\n\nNY\n100.0\n100.0\n4.3\n4.3\n\n\nOK\n100.0\n100.0\n2.1\n2.1\n\n\nOR\n100.0\n100.0\n2.1\n2.1\n\n\nPA\n100.0\n100.0\n4.3\n4.3\n\n\nRI\n100.0\n100.0\n6.4\n6.4\n\n\nUT\n6.4\n6.4\n6.4\n6.4\n\n\nVT\n100.0\n100.0\n12.8\n12.8\n\n\nWA\n100.0\n100.0\n4.3\n4.3\n\n\nWI\n2.1\n2.1\n2.1\n2.1\n\n\nWV\n10.6\n10.6\n10.6\n10.6\n\n\nWY\n6.4\n6.4\n6.4\n6.4\n\n\nYC\n100.0\n100.0\n6.4\n6.4\n\n\n\n\n\nData on alcohol use is more comprehensive than on anxiety and depression. Please note that just because we’re focused on data visualization does not mean our concerns over missing data are any less important than, say, when we are doing staitstical modeling."
  },
  {
    "objectID": "examples/prams_1_data_prep.html#save-the-data",
    "href": "examples/prams_1_data_prep.html#save-the-data",
    "title": "Data Preparation Example",
    "section": "Save The Data",
    "text": "Save The Data\nNow, let’s save this processed data to an RDS file. Before we do, let’s merge back in our full location for reference and turn our location variables into factors.\n\ndf_final &lt;- df_final |&gt;\n  left_join(\n    df_clean |&gt;\n      select(location_abbr, location) |&gt;\n      distinct(),\n    by = \"location_abbr\"\n  ) |&gt;\n  mutate(\n    location_abbr = factor(location_abbr),\n    location = factor(location)\n  )\n\nsaveRDS(df_final, here::here(\"data\", \"processed\", \"cdc_prams_df_final.rds\"))"
  },
  {
    "objectID": "examples/colors_and_accessibility.html",
    "href": "examples/colors_and_accessibility.html",
    "title": "Color Systems in R",
    "section": "",
    "text": "# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"ggplot2\", \"RColorBrewer\", \"viridis\", \"kableExtra\", \"tidyr\", \"ggridges\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(viridis)\nlibrary(kableExtra)\nlibrary(tidyr)\nlibrary(ggridges)"
  },
  {
    "objectID": "examples/colors_and_accessibility.html#color-systems-in-r",
    "href": "examples/colors_and_accessibility.html#color-systems-in-r",
    "title": "Color Systems in R",
    "section": "Color Systems in R",
    "text": "Color Systems in R\nWe can take several different approaches to color in data visualization, depending on our audience and our goals.\n\nData-focused colors: Designed for accessibility and clear data communication\nPerceptually uniform colors: Colorblind-friendly palettes with perceptual uniformity\nDesign system colors: For polished, consistent visual language"
  },
  {
    "objectID": "examples/colors_and_accessibility.html#rcolorbrewer-accessible-data-visualization",
    "href": "examples/colors_and_accessibility.html#rcolorbrewer-accessible-data-visualization",
    "title": "Color Systems in R",
    "section": "RColorBrewer: Accessible Data Visualization",
    "text": "RColorBrewer: Accessible Data Visualization\nRColorBrewer provides several types of color palettes designed for data visualization:\n\nSequential: for ordered data (e.g., “Blues”, “Greens”)\nDiverging: for data that deviates from a middle value (e.g., “RdBu”, “Spectral”)\nQualitative: for categorical data (e.g., “Set1”, “Set2”)\n\nHere are some examples of RColorBrewer palettes:\n\n# Create a function to display color palettes\ndisplay_brewer_pal &lt;- function(name, n) {\n  colors &lt;- brewer.pal(n, name)\n  plot(1:n, rep(1, n),\n       col = colors,\n       pch = 19,\n       cex = 5,\n       xlab = \"\",\n       ylab = \"\",\n       axes = FALSE)\n  title(name)\n}\n\n# Display some example palettes\npar(mfrow = c(2, 2))\ndisplay_brewer_pal(\"Blues\", 9)    # Sequential\ndisplay_brewer_pal(\"RdBu\", 9)     # Diverging\ndisplay_brewer_pal(\"Set1\", 9)     # Qualitative\ndisplay_brewer_pal(\"Spectral\", 9) # Diverging"
  },
  {
    "objectID": "examples/colors_and_accessibility.html#using-rcolorbrewer-in-ggplot2",
    "href": "examples/colors_and_accessibility.html#using-rcolorbrewer-in-ggplot2",
    "title": "Color Systems in R",
    "section": "Using RColorBrewer in ggplot2",
    "text": "Using RColorBrewer in ggplot2\nHere’s how RColorBrewer can be used in different visualization scenarios:\n\n# Create example data\ndf &lt;- data.frame(\n  category = letters[1:5],\n  value = c(10, 20, 15, 25, 30)\n)\n\n# Plot with qualitative colors (categorical data)\nggplot(df, aes(x = category, y = value, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Qualitative Colors (Set1)\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n# Create data for a heatmap\nset.seed(123)\nheatmap_data &lt;- expand.grid(\n  x = 1:10,\n  y = 1:10\n) |&gt;\n  mutate(\n    value = rnorm(100, mean = 50, sd = 15)\n  )\n\n# Plot with sequential colors (continuous data)\nggplot(heatmap_data, aes(x = x, y = y, fill = value)) +\n  geom_tile() +\n  scale_fill_distiller(palette = \"Blues\") +\n  labs(\n    title = \"Sequential Colors (Blues)\",\n    subtitle = \"Heatmap showing continuous data distribution\"\n  ) +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n# Create sample data with a moderate relationship and outliers\n\n# Create sample survey response data\nset.seed(789)\ncategories &lt;- c(\"Vaccination Requirements\", \"Mask Mandates\", \"Social Distancing\",\n               \"Contact Tracing\", \"Travel Restrictions\", \"Quarantine Rules\")\ndata &lt;- data.frame(\n  category = factor(categories, levels = categories),\n  support = c(90, 80, 70, 40, 30, 20),\n  oppose = c(10, 20, 30, 60, 70, 80)\n)\n\n# Calculate differences\ndata$difference &lt;- data$support - data$oppose\n\n# Reshape data for diverging bars\ndata_long &lt;- data |&gt;\n  pivot_longer(\n    cols = c(support, oppose),\n    names_to = \"response\",\n    values_to = \"percentage\"\n  ) |&gt;\n  mutate(\n    percentage = ifelse(response == \"oppose\", -percentage, percentage)\n  )\n\n# Create diverging bar chart\nggplot(data_long, aes(x = category, y = percentage, fill = percentage)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_distiller(\n    palette = \"RdBu\",\n    direction = 1,\n    limits = c(-100, 100),\n    breaks = c(-100, -50, 0, 50, 100),\n    labels = c(\"100%\", \"50%\", \"0%\", \"50%\", \"100%\")\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_line(color = \"gray90\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14)\n  ) +\n  labs(\n    title = \"Public Support for Health Measures\",\n    subtitle = \"Percentage support vs. oppose\",\n    x = \"\",\n    y = \"Percentage\",\n    fill = \"Support/Oppose\"\n  )\n\n\n\n\n\n\n\n# Create a lollipop chart version with annotations\nggplot(data_long, aes(x = category, y = percentage, color = percentage)) +\n  geom_segment(aes(xend = category, yend = 0), linewidth = 1) +\n  geom_point(size = 4) +\n  # Add difference annotations\n  geom_text(\n    data = data,\n    aes(x = category, y = 0, label = sprintf(\"%+d%%\", difference)),\n    hjust = 1.2,\n    vjust = -1.0,\n    size = 3.5,\n    inherit.aes = FALSE  # Don't inherit aesthetics from the main plot\n  ) +\n  scale_color_distiller(\n    palette = \"RdBu\",\n    direction = 1,\n    limits = c(-100, 100),\n    breaks = c(-100, -50, 0, 50, 100),\n    labels = c(\"100%\", \"50%\", \"0%\", \"50%\", \"100%\")\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_line(color = \"gray90\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14)\n  ) +\n  labs(\n    title = \"Public Support for Health Measures\",\n    subtitle = \"Percentage support vs. oppose (Lollipop Chart)\",\n    x = \"\",\n    y = \"Percentage\",\n    color = \"Support/Oppose\"\n  )"
  },
  {
    "objectID": "examples/colors_and_accessibility.html#viridis-perceptually-uniform-colors",
    "href": "examples/colors_and_accessibility.html#viridis-perceptually-uniform-colors",
    "title": "Color Systems in R",
    "section": "Viridis: Perceptually Uniform Colors",
    "text": "Viridis: Perceptually Uniform Colors\nPerceptually uniform colors are designed to:\n\nAllow humans to interpret magnitude differences using color (E.g., equal steps in data result in equal perceptual differences in color)\nEnsure that color transitions appear evenly spaced, so a change from 0 to 10 looks as distinct as 10 to 20\nRemain interpretable when converted to grayscale (useful for printing and accessibility)\n\nThe viridis package provides colorblind-friendly palettes that are perceptually uniform.\n\n# Create a function to display viridis palettes\ndisplay_viridis_pal &lt;- function(option, n) {\n  colors &lt;- viridis(n, option = option)\n  plot(1:n, rep(1, n),\n       col = colors,\n       pch = 19,\n       cex = 5,\n       xlab = \"\",\n       ylab = \"\",\n       axes = FALSE)\n  title(option)\n}\n\n# Display viridis palettes\npar(mfrow = c(2, 2))\ndisplay_viridis_pal(\"viridis\", 9)     # Default viridis\ndisplay_viridis_pal(\"magma\", 9)       # Magma\ndisplay_viridis_pal(\"plasma\", 9)      # Plasma\ndisplay_viridis_pal(\"inferno\", 9)     # Inferno\n\n\n\n\n\n\n\n\nHere’s how viridis can be used in ggplot2:\n\n# Create example data with more categories\ndf &lt;- data.frame(\n  category = letters[1:8],\n  value = c(10, 20, 15, 25, 30, 22, 18, 27)\n)\n\n# Plot with viridis colors\nggplot(df, aes(x = category, y = value, fill = value)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis() +\n  labs(title = \"Viridis Colors\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n## Plot with magma colors\n# Create simulated temperature data for different cities\nset.seed(789)\nn_samples &lt;- 1000\ncities &lt;- c(\"Phoenix\", \"Miami\", \"Los Angeles\", \"New York\", \"Chicago\", \"Seattle\")\nridge_data &lt;- data.frame(\n  city = rep(cities, each = n_samples),\n  temperature = c(\n    rnorm(n_samples, mean = 95, sd = 8),   # Phoenix\n    rnorm(n_samples, mean = 85, sd = 6),   # Miami\n    rnorm(n_samples, mean = 75, sd = 5),   # Los Angeles\n    rnorm(n_samples, mean = 65, sd = 7),   # New York\n    rnorm(n_samples, mean = 55, sd = 8),   # Chicago\n    rnorm(n_samples, mean = 45, sd = 6)    # Seattle\n  )\n)\n\n# Calculate mean temperatures and sort cities\ncity_means &lt;- ridge_data |&gt;\n  group_by(city) |&gt;\n  summarize(mean_temp = mean(temperature)) |&gt;\n  arrange(desc(mean_temp))\n\n# Reorder the factor levels of city based on mean temperature\nridge_data$city &lt;- factor(ridge_data$city, levels = city_means$city)\n\n# Create ridgeline plot of temperature distributions\nggplot(ridge_data, aes(x = temperature, y = city, fill = after_stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01,\n    gradient_lwd = 0.5\n  ) +\n  scale_fill_viridis_c(\n    option = \"plasma\",\n    direction = 1,\n    limits = c(20, 120),\n    breaks = seq(20, 120, 20),\n    labels = paste0(seq(20, 120, 20), \"°F\")\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_line(color = \"gray90\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14)\n  ) +\n  labs(\n    title = \"Summer Temperature Distributions\",\n    subtitle = \"Daily high temperatures by city (sorted by mean temperature)\",\n    x = \"Temperature (°F)\",\n    y = \"\",\n    fill = \"Temperature\"\n  )"
  },
  {
    "objectID": "examples/colors_and_accessibility.html#tailwind-one-option-for-custom-design",
    "href": "examples/colors_and_accessibility.html#tailwind-one-option-for-custom-design",
    "title": "Color Systems in R",
    "section": "Tailwind: One Option for Custom Design",
    "text": "Tailwind: One Option for Custom Design\nTailwind’s color palette offers another approach to visualization design. Here’s an example of how you might use it to create visual hierarchy and data representation:\n\n# Source the Tailwind colors\nsource(\"colors.R\")\n\n# Create example data with categories and subcategories\ndf &lt;- data.frame(\n  category = rep(c(\"Primary Care\", \"Emergency Care\", \"Specialty Care\"), each = 3),\n  subcategory = rep(c(\"Urban\", \"Suburban\", \"Rural\"), times = 3),\n  value = c(85, 75, 65, 90, 80, 70, 95, 85, 75)  # Healthcare access percentages\n)\n\n# One way to create visual hierarchy with grays\ncustom_theme &lt;- theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    plot.background = element_rect(fill = colors[[\"gray\"]][[\"50\"]], color = NA),\n    panel.background = element_rect(fill = colors[[\"gray\"]][[\"50\"]], color = NA),\n    axis.text = element_text(color = colors[[\"gray\"]][[\"700\"]]),\n    axis.title = element_text(color = colors[[\"gray\"]][[\"900\"]]),\n    plot.title = element_text(color = colors[[\"gray\"]][[\"900\"]], face = \"bold\"),\n    legend.text = element_text(color = colors[[\"gray\"]][[\"700\"]]),\n    legend.title = element_text(color = colors[[\"gray\"]][[\"900\"]])\n  )\n\n# One approach to using blues for data representation\nblue_palette &lt;- c(\n  colors[[\"blue\"]][[\"300\"]],\n  colors[[\"blue\"]][[\"500\"]],\n  colors[[\"blue\"]][[\"700\"]]\n)\n\n# The resulting visualization\nggplot(df, aes(x = category, y = value, fill = subcategory)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = blue_palette) +\n  labs(\n    title = \"Healthcare Access by Care Type and Location\",\n    subtitle = \"Percentage of population with access to healthcare services\",\n    x = NULL,\n    y = \"Access Rate (%)\",\n    fill = \"Location\"\n  ) +\n  custom_theme"
  },
  {
    "objectID": "examples/colors_and_accessibility.html#resources",
    "href": "examples/colors_and_accessibility.html#resources",
    "title": "Color Systems in R",
    "section": "Resources",
    "text": "Resources\n\nRColorBrewer Documentation\nColorBrewer Website\nViridis Package\nColor Accessibility Tools\nTailwind Colors"
  },
  {
    "objectID": "presentation.html#who-am-i",
    "href": "presentation.html#who-am-i",
    "title": "Data Visualization",
    "section": "Who am I?",
    "text": "Who am I?\n\nI am a data scientist at the Johns Hopkins Bloomberg School of Public Health\nI work out of the Johns Hopkins Biostatistics Center in the Department of Biostatistics\nI was trained in the social sciences and have worked profesionally as a data scientist and software developer for over 10 years"
  },
  {
    "objectID": "presentation.html#contact-information",
    "href": "presentation.html#contact-information",
    "title": "Data Visualization",
    "section": "Contact information",
    "text": "Contact information\n\nEmail: ewestlund@jhu.edu"
  },
  {
    "objectID": "presentation.html#edward-tufte-graphical-excellence-is.",
    "href": "presentation.html#edward-tufte-graphical-excellence-is.",
    "title": "Data Visualization",
    "section": "Edward Tufte: Graphical Excellence is….",
    "text": "Edward Tufte: Graphical Excellence is….\n\n\n\n\n\nEdward Tufte\n\n\n\n\n“Graphical excellence is the well-designed presentation of interesting data—a matter of substance, of statistics, and of design…”\n\nEdward Tufte, The Visual Display of Quantitative Information, 1983"
  },
  {
    "objectID": "presentation.html#dense",
    "href": "presentation.html#dense",
    "title": "Data Visualization",
    "section": "Dense",
    "text": "Dense\n\n\n\n\n\nEdward Tufte\n\n\n\n\n“It is that which gives to the view the great number of ideas in the shortest time with the least ink in the smallest space…”\n\nEdward Tufte, The Visual Display of Quantitative Information, 1983"
  },
  {
    "objectID": "presentation.html#multivariate",
    "href": "presentation.html#multivariate",
    "title": "Data Visualization",
    "section": "Multivariate",
    "text": "Multivariate\n\n\n\n\n\nEdward Tufte\n\n\n\n\n“It is nearly always multivariate…”\n\nEdward Tufte, The Visual Display of Quantitative Information, 1983"
  },
  {
    "objectID": "presentation.html#truthful",
    "href": "presentation.html#truthful",
    "title": "Data Visualization",
    "section": "Truthful",
    "text": "Truthful\n\n\n\n\n\nEdward Tufte\n\n\n\n\n“Graphical excellence requires telling the truth about the data…”\n\nEdward Tufte, The Visual Display of Quantitative Information, 1983"
  },
  {
    "objectID": "presentation.html#exemplar-napoleons-march",
    "href": "presentation.html#exemplar-napoleons-march",
    "title": "Data Visualization",
    "section": "Exemplar: Napoleon’s March",
    "text": "Exemplar: Napoleon’s March\n\nCharles Minard’s Napoleon’s March"
  },
  {
    "objectID": "presentation.html#achieving-minards-graphical-excellence",
    "href": "presentation.html#achieving-minards-graphical-excellence",
    "title": "Data Visualization",
    "section": "Achieving Minard’s Graphical Excellence",
    "text": "Achieving Minard’s Graphical Excellence\n\n“[Minard’s classic image] can be described and admired, but there are no compositional principles on how to create that one wonder graphic in a million.””\n\nEdward Tufte, The Visual Display of Quantitative Information, 1983"
  },
  {
    "objectID": "presentation.html#for-the-rest-of-us",
    "href": "presentation.html#for-the-rest-of-us",
    "title": "Data Visualization",
    "section": "For The Rest of Us",
    "text": "For The Rest of Us\nInstead, Tufte suggests:\n\n“[For] more routine, workaday designs”\n“[Have] a properly chosen format and design”\n“Use words, numbers, and drawing together”\n“[D]isplay an accessible complexity of detail”\n“Avoid content-free decoration, including chartjunk”"
  },
  {
    "objectID": "presentation.html#r",
    "href": "presentation.html#r",
    "title": "Data Visualization",
    "section": "R",
    "text": "R\n\n\n\nR has the most developed data visualization tools for working statisticians and data scientists\nR can be downloaded from r-project.org\n\n\n\n\n\nR Logo"
  },
  {
    "objectID": "presentation.html#ggplot2",
    "href": "presentation.html#ggplot2",
    "title": "Data Visualization",
    "section": "ggplot2",
    "text": "ggplot2\n\n\n\nggplot2 is a powerful package for creating data visualizations\nIt is built on the grammar of graphics\nIt is a declarative grammar for data visualization\n\n\n\n\n\nggplot2 Logo"
  },
  {
    "objectID": "presentation.html#scientific-notebooks",
    "href": "presentation.html#scientific-notebooks",
    "title": "Data Visualization",
    "section": "Scientific Notebooks",
    "text": "Scientific Notebooks\n\n\n\nNotebooks are a powerful way to work with data and do data visualization\nThey allow you to embed code, text, and visualizations in a single document\nThey thus allow you to easily share both the process and the results of your work\n\n\n\n\n\nNotebook Logo"
  },
  {
    "objectID": "presentation.html#notebooks-quarto",
    "href": "presentation.html#notebooks-quarto",
    "title": "Data Visualization",
    "section": "Notebooks: Quarto",
    "text": "Notebooks: Quarto\n\n\n\nQuarto is an open source scientific and technical publishing system\nYou can create reports, websites, presentations, and books with Quarto\nThis presentation is built with Quarto\nYou can embed Python, R, and other code in in plaintext Quarto documents\nQuarto renders down to a document in HTML, PDF, or Word format\n\n\n\n\n\nQuarto Logo"
  },
  {
    "objectID": "presentation.html#rmarkdown",
    "href": "presentation.html#rmarkdown",
    "title": "Data Visualization",
    "section": "RMarkdown",
    "text": "RMarkdown\n\n\n\nRMarkdown is a way to create documents that mix R code and text\nIt integrates with RStudio well and has a very similar workflow to Quarto\nRMarkdown renders down to a document in HTML, PDF, or Word format; the files them selves are plain text\nEasy to store notebooks in version control with git\n\n\n\n\n\nRMarkdown Logo"
  },
  {
    "objectID": "presentation.html#jupyter",
    "href": "presentation.html#jupyter",
    "title": "Data Visualization",
    "section": "Jupyter",
    "text": "Jupyter\n\n\n\nJupyter is a notebook system popular with Python users\nJupyter stores code and results in the same document (Quarto/RMarkdown render into a separate document)\nJupyter supports R and other languages\n\n\n\n\n\nJupyter Logo"
  },
  {
    "objectID": "presentation.html#optionalpopular-software",
    "href": "presentation.html#optionalpopular-software",
    "title": "Data Visualization",
    "section": "Optional/Popular Software",
    "text": "Optional/Popular Software"
  },
  {
    "objectID": "presentation.html#positronvs-code",
    "href": "presentation.html#positronvs-code",
    "title": "Data Visualization",
    "section": "Positron/VS Code",
    "text": "Positron/VS Code\n\n\n\nPositron is Posit’s new IDE\nIt is built on VS\nBuilds in panels and tools for examining data, viewing plots, etc.\n\n\n\n\n\nPositron, making this presentation"
  },
  {
    "objectID": "presentation.html#rstudio",
    "href": "presentation.html#rstudio",
    "title": "Data Visualization",
    "section": "RStudio",
    "text": "RStudio\n\n\n\nRStudio is a powerful IDE for R\nIt is free and open source\nIt helps you understand what is in your environment (e.g., variables, functions, packages, etc.)\nIt also makes it easy to view your visualizations as you make them\n\n\n\n\n\nRStudio Logo"
  },
  {
    "objectID": "presentation.html#python",
    "href": "presentation.html#python",
    "title": "Data Visualization",
    "section": "Python",
    "text": "Python\n\n\n\nPython is a powerful general purpose programming language that is very popular in the data science community, especially in machine learning\nIt has A-tier data management and scientific computing libraries, such as pandas and numpy\nIt has a large ecosystem of packages for data visualization, including matplotlib and seaborn\n\n\n\n\n\nPython Logo"
  },
  {
    "objectID": "presentation.html#llms",
    "href": "presentation.html#llms",
    "title": "Data Visualization",
    "section": "LLMs",
    "text": "LLMs\n\n\n\nLLMs are commonly used to help with code\nCommon ones used in data science are ChatGPT, Claude, Gemini, and GitHub’s Copilot\nThey can help you write code, debug code, and write documentation\nThey can also make mistakes, so you cannot blindly trust their work\n\n\n\n\n\nGitHub Copilot Logo"
  },
  {
    "objectID": "presentation.html#ai-and-data-visualization",
    "href": "presentation.html#ai-and-data-visualization",
    "title": "Data Visualization",
    "section": "AI and Data Visualization",
    "text": "AI and Data Visualization\n\nAI and LLMs are becoming more and more powerful\nThey can help you with many data-related tasks, but require care"
  },
  {
    "objectID": "presentation.html#my-philosophy",
    "href": "presentation.html#my-philosophy",
    "title": "Data Visualization",
    "section": "My Philosophy",
    "text": "My Philosophy\n\nI use LLMs in nearly all aspects of my work\nI have found that there is now less value in being able to “make computer do something” and more in high level concepts\nTo that extent, when discussing data visualization, I now focus a little more on concepts and less on ggplot2 syntax, since LLMs really can mostly solve technical visualization problems with a few minutes of “conversation”"
  },
  {
    "objectID": "presentation.html#proof-of-concepts",
    "href": "presentation.html#proof-of-concepts",
    "title": "Data Visualization",
    "section": "Proof of Concepts",
    "text": "Proof of Concepts\n\nLet’s take the mtcars dataset and see what we can do\n\nI have a plot of auto data. The data set is named \"auto.\" The variables I'm interested are: price, efficiency (variable name: mpg), weight, and manufacturer.  The data set only includes the variable \"make,\" which has both the manufacturer and the car name.  Can you first split \"make\" into \"manufacturer\" and \"model.\" Weight is a continuous measure, but it would be helpful to split this data into categories of small, medium, and large.\n\nI'd then like a plot comparing efficiency to weight. Facet this into small, medium, and large cars. On each figure, annotate the least efficient car with its make and model, one average car with its make and model, and the most efficient car with its make and model.\n\nPut the code for this in a new quarto notebook named llm-example.qmd"
  },
  {
    "objectID": "presentation.html#ggplot2-and-the-grammar-of-graphics",
    "href": "presentation.html#ggplot2-and-the-grammar-of-graphics",
    "title": "Data Visualization",
    "section": "ggplot2 and the Grammar of Graphics",
    "text": "ggplot2 and the Grammar of Graphics\n\nThe “gg” in ggplot2 stands for the grammar of graphics, a concept coined by Leland Wilkinson and popularized by Hadley Wickham.\nGrammar in language is a set of rules for how language is structured.\nGrammar in graphics is a set of rules for how visual representations of data are structured."
  },
  {
    "objectID": "presentation.html#key-concepts-in-ggplot2",
    "href": "presentation.html#key-concepts-in-ggplot2",
    "title": "Data Visualization",
    "section": "Key Concepts in ggplot2",
    "text": "Key Concepts in ggplot2\n\nggplot2 workflow"
  },
  {
    "objectID": "presentation.html#ggplot2-workflow",
    "href": "presentation.html#ggplot2-workflow",
    "title": "Data Visualization",
    "section": "ggplot2 Workflow",
    "text": "ggplot2 Workflow\n\nStart with data\nPick an aesthetic mapping\nChoose a geometric object\nAdd statistical transformations\nAdjust finer details: scales, coordinate systems, faceting, etc."
  },
  {
    "objectID": "presentation.html#aesthetic-mappings",
    "href": "presentation.html#aesthetic-mappings",
    "title": "Data Visualization",
    "section": "Aesthetic Mappings",
    "text": "Aesthetic Mappings\n\nThis is how your variables map onto the aesthetics of your figure\nIt sounds highfalutin, but it usually just means:\n\nWhat is your x?\nWhat is your y?"
  },
  {
    "objectID": "presentation.html#geometric-objects-geoms",
    "href": "presentation.html#geometric-objects-geoms",
    "title": "Data Visualization",
    "section": "Geometric Objects (“geoms”)",
    "text": "Geometric Objects (“geoms”)\n\nGeoms are the elements used to represent the data\nGeoms have associated stats/functions/statistical transformations\nStats such as counts for groups often involve aggregations"
  },
  {
    "objectID": "presentation.html#geoms-cont.",
    "href": "presentation.html#geoms-cont.",
    "title": "Data Visualization",
    "section": "Geoms (cont.)",
    "text": "Geoms (cont.)\n\n\n\n\n\n\n\n\n\n\nGeom\nR Function\nStat/Transformation\nUse for\n\n\n\n\nPoint\ngeom_point()\nIdentity\nScatter plots\n\n\nBar\ngeom_bar()\nCount\nBar plots\n\n\nAB Line\ngeom_abline()\nSlope\nLine plots\n\n\nHorizontal line\ngeom_hline()\nY intercept\nReference lines\n\n\nVertical line\ngeom_vline()\nX intercept\nReference lines\n\n\nSmoother\ngeom_smooth()\nSmoothing function (GAM, Loess, etc.)\nShowing patterns/trends"
  },
  {
    "objectID": "presentation.html#positionfill",
    "href": "presentation.html#positionfill",
    "title": "Data Visualization",
    "section": "Position/Fill",
    "text": "Position/Fill\n\nYou can use position/fill to move elements around and color code geoms by aspects of the data, such as categories.\nFor example, with a bar chart:\n\nstack: Stack bars on top of each other\nfill: Stack bars, but make them always fill up vertical space to 1\ndodge: Put the bars next to each other\n\nTip: When you reach for fill, always consider faceting in small multiples. The brain struggles with fills. More on this later."
  },
  {
    "objectID": "presentation.html#aggregations",
    "href": "presentation.html#aggregations",
    "title": "Data Visualization",
    "section": "Aggregations",
    "text": "Aggregations\n\nTo make a bar chart, one groups data and takes a mean, proportion, or count\nSoftware like ggplot will try to do this for you and it works well for simple cases\nThis can fail with more difficult aggregations or when doing things like applying annotation labels\nA powerful and frustration-reducing technique is to do the aggregation yourself and then use the identity mapping\nThis separates the statistical logic from the visual logic and is often what makes a tricky figure easier to make"
  },
  {
    "objectID": "presentation.html#some-topics",
    "href": "presentation.html#some-topics",
    "title": "Data Visualization",
    "section": "Some Topics",
    "text": "Some Topics\nI’ve prepared a web site at the JHBC GitHub.\nWe can choose our own adventure. The order is somewhat sensible.\n\nExploratory Data Analysis: Exploring data to understand it\nColors & Theming: Accessibilty for the vision impaired, reusable design systems\nCausal Inference & DAGs: Generating DAGs easily to help think about causal structures for causal inference\nSelected Topics: A handful of important topics such as use of position and scales, choosing good visualizations, applications\nRefine & Polish: Using R and ggplot2 to build up and refine visualizations\nShiny: Interactive dashboards"
  },
  {
    "objectID": "presentation.html#foundations",
    "href": "presentation.html#foundations",
    "title": "Data Visualization",
    "section": "Foundations",
    "text": "Foundations"
  },
  {
    "objectID": "presentation.html#understanding-univariate-data-location-and-spread",
    "href": "presentation.html#understanding-univariate-data-location-and-spread",
    "title": "Data Visualization",
    "section": "Understanding Univariate Data: Location and Spread",
    "text": "Understanding Univariate Data: Location and Spread\n\nFollowing William Cleveland’s systematic approach to exploratory data analysis\nVisual assessment of location, spread, and their relationship:\nQuantiles, residuals, S-F plots, M-D plots, Q-Q plots, and more\n\nView Example"
  },
  {
    "objectID": "presentation.html#applications",
    "href": "presentation.html#applications",
    "title": "Data Visualization",
    "section": "Applications",
    "text": "Applications"
  },
  {
    "objectID": "presentation.html#effective-use-of-position-and-scales",
    "href": "presentation.html#effective-use-of-position-and-scales",
    "title": "Data Visualization",
    "section": "Effective Use of Position and Scales",
    "text": "Effective Use of Position and Scales\n\nWhy pie charts and bubble charts fail: humans are poor at comparing areas and angles\nHow truncated axes and selective time windows can mislead viewers\n\nView Example"
  },
  {
    "objectID": "presentation.html#choropleths-for-spatial-data",
    "href": "presentation.html#choropleths-for-spatial-data",
    "title": "Data Visualization",
    "section": "Choropleths for Spatial Data",
    "text": "Choropleths for Spatial Data\n\nMapping geographic data with color-coded regions\nBest practices for choosing color scales and handling missing data\n\nView Example"
  },
  {
    "objectID": "presentation.html#dot-plots-for-spatial-data",
    "href": "presentation.html#dot-plots-for-spatial-data",
    "title": "Data Visualization",
    "section": "Dot Plots for Spatial Data",
    "text": "Dot Plots for Spatial Data\n\nAlternative to choropleths that avoids area bias\nBetter for comparing magnitudes across geographic regions\n\nView Example"
  },
  {
    "objectID": "presentation.html#distribution-plots",
    "href": "presentation.html#distribution-plots",
    "title": "Data Visualization",
    "section": "Distribution Plots",
    "text": "Distribution Plots\n\nBox plots, violin plots, and ridgeline plots for showing data distributions\nChoosing the right visualization for your data’s characteristics\n\nView Example"
  },
  {
    "objectID": "presentation.html#visualizing-time-trends",
    "href": "presentation.html#visualizing-time-trends",
    "title": "Data Visualization",
    "section": "Visualizing Time Trends",
    "text": "Visualizing Time Trends\n\nTime series plots with smoothing and trend lines\nHandling seasonal patterns and multiple time series\n\nView Example"
  },
  {
    "objectID": "presentation.html#visualizing-correlations-and-models",
    "href": "presentation.html#visualizing-correlations-and-models",
    "title": "Data Visualization",
    "section": "Visualizing Correlations and Models",
    "text": "Visualizing Correlations and Models\n\nScatter plots with regression lines and confidence intervals\nVisualizing interaction effects and predicted probabilities\n\nView Example"
  },
  {
    "objectID": "presentation.html#refining-visualizations",
    "href": "presentation.html#refining-visualizations",
    "title": "Data Visualization",
    "section": "Refining Visualizations",
    "text": "Refining Visualizations"
  },
  {
    "objectID": "presentation.html#prams-data-preparation",
    "href": "presentation.html#prams-data-preparation",
    "title": "Data Visualization",
    "section": "PRAMS Data Preparation",
    "text": "PRAMS Data Preparation\n\nLoading and cleaning CDC PRAMS data\nStructuring data for visualization with ggplot2\n\nView Example"
  },
  {
    "objectID": "presentation.html#ggplot2-fundamentals",
    "href": "presentation.html#ggplot2-fundamentals",
    "title": "Data Visualization",
    "section": "ggplot2 Fundamentals",
    "text": "ggplot2 Fundamentals\n\nBuilding plots layer by layer: aesthetics, geoms, scales\nCreating lollipop charts and emphasizing specific values\n\nView Example"
  },
  {
    "objectID": "presentation.html#iteration-and-aggregation",
    "href": "presentation.html#iteration-and-aggregation",
    "title": "Data Visualization",
    "section": "Iteration and Aggregation",
    "text": "Iteration and Aggregation\n\nImproving plots step-by-step through iteration\nCreating forest plots and faceted visualizations\n\nView Example"
  },
  {
    "objectID": "presentation.html#colors-and-accessibility",
    "href": "presentation.html#colors-and-accessibility",
    "title": "Data Visualization",
    "section": "Colors and Accessibility",
    "text": "Colors and Accessibility\n\nRColorBrewer for data-focused palettes\nViridis for perceptually uniform, colorblind-friendly colors\n\nView Example"
  },
  {
    "objectID": "presentation.html#saving-visualizations",
    "href": "presentation.html#saving-visualizations",
    "title": "Data Visualization",
    "section": "Saving Visualizations",
    "text": "Saving Visualizations\n\nExport formats (PNG, PDF, SVG) and when to use each\nResolution, DPI, and file size considerations\n\nView Example"
  },
  {
    "objectID": "presentation.html#advanced-topics",
    "href": "presentation.html#advanced-topics",
    "title": "Data Visualization",
    "section": "Advanced Topics",
    "text": "Advanced Topics"
  },
  {
    "objectID": "presentation.html#causal-inference-dags",
    "href": "presentation.html#causal-inference-dags",
    "title": "Data Visualization",
    "section": "Causal Inference & DAGs",
    "text": "Causal Inference & DAGs\n\nSimulating data from directed acyclic graphs\nTesting assumptions before collecting expensive data\n\nView Example"
  },
  {
    "objectID": "presentation.html#themes-and-staying-dry",
    "href": "presentation.html#themes-and-staying-dry",
    "title": "Data Visualization",
    "section": "Themes and Staying DRY",
    "text": "Themes and Staying DRY\n\nCreating reusable theme functions to avoid repetition\nBuilding a consistent design system across all visualizations\n\nView Example"
  },
  {
    "objectID": "presentation.html#working-with-ai",
    "href": "presentation.html#working-with-ai",
    "title": "Data Visualization",
    "section": "Working with AI",
    "text": "Working with AI"
  },
  {
    "objectID": "presentation.html#ai-llm-for-data-visualization",
    "href": "presentation.html#ai-llm-for-data-visualization",
    "title": "Data Visualization",
    "section": "AI & LLM for Data Visualization",
    "text": "AI & LLM for Data Visualization\n\nUsing language models to assist with data cleaning and visualization\nBest practices and limitations when working with AI coding assistants\n\nView Example"
  },
  {
    "objectID": "presentation.html#questions",
    "href": "presentation.html#questions",
    "title": "Data Visualization",
    "section": "Questions?",
    "text": "Questions?\nContact: ewestlund@jhu.edu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "This site contains materials for learning data visualization in R using ggplot2 and other tools. It was assembled for the SPARC data visualization seminar (2025)."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data Visualization",
    "section": "",
    "text": "This site contains materials for learning data visualization in R using ggplot2 and other tools. It was assembled for the SPARC data visualization seminar (2025)."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Data Visualization",
    "section": "Contents",
    "text": "Contents\n\nPresentation: Data Visualization\n\nThe substance, statistics, and design behind good, honest graphics\n\nEdward Tufte’s principles of graphical excellence\nTooling and workflow (R, ggplot2, scientific notebooks)\nThe grammar of graphics\nSelected topics\n\n\n\n\nExamples & Illustrations\n\nUnderstanding Univariate Data: Location and Spread - Following William Cleveland’s systematic approach to exploratory data analysis\nEffective Use of Position and Scales - Understanding how scale choices affect interpretation\nColors and Accessibility - RColorBrewer, Viridis, and custom palettes\nThemes and Staying DRY - Reusable design systems\nMaking DAGs - Simulating data from causal models\nChoropleths for Spatial Data - Mapping geographic data\nDot Plots for Spatial Data - Alternative spatial visualizations\nDistribution Plots - Showing data distributions effectively\nVisualizing Time Trends - Time series best practices\nVisualizing Correlations and Models - Regression and relationship plots\nSaving Visualizations - Export formats and best practices\n\n\nRefining Visualizations with ggplot2 and PRAMS Data\n\nPRAMS Data Preparation - Preparing data for visualization\nggplot2 Fundamentals - Core concepts and workflow\nIteration and Aggregation - Building better plots step-by-step\n\n\n\nAdvanced Topics\n\nShiny Dashboards - Interactive web applications (requires running locally with R). See examples/shiny_example.R"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Data Visualization",
    "section": "Data",
    "text": "Data\nThe repository contains two main data directories:\n\ndata/raw/: Contains the original, unmodified data files used in the course\ndata/processed/: Contains processed and cleaned versions of the data, ready for analysis\n\n\nPRAMS Data\nThe Pregnancy Risk Assessment Monitoring System (PRAMS) data used in the examples on Day 2 is not included in the repository due to its large size (150MB+). However, you can download it directly from the CDC’s data portal:\nCDC PRAMStat Data for 2011\nThis dataset contains state-level maternal health data from 2011, including information about maternal behaviors and experiences before, during, and after pregnancy. It is used in the a"
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Data Visualization",
    "section": "Credits",
    "text": "Credits\n\nA major debt is owed to Kieran Healy for his Data Visualization book.\nMany of the materials here were first developed for the Johns Hopkins Maternal Health Data Innovation and Coordinating Hub of the NIH Maternal Health Research Centers of Excellence.\nLLMs were used to aid in creation of many of the visuals in this course. Any mistakes made by LLMs are my responsibility.\n\n\nContact\nErik Westlund Johns Hopkins Biostatistics Center ewestlund@jhu.edu"
  },
  {
    "objectID": "examples/prams_3_iteration_aggregation.html",
    "href": "examples/prams_3_iteration_aggregation.html",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "",
    "text": "# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"ggplot2\", \"forcats\", \"kableExtra\", \"readr\", \"ggtext\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(ggtext)\n\nsource(here::here(\"examples\", \"colors.R\"))"
  },
  {
    "objectID": "examples/prams_3_iteration_aggregation.html#goal",
    "href": "examples/prams_3_iteration_aggregation.html#goal",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Goal",
    "text": "Goal\nIn this file, we’ll show:\n\nHow to iterate on a figure to make it more informative and visually appealing.\nThe difference between using stat = \"identity\" and other statistical transformations."
  },
  {
    "objectID": "examples/prams_3_iteration_aggregation.html#research-question",
    "href": "examples/prams_3_iteration_aggregation.html#research-question",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Research Question",
    "text": "Research Question\nLet’s examine the relationship between depression in the 3 months before birth and the number of previous live births, focusing specifically on Wisconsin."
  },
  {
    "objectID": "examples/prams_3_iteration_aggregation.html#data-preparation",
    "href": "examples/prams_3_iteration_aggregation.html#data-preparation",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, let’s load and prepare our data:\n\ndf_final &lt;- readRDS(here::here(\"data\", \"processed\", \"cdc_prams_df_final.rds\"))\n\n# Filter for Wisconsin and relevant variables\ndf_wi &lt;- df_final |&gt;\n  filter(\n    location_abbr == \"WI\",\n    subgroup_cat == \"Number of Previous Live Births\"\n  ) |&gt;\n  select(\n    location_abbr,\n    subgroup,\n    depression_within_3_months_birth\n  )\n\n# Let's look at the structure of our filtered data\ndf_wi |&gt;\n  glimpse()\n\nRows: 2\nColumns: 3\n$ location_abbr                    &lt;fct&gt; WI, WI\n$ subgroup                         &lt;chr&gt; \"0\", \"1 or more\"\n$ depression_within_3_months_birth &lt;dbl&gt; 13.8, 13.0\n\n\nLet’s examine the unique values in our key variables:\n\n# Check unique values in depression variable\ndf_wi |&gt;\n  select(depression_within_3_months_birth) |&gt;\n  distinct() |&gt;\n  kable()\n\n\n\n\ndepression_within_3_months_birth\n\n\n\n\n13.8\n\n\n13.0\n\n\n\n\n# Check unique values in subgroup (number of previous live births)\ndf_wi |&gt;\n  select(subgroup) |&gt;\n  distinct() |&gt;\n  kable()\n\n\n\n\nsubgroup\n\n\n\n\n0\n\n\n1 or more\n\n\n\n\n\nNow let’s create our first visualization:\n\np1 &lt;- df_wi |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    fill = colors$blue[['100']],\n    color = colors$blue[['400']],\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"Wisconsin PRAMS Data\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\np1"
  },
  {
    "objectID": "examples/prams_3_iteration_aggregation.html#faceting-by-small-multiples",
    "href": "examples/prams_3_iteration_aggregation.html#faceting-by-small-multiples",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Faceting By Small Multiples",
    "text": "Faceting By Small Multiples\nA great strategy for showing patterns by groups is to facet by small multiples.\nLet’s start again with our data, not filtering by Wisconsin, but showing little bar charts for each state.\n\np2 &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    fill = colors$blue[['100']],\n    color = colors$blue[['400']],\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  facet_wrap(~ location_abbr) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"By State\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.text = element_text(face = \"bold\")\n  )\n\np2\n\nWarning: Removed 32 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\nWarning: Removed 32 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nThis a start. it has issues, though. First, we have a lot of missing states. Let’s fix that first.\n\np3 &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    fill = colors$blue[['100']],\n    color = colors$blue[['400']],\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  facet_wrap(~ location_abbr) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"By State\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.text = element_text(face = \"bold\")\n  )\n\np3\n\n\n\n\n\n\n\n\nThis is an improvement, but we see some issues. First, our y-axis is too short, and annotations are getting cut off. We can fix our y axis to go to 20\n\np4 &lt;- p3 +\n  scale_y_continuous(limits = c(0, 20))\n\np4\n\n\n\n\n\n\n\n\nThat’s better. Some stylistic adjustments:\n\nNeed more space between the labels and the plot:\n\n\np5 &lt;- p4 +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\np5\n\n\n\n\n\n\n\n\nNow, let’s sort it by the difference between the birth. This requires data transformation, so we’ll start from scratch.\n\n# Calculate differences and sort states\nstate_diffs &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  group_by(location_abbr) |&gt;\n  summarize(\n    first_birth = depression_within_3_months_birth[subgroup == \"0\"],\n    later_births = mean(depression_within_3_months_birth[subgroup != \"0\"]),\n    diff = first_birth - later_births\n  ) |&gt;\n  arrange(desc(diff))\n\n# Create ordered factor for states\nstate_order &lt;- state_diffs$location_abbr\n\n# Create plot with sorted states\np6 &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  mutate(location_abbr = factor(location_abbr, levels = state_order)) |&gt;\n  left_join(\n    state_diffs |&gt; select(location_abbr, diff),\n    by = \"location_abbr\"\n  ) |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    aes(fill = diff &gt; 0, color = diff &gt; 0),\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  scale_fill_manual(\n    name = \"Difference\",\n    values = c(\n      \"TRUE\" = colors$blue[['100']],\n      \"FALSE\" = colors$red[['100']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_color_manual(\n    name = \"Difference\",\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  facet_wrap(~ location_abbr) +\n  scale_y_continuous(limits = c(0, 20)) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"States ordered by difference between first birth and subsequent births\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.text = element_text(face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10)),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np6\n\n\n\n\n\n\n\n\nThis is interesting, but I wonder if we’re using the right graph.\nIf we’re interested in the difference, we can do a forest plot by state in a single plot. Let’s start with a basic version.\n\n# Calculate differences for forest plot\nforest_data &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  group_by(location_abbr) |&gt;\n  summarize(\n    first_birth = depression_within_3_months_birth[subgroup == \"0\"],\n    later_births = mean(depression_within_3_months_birth[subgroup != \"0\"]),\n    diff = first_birth - later_births\n  ) |&gt;\n  arrange(diff)  # Sort by difference\n\n# Create ordered factor for states\nstate_order &lt;- forest_data$location_abbr\n\n# Create forest plot\np7 &lt;- forest_data |&gt;\n  mutate(location_abbr = factor(location_abbr, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location_abbr)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location_abbr,\n      yend = location_abbr,\n      color = diff &gt; 0\n    ),\n    linewidth = 1\n  ) +\n  scale_color_manual(\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np7\n\n\n\n\n\n\n\n\nThis is a decent start. Let’s add text annotations to the plot. Let’s also use the location rather than the abbrevaition.\n\n# Calculate differences for forest plot\nforest_data &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  group_by(location_abbr, location) |&gt;\n  summarize(\n    first_birth = depression_within_3_months_birth[subgroup == \"0\"],\n    later_births = mean(depression_within_3_months_birth[subgroup != \"0\"]),\n    diff = first_birth - later_births,\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(diff)  # Sort by difference\n\n# Create ordered factor for states\nstate_order &lt;- forest_data$location\n\n# Create forest plot\np8 &lt;- forest_data |&gt;\n  mutate(location = factor(location, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location,\n      yend = location,\n      color = diff &gt; 0\n    ),\n    linewidth = 1\n  ) +\n  geom_text(\n    aes(\n      x = diff,\n      label = sprintf(\"%+.1f%%\", diff)\n    ),\n    hjust = ifelse(forest_data$diff &gt; 0, -0.3, 1.3),\n    size = 3.5,\n    fontface = \"bold\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np8\n\n\n\n\n\n\n\n\nThe spacing is a little wide between the annotations. Let’s also make them bold.\n\np9 &lt;- forest_data |&gt;\n  mutate(location = factor(location, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location,\n      yend = location,\n      color = diff &gt; 0\n    ),\n    linewidth = 1\n  ) +\n  geom_text(\n    aes(\n      x = diff,\n      label = sprintf(\"%+.1f%%\", diff)\n    ),\n    hjust = ifelse(forest_data$diff &gt; 0, -0.3, 1.3),\n    size = 3.5,\n    fontface = \"bold\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np9\n\n\n\n\n\n\n\n\nLet’s step back and think.\n\nThe subtitle does the work of the caption. We can remove it and simplify the code.\nThe bold is too much. Let’s remove it.\nWe need more top margin on our y axis.\n\n\np10 &lt;- forest_data |&gt;\n  mutate(location = factor(location, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location,\n      yend = location,\n      color = ifelse(diff &gt; 0, \"positive\", \"negative\")\n    ),\n    linewidth = 1\n  ) +\n  geom_text(\n    aes(\n      x = diff,\n      label = sprintf(\"%+.1f%%\", diff)\n    ),\n    hjust = ifelse(forest_data$diff &gt; 0, -0.3, 1.3),\n    size = 3.5\n  ) +\n  scale_color_manual(\n    values = c(\n      \"positive\" = colors$blue[['400']],\n      \"negative\" = colors$red[['400']]\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"none\",\n    axis.text.y = element_text(margin = margin(r = 10))\n  )\n\np10"
  },
  {
    "objectID": "examples/prams_3_iteration_aggregation.html#thinking-about-aggregations-transformations",
    "href": "examples/prams_3_iteration_aggregation.html#thinking-about-aggregations-transformations",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Thinking About Aggregations & Transformations",
    "text": "Thinking About Aggregations & Transformations\nLet’s create a simulated dataset to illustrate how we were using stat = \"identity\" above. We’ll create individual-level data that mimics our PRAMS data structure.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create simulated data\nsim_data &lt;- tibble(\n  state = rep(state.name, each = 100)  # 100 people per state\n) |&gt;\n  mutate(\n    person_id = row_number(),  # Generate unique IDs\n    # Generate number of previous births using Poisson distribution (mean = 1)\n    previous_births_count = rpois(n(), lambda = 1),\n    # Create factor for 0 vs 1+ births\n    previous_births = factor(\n      ifelse(previous_births_count == 0, \"0\", \"1+\"),\n      levels = c(\"0\", \"1+\")\n    ),\n\n    # Generate depression status with different probabilities based on previous births\n    # Higher probability of depression for first births (0) than subsequent births (1+)\n    depression_prob = ifelse(previous_births == \"0\", 0.15, 0.10),  # 15% vs 10% base rates\n    depression = rbinom(n(), 1, depression_prob) == 1  # Convert to TRUE/FALSE\n  ) |&gt;\n  select(-depression_prob)\n\n# Let's look at the structure\nsim_data |&gt;\n  glimpse()\n\nRows: 5,000\nColumns: 5\n$ state                 &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala…\n$ person_id             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ previous_births_count &lt;int&gt; 0, 2, 1, 2, 3, 0, 1, 2, 1, 1, 3, 1, 1, 1, 0, 2, …\n$ previous_births       &lt;fct&gt; 0, 1+, 1+, 1+, 1+, 0, 1+, 1+, 1+, 1+, 1+, 1+, 1+…\n$ depression            &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n\nsim_data |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nstate\nperson_id\nprevious_births_count\nprevious_births\ndepression\n\n\n\n\nAlabama\n1\n0\n0\nFALSE\n\n\nAlabama\n2\n2\n1+\nTRUE\n\n\nAlabama\n3\n1\n1+\nFALSE\n\n\nAlabama\n4\n2\n1+\nFALSE\n\n\nAlabama\n5\n3\n1+\nFALSE\n\n\nAlabama\n6\n0\n0\nFALSE\n\n\n\n\n# Quick summary to verify our data\nsim_data |&gt;\n  group_by(state, previous_births) |&gt;\n  summarize(\n    n = n(),\n    depression_rate = mean(depression) * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\nstate\nprevious_births\nn\ndepression_rate\n\n\n\n\nAlabama\n0\n35\n20.000000\n\n\nAlabama\n1+\n65\n12.307692\n\n\nAlaska\n0\n33\n18.181818\n\n\nAlaska\n1+\n67\n5.970149\n\n\nArizona\n0\n39\n15.384615\n\n\nArizona\n1+\n61\n6.557377"
  },
  {
    "objectID": "examples/prams_3_iteration_aggregation.html#recreating-visualizations-with-simulated-data",
    "href": "examples/prams_3_iteration_aggregation.html#recreating-visualizations-with-simulated-data",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Recreating Visualizations with Simulated Data",
    "text": "Recreating Visualizations with Simulated Data\nLet’s recreate our visualizations using the simulated data, starting with Wisconsin.\nBelow we are using the stat = \"summary\" to aggregate the data and the scales functionality to format the y axis as a percentage.\n\n# Filter for Wisconsin\nwi_data &lt;- sim_data |&gt;\n  filter(state == \"Wisconsin\") |&gt;\n  mutate(depression = as.numeric(depression))\n\n# Approach 1: Using scales::percent_format()\np1_sim &lt;- wi_data |&gt;\n  ggplot(aes(x = previous_births, y = depression)) +\n  geom_bar(\n    stat = \"summary\",  # We let ggplot2 do the aggregation for us\n    fun = \"mean\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  )\n\np1_sim\n\n\n\n\n\n\n\n\nAn alternative approach is – similar to above – is to do the aggregation ourselves and use identity.\n\n# Approach 2: Multiplying by 100 in the aggregation\np1_sim_self_aggregate &lt;- wi_data |&gt;\n  group_by(previous_births) |&gt;\n  summarize(\n    depression_rate = mean(depression) * 100,  # Convert to percentage\n    .groups = \"drop\"\n  ) |&gt;\n  ggplot(aes(x = previous_births, y = depression_rate)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  )\n\n# Display both plots\n\np1_sim_self_aggregate\n\n\n\n\n\n\n\n\nLife tends to be easier when you summarize the data yourself. I find it more explicit and it avoids confusion from trying to figure out exactly what ggplot2 is doing."
  },
  {
    "objectID": "examples/applications_4_distribution_plots.html",
    "href": "examples/applications_4_distribution_plots.html",
    "title": "Application 4: Distribution Plots",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"ggridges\",\n  \"kableExtra\",\n  \"purrr\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}"
  },
  {
    "objectID": "examples/applications_4_distribution_plots.html#distribution",
    "href": "examples/applications_4_distribution_plots.html#distribution",
    "title": "Application 4: Distribution Plots",
    "section": "Distribution",
    "text": "Distribution\nThe distribution or spread of data is often of comparable importance to measures of central tendency. There are numerous visualizations that can be used to understand distributions. Below we will explore box plots, violin plots, and ridgeline plots."
  },
  {
    "objectID": "examples/applications_4_distribution_plots.html#simulate-physical-activity-data",
    "href": "examples/applications_4_distribution_plots.html#simulate-physical-activity-data",
    "title": "Application 4: Distribution Plots",
    "section": "Simulate Physical Activity Data",
    "text": "Simulate Physical Activity Data\nLet’s simulate data on weekly minutes of moderate-to-vigorous physical activity (MVPA) across different job types in New England states.\n\nset.seed(123)\n\n# Number of observations\nn &lt;- 5000\n\n# Function to generate data with outliers\ngenerate_with_outliers &lt;- function(n, mean, sd, outlier_prob = 0.1, outlier_sd = 3) {\n  # Generate main distribution\n  main_data &lt;- rnorm(n, mean, sd)\n\n  # Add outliers\n  is_outlier &lt;- runif(n) &lt; outlier_prob\n  outliers &lt;- rnorm(n, mean, sd * outlier_sd)\n\n  # Combine\n  ifelse(is_outlier, outliers, main_data)\n}\n\n# Function to generate bimodal distribution\ngenerate_bimodal &lt;- function(n, mean1, mean2, sd1, sd2, prob1 = 0.5) {\n  # Generate two normal distributions\n  dist1 &lt;- rnorm(n, mean1, sd1)\n  dist2 &lt;- rnorm(n, mean2, sd2)\n\n  # Randomly choose between the two distributions\n  is_dist1 &lt;- runif(n) &lt; prob1\n\n  # Combine\n  ifelse(is_dist1, dist1, dist2)\n}\n\n# Simulate data\nactivity_data &lt;- tibble(\n  # Job types\n  job_type = sample(c(\"Office Work\", \"Service Industry\", \"Healthcare\", \"Construction\", \"Education\", \"Professional Athlete\"),\n                   size = n,\n                   replace = TRUE,\n                   prob = c(0.25, 0.25, 0.2, 0.15, 0.1, 0.05))\n) |&gt;\n  # Generate MVPA minutes based on job type\n  mutate(\n    # Base MVPA with job-specific variance and outliers\n    mvpa = case_when(\n      job_type == \"Office Work\" ~ rnorm(n, mean = 80, sd = 30),\n      job_type == \"Service Industry\" ~ generate_with_outliers(n, mean = 130, sd = 35, outlier_prob = 0.15),\n      job_type == \"Healthcare\" ~ generate_bimodal(n,\n                                                 mean1 = 120, sd1 = 30,  # First mode (more sedentary)\n                                                 mean2 = 280, sd2 = 40,  # Second mode (more active)\n                                                 prob1 = 0.6),           # 60% in first mode\n      job_type == \"Education\" ~ rnorm(n, mean = 160, sd = 40),\n      job_type == \"Construction\" ~ rnorm(n, mean = 250, sd = 20),\n      job_type == \"Professional Athlete\" ~ rnorm(n, mean = 450, sd = 30)\n    ),\n    # Ensure non-negative values\n    mvpa = pmax(0, mvpa)\n  )\n\n# Show first few rows\nactivity_data |&gt; head() |&gt; kable()\n\n\n\n\njob_type\nmvpa\n\n\n\n\nOffice Work\n59.63577\n\n\nConstruction\n274.26397\n\n\nOffice Work\n58.86456\n\n\nEducation\n177.27274\n\n\nEducation\n194.10753\n\n\nService Industry\n86.05837"
  },
  {
    "objectID": "examples/applications_4_distribution_plots.html#box-plots",
    "href": "examples/applications_4_distribution_plots.html#box-plots",
    "title": "Application 4: Distribution Plots",
    "section": "Box Plots",
    "text": "Box Plots\nBox plots are great for showing the distribution of a continuous variable across different groups. They show: - Median (middle line) - Interquartile range (box) - Range (whiskers) - Outliers (points)\n\n# Create box plots by job type\np_box &lt;- ggplot(activity_data, aes(x = job_type, y = mvpa, fill = job_type)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_viridis_d(option = \"plasma\") +\n  labs(\n    title = \"Weekly Minutes of Moderate-to-Vigorous Physical Activity by Job Type\",\n    x = \"Job Type\",\n    y = \"Minutes per Week\",\n    fill = \"Job Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 8, face = \"bold\"),\n    legend.text = element_text(size = 8)\n  )\n\np_box"
  },
  {
    "objectID": "examples/applications_4_distribution_plots.html#violin-plots",
    "href": "examples/applications_4_distribution_plots.html#violin-plots",
    "title": "Application 4: Distribution Plots",
    "section": "Violin Plots",
    "text": "Violin Plots\nViolin plots show the full distribution of the data, not just the summary statistics. They’re particularly useful for: - Seeing the shape of the distribution - Identifying multiple modes - Comparing distributions across groups\n\n# Create violin plots by job type\np_violin &lt;- ggplot(activity_data, aes(x = job_type, y = mvpa, fill = job_type)) +\n  geom_violin(alpha = 0.7) +\n  scale_fill_viridis_d(option = \"plasma\") +\n  labs(\n    title = \"Weekly Minutes of Moderate-to-Vigorous Physical Activity by Job Type\",\n    x = \"Job Type\",\n    y = \"Minutes per Week\",\n    fill = \"Job Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 8, face = \"bold\"),\n    legend.text = element_text(size = 8)\n  )\n\np_violin"
  },
  {
    "objectID": "examples/applications_4_distribution_plots.html#violin-box-plot",
    "href": "examples/applications_4_distribution_plots.html#violin-box-plot",
    "title": "Application 4: Distribution Plots",
    "section": "Violin-Box Plot",
    "text": "Violin-Box Plot\n\n# Create violin-box plot\np_violin_box &lt;- ggplot(activity_data, aes(x = mvpa, y = reorder(job_type, mvpa, FUN = median), fill = job_type)) +\n  # Add violin plot\n  geom_violin(\n    alpha = 0.7,\n    scale = \"width\"\n  ) +\n  # Add boxplot\n  geom_boxplot(\n    width = 0.2,\n    alpha = 0.7,\n    fill = \"white\",\n    outlier.shape = NA\n  ) +\n  # Customize scales\n  scale_x_continuous(\n    name = \"Weekly MVPA Minutes\",\n    limits = c(0, 600),\n    breaks = seq(0, 600, 100)\n  ) +\n  scale_fill_viridis_d(option = \"magma\") +\n  # Add labels\n  labs(\n    title = \"Distribution of Weekly Physical Activity by Job Type\",\n    caption = \"Box shows median and quartiles, violin shows full distribution\",\n    y = \"Job Type\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 10, hjust = 0.5, margin = margin(t = 20)),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"none\"\n  )\n\np_violin_box"
  },
  {
    "objectID": "examples/applications_4_distribution_plots.html#ridgeline-plots",
    "href": "examples/applications_4_distribution_plots.html#ridgeline-plots",
    "title": "Application 4: Distribution Plots",
    "section": "Ridgeline Plots",
    "text": "Ridgeline Plots\nRidgeline plots are great for comparing distributions across many groups. They’re particularly useful when: - You have many groups to compare - You want to see the full distribution for each group - You want to identify patterns across groups\n\n# Create ridgeline plot\np_ridge &lt;- ggplot(activity_data, aes(x = mvpa, y = reorder(job_type, mvpa, FUN = median), fill = after_stat(x))) +\n  # Add density ridges\n  geom_density_ridges_gradient(\n    scale = 3,\n    alpha = 0.7,\n    quantile_lines = TRUE,\n    quantiles = 2,\n    show.legend = TRUE\n  ) +\n  # Customize scales\n  scale_x_continuous(\n    name = \"Weekly MVPA Minutes\",\n    limits = c(0, 600),\n    breaks = seq(0, 600, 100)\n  ) +\n  scale_fill_viridis_c(option = \"magma\", name = \"Minutes\") +\n  # Add labels\n  labs(\n    title = \"Distribution of Weekly Physical Activity by Job Type\",\n    caption = \"Vertical lines show median minutes of activity for each job type\",\n    y = \"Job Type\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 10, hjust = 0.5, margin = margin(t = 20)),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.title = element_text(size = 8, face = \"bold\"),\n    legend.text = element_text(size = 8)\n  )\n\np_ridge"
  },
  {
    "objectID": "examples/saving_visualizations.html",
    "href": "examples/saving_visualizations.html",
    "title": "Application 7: Saving Visualizations",
    "section": "",
    "text": "There are several ways to save visualizations in R, each with its own advantages. Here we’ll cover the most common approaches.\n\n\nThe ggsave() function is the most straightforward way to save ggplot2 visualizations. It automatically detects the file type from the extension and saves with appropriate settings.\n\n# Basic usage - saves last plot\nggsave(\"figures/correlation_plot.png\", width = 10, height = 8, dpi = 300)\n\n# Save a specific plot\np &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggsave(\"figures/custom_plot.png\", p, width = 10, height = 8, dpi = 300)\n\n# Save with different formats\nggsave(\"figures/plot.pdf\", p, width = 10, height = 8)  # PDF\nggsave(\"figures/plot.svg\", p, width = 10, height = 8)  # SVG\nggsave(\"figures/plot.jpg\", p, width = 10, height = 8, quality = 0.9)  # JPEG\n\n\n\n\nFor base R graphics, you can use functions like png(), pdf(), jpeg(), etc.:\n\n# Save a base R plot\npng(\"figures/base_plot.png\", width = 1000, height = 800, res = 100)\nplot(1:10, 1:10)\ndev.off()\n\n# Save multiple plots to PDF\npdf(\"figures/multiple_plots.pdf\", width = 10, height = 8)\nplot(1:10, 1:10)\nplot(1:10, 10:1)\ndev.off()\n\n\n\n\nThe Cairo package provides high-quality graphics with better font rendering:\n\n# Install and load Cairo\nif (!require(Cairo)) install.packages(\"Cairo\")\nlibrary(Cairo)\n\n# Save with Cairo\nCairoPNG(\"figures/cairo_plot.png\", width = 1000, height = 800, dpi = 100)\nplot(1:10, 1:10)\ndev.off()\n\n\n\n\nAbove all, consider your audience. For example, read the submission guidelines for the academic journal you are targeting. (It’s worth also considering the audience before you overpolish a figure!)\nHere are some best practices for saving visuals:\n\nResolution and Size:\n\nFor web: 72-96 DPI\nFor print: 300-600 DPI\nFor presentations: 150-200 DPI\n\nFile Formats:\n\nPNG: Best for web, supports transparency\nPDF: Best for print, scalable\nSVG (vector graphics): Best for web, when it works\nJPEG: Best for photographs, smaller file size\n\n\n\n\n\nTo save multiple plots efficiently:\n\n# Save multiple plots to a single PDF\npdf(\"figures/all_plots.pdf\", width = 10, height = 8)\nprint(p)  # First plot\nprint(p + theme_dark())  # Second plot\ndev.off()\n\n# Save multiple plots to separate files\nplots &lt;- list(p1 = p, p2 = p + theme_dark())\nfor (i in seq_along(plots)) {\n  ggsave(\n    sprintf(\"figures/plot_%d.png\", i),\n    plots[[i]],\n    width = 10,\n    height = 8,\n    dpi = 300\n  )\n}"
  },
  {
    "objectID": "examples/saving_visualizations.html#saving-visualizations",
    "href": "examples/saving_visualizations.html#saving-visualizations",
    "title": "Application 7: Saving Visualizations",
    "section": "",
    "text": "There are several ways to save visualizations in R, each with its own advantages. Here we’ll cover the most common approaches.\n\n\nThe ggsave() function is the most straightforward way to save ggplot2 visualizations. It automatically detects the file type from the extension and saves with appropriate settings.\n\n# Basic usage - saves last plot\nggsave(\"figures/correlation_plot.png\", width = 10, height = 8, dpi = 300)\n\n# Save a specific plot\np &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggsave(\"figures/custom_plot.png\", p, width = 10, height = 8, dpi = 300)\n\n# Save with different formats\nggsave(\"figures/plot.pdf\", p, width = 10, height = 8)  # PDF\nggsave(\"figures/plot.svg\", p, width = 10, height = 8)  # SVG\nggsave(\"figures/plot.jpg\", p, width = 10, height = 8, quality = 0.9)  # JPEG\n\n\n\n\nFor base R graphics, you can use functions like png(), pdf(), jpeg(), etc.:\n\n# Save a base R plot\npng(\"figures/base_plot.png\", width = 1000, height = 800, res = 100)\nplot(1:10, 1:10)\ndev.off()\n\n# Save multiple plots to PDF\npdf(\"figures/multiple_plots.pdf\", width = 10, height = 8)\nplot(1:10, 1:10)\nplot(1:10, 10:1)\ndev.off()\n\n\n\n\nThe Cairo package provides high-quality graphics with better font rendering:\n\n# Install and load Cairo\nif (!require(Cairo)) install.packages(\"Cairo\")\nlibrary(Cairo)\n\n# Save with Cairo\nCairoPNG(\"figures/cairo_plot.png\", width = 1000, height = 800, dpi = 100)\nplot(1:10, 1:10)\ndev.off()\n\n\n\n\nAbove all, consider your audience. For example, read the submission guidelines for the academic journal you are targeting. (It’s worth also considering the audience before you overpolish a figure!)\nHere are some best practices for saving visuals:\n\nResolution and Size:\n\nFor web: 72-96 DPI\nFor print: 300-600 DPI\nFor presentations: 150-200 DPI\n\nFile Formats:\n\nPNG: Best for web, supports transparency\nPDF: Best for print, scalable\nSVG (vector graphics): Best for web, when it works\nJPEG: Best for photographs, smaller file size\n\n\n\n\n\nTo save multiple plots efficiently:\n\n# Save multiple plots to a single PDF\npdf(\"figures/all_plots.pdf\", width = 10, height = 8)\nprint(p)  # First plot\nprint(p + theme_dark())  # Second plot\ndev.off()\n\n# Save multiple plots to separate files\nplots &lt;- list(p1 = p, p2 = p + theme_dark())\nfor (i in seq_along(plots)) {\n  ggsave(\n    sprintf(\"figures/plot_%d.png\", i),\n    plots[[i]],\n    width = 10,\n    height = 8,\n    dpi = 300\n  )\n}"
  },
  {
    "objectID": "examples/git_troubleshooting.html",
    "href": "examples/git_troubleshooting.html",
    "title": "Git Troubleshooting",
    "section": "",
    "text": "Several students had trouble pushing to GitHub with their work. Please try the following steps:"
  },
  {
    "objectID": "examples/git_troubleshooting.html#github-issues",
    "href": "examples/git_troubleshooting.html#github-issues",
    "title": "Git Troubleshooting",
    "section": "",
    "text": "Several students had trouble pushing to GitHub with their work. Please try the following steps:"
  },
  {
    "objectID": "examples/git_troubleshooting.html#using-the-command-line",
    "href": "examples/git_troubleshooting.html#using-the-command-line",
    "title": "Git Troubleshooting",
    "section": "Using The Command Line",
    "text": "Using The Command Line\nIf you having issues with RStudio’s Git functionality, you can try using the command line. Click “Terminal” in the bottom left pane or select “Tools” &gt; “Shell” &gt; “Terminal” from the menu.\nThere you can use the following commands:\ngit add .\ngit commit -m \"Your message\"\ngit push\nIn general, I find that using the command line is more reliable than using RStudio’s Git functionality. It is also faster."
  },
  {
    "objectID": "examples/git_troubleshooting.html#authentication-issues",
    "href": "examples/git_troubleshooting.html#authentication-issues",
    "title": "Git Troubleshooting",
    "section": "Authentication Issues",
    "text": "Authentication Issues\nIf you are having trouble getting GitHub to accept your push after providing it credentials, try the following:\n\nGo to your GitHub account settings.\nClick “Developer settings”\nClick “Personal access tokens”\nClick “Generate new token (Classic)””\nGive it a name and description\nSelect all boxes on “repo”\nClick “Generate token”\nCopy the token\nUse the token as your GitHub password when you push"
  },
  {
    "objectID": "examples/git_troubleshooting.html#remote-issues",
    "href": "examples/git_troubleshooting.html#remote-issues",
    "title": "Git Troubleshooting",
    "section": "Remote Issues",
    "text": "Remote Issues\nGitHub is a git remote. A remote is a copy of a repository that is stored on a different server. If GitHub is your primary remote, it is conventional to call it origin.\nEnsure your remote is set correctly.\nFirst, check out which if any remotes are set:\ngit remote -v\nIf you see nothing, then run:\ngit remote add origin https://github.com/YOUR-USERNAME/data-viz-summer-25.git\nIf you see an unfamiliar remote, you can remove it with:\ngit remote remove origin\nIf you see a remote, you can set it to the correct one with:\ngit remote set-url origin https://github.com/YOUR-USERNAME/data-viz-summer-25.git\nNote that the above call GitHub the origin remote."
  },
  {
    "objectID": "examples/git_troubleshooting.html#branch-issues",
    "href": "examples/git_troubleshooting.html#branch-issues",
    "title": "Git Troubleshooting",
    "section": "Branch Issues",
    "text": "Branch Issues\nSome installations will default to a master branch. Community standards are moving away from the term master branch in favor of a main branch. If you are using master, you can change it to main with:\ngit branch -m master main\nSome commands I have used assume an origin remote and a main branch. If you are using something else, you will need to adjust the commands accordingly.\nYou can then push or pull like so, assuming an origin remote is set:\ngit push origin main\ngit pull origin main"
  },
  {
    "objectID": "examples/git_troubleshooting.html#errors-about-the-materials-in-your-repo-being-more-up-to-date",
    "href": "examples/git_troubleshooting.html#errors-about-the-materials-in-your-repo-being-more-up-to-date",
    "title": "Git Troubleshooting",
    "section": "Errors About The Materials In Your Repo Being More Up To Date",
    "text": "Errors About The Materials In Your Repo Being More Up To Date\nIf you are receiving errors about the materials in your repo being more up to date than your local copy, you can try the following.\nIf you want to pull the latest materials from the repository, you can do so with the following command:\ngit pull origin main\nThis assumes you are using the main branch and an origin remote.\nIt is good to ensure you have saved your work before pulling.\nIf you cannot push your work up, and you are absolutely certain you’ve never pushed any work up, you can force the materials in on the command line:\ngit push origin main --force\nThis will overwrite the materials in the repository with your current work."
  },
  {
    "objectID": "examples/ai_llm_illustration.html",
    "href": "examples/ai_llm_illustration.html",
    "title": "AI & LLM Example",
    "section": "",
    "text": "We’ll use an LLM to help us prepare data for a visualization\nLet’s use the CDC’s data on influenza vaccination coverage for all ages"
  },
  {
    "objectID": "examples/ai_llm_illustration.html#overview",
    "href": "examples/ai_llm_illustration.html#overview",
    "title": "AI & LLM Example",
    "section": "",
    "text": "We’ll use an LLM to help us prepare data for a visualization\nLet’s use the CDC’s data on influenza vaccination coverage for all ages"
  },
  {
    "objectID": "examples/ai_llm_illustration.html#data-preparation",
    "href": "examples/ai_llm_illustration.html#data-preparation",
    "title": "AI & LLM Example",
    "section": "Data Preparation",
    "text": "Data Preparation"
  },
  {
    "objectID": "examples/ai_llm_illustration.html#overview-1",
    "href": "examples/ai_llm_illustration.html#overview-1",
    "title": "AI & LLM Example",
    "section": "Overview",
    "text": "Overview\n\nWe’ll use an LLM to help us prepare data for a visualization\nLet’s use the CDC’s data on influenza vaccination coverage for all ages"
  },
  {
    "objectID": "examples/ai_llm_illustration.html#data-preparation-1",
    "href": "examples/ai_llm_illustration.html#data-preparation-1",
    "title": "AI & LLM Example",
    "section": "Data Preparation",
    "text": "Data Preparation\n\ndf &lt;- read_csv(here::here(\"data\", \"raw\", \"cdc_Influenza_Vaccination_Coverage_for_All_Ages__6__Months__20250610.csv\"))\n\ndf |&gt; glimpse()\n\nRows: 220,729\nColumns: 11\n$ Vaccine              &lt;chr&gt; \"Seasonal Influenza\", \"Seasonal Influenza\", \"Seas…\n$ `Geography Type`     &lt;chr&gt; \"Counties\", \"Counties\", \"Counties\", \"Counties\", \"…\n$ Geography            &lt;chr&gt; \"New Haven\", \"New Haven\", \"New Haven\", \"New Haven…\n$ FIPS                 &lt;chr&gt; \"09009\", \"09009\", \"09009\", \"09009\", \"09009\", \"090…\n$ `Season/Survey Year` &lt;chr&gt; \"2018\", \"2021\", \"2020\", \"2021\", \"2018\", \"2019\", \"…\n$ Month                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 5, 4, 11, 10, 9, 8, 9, 1, 12, 1…\n$ `Dimension Type`     &lt;chr&gt; \"&gt;=18 Years\", \"&gt;=18 Years\", \"Age\", \"Age\", \"Age\", …\n$ Dimension            &lt;chr&gt; \"Non-Medical Setting\", \"Non-Medical Setting\", \"&gt;=…\n$ `Estimate (%)`       &lt;chr&gt; \"45.5\", \"53.0\", \"52.4\", \"50.2\", \"34.0\", \"49.5\", \"…\n$ `95% CI (%)`         &lt;chr&gt; \"43.9 to 47.2\", \"46.0 to 60.9\", \"50.6 to 54.3\", \"…\n$ `Sample Size`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 48, 48, 552, 552, 552, 55…"
  },
  {
    "objectID": "examples/ai_llm_illustration.html#chat-with-the-llm",
    "href": "examples/ai_llm_illustration.html#chat-with-the-llm",
    "title": "AI & LLM Example",
    "section": "Chat with the LLM",
    "text": "Chat with the LLM\n\nData Preparation\nFrom here, we can chat with the LLM to help us prepare the data for a visualization.\nWe’ll start with the most lazy prompt we can, and see how it goes.\nI am completely unqualified for my job. Can you look at the below notebook, clean up the data, and create me a plot of influenze vaccine coverage by age group?\n\nMy job depends on it and I only have five minutes, please help. I just saw the boss's car pull into the parking lot."
  },
  {
    "objectID": "examples/applications_2_choropleths_for_spatial_data.html",
    "href": "examples/applications_2_choropleths_for_spatial_data.html",
    "title": "Application 2: Choropleths",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"readr\",\n  \"dplyr\",\n  \"ggplot2\",\n  \"forcats\",\n  \"tidyr\",\n  \"kableExtra\",\n  \"stringr\",\n  \"ggrepel\",\n  \"maps\",\n  \"usmap\",\n  \"sf\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}\nsource(here::here(\"examples\", \"colors.R\"))"
  },
  {
    "objectID": "examples/applications_2_choropleths_for_spatial_data.html#load-and-prepare",
    "href": "examples/applications_2_choropleths_for_spatial_data.html#load-and-prepare",
    "title": "Application 2: Choropleths",
    "section": "Load and Prepare",
    "text": "Load and Prepare\nAs usual, we start by loading and preparing the data.\n\ndata &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"simulated_data.csv\"))\nstate_data &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"state_data.csv\"))\n\ndata &lt;- data |&gt;\n  mutate(\n    state = as.factor(state),\n    received_comprehensive_postnatal_care = as.numeric(received_comprehensive_postnatal_care),\n    insurance = fct_relevel(insurance, \"no_insurance\"),\n    race_ethnicity = fct_relevel(race_ethnicity, \"white\"),\n    edu = fct_relevel(edu, \"hs\"),\n    job_type = fct_relevel(job_type, \"unemployed\"),\n  )\n\ndata$self_report_income &lt;- factor(data$self_report_income, levels = c(\n  \"$0–$24,999\",\n  \"$25,000–$49,999\",\n  \"$50,000–$74,999\",\n  \"$75,000–$99,999\",\n  \"$100,000–$124,999\",\n  \"$125,000–$149,999\",\n  \"$150,000–$174,999\",\n  \"$175,000+\"\n))\n\n# Set \"$50,000–$75,000\" as the reference category\ndata$self_report_income &lt;- fct_relevel(data$self_report_income, \"$50,000–$75,000\")\n\nWarning: 1 unknown level in `f`: $50,000–$75,000\n\nrace_ethnicity_labels &lt;- c(\"American Indian or Alaska Native\" = \"aian\",\n          \"White\" = \"white\",\n          \"Black\" = \"black\",\n          \"Asian\" = \"asian\",\n          \"Hispanic\" = \"hispanic\",\n          \"Native Hawaiian or Pacific Islander\" = \"nhpi\",\n          \"Other\" = \"other\")\n\ndata |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nprovider_id\nstate\nreceived_comprehensive_postnatal_care\nself_report_income\nage\nedu\nrace_ethnicity\ninsurance\njob_type\ndependents\ndistance_to_provider\nobesity\nmultiple_gestation\ndiabetes\nheart_disease\nplacenta_previa\nhypertension\ngest_hypertension\npreeclampsia\n\n\n\n\n1\n1\nAK\n0\n$25,000–$49,999\n40\nhs\nwhite\nno_insurance\nunskilled\n3\n18.856908\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\nAK\n1\n$75,000–$99,999\n21\nsome_college\nwhite\nno_insurance\nprofessional\n2\n4.096578\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\nAK\n0\n$50,000–$74,999\n29\nhs\nhispanic\nno_insurance\ntrade\n2\n2.841914\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\nAK\n0\n$25,000–$49,999\n31\nhs\nwhite\nno_insurance\nunskilled\n4\n6.459543\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n5\n1\nAK\n0\n$75,000–$99,999\n27\nhs\nwhite\nno_insurance\nunskilled\n3\n10.126509\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1\nAK\n0\n$50,000–$74,999\n17\nless_than_hs\nwhite\nno_insurance\nunemployed\n3\n5.235320\n1\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "examples/applications_2_choropleths_for_spatial_data.html#basic-visualizations",
    "href": "examples/applications_2_choropleths_for_spatial_data.html#basic-visualizations",
    "title": "Application 2: Choropleths",
    "section": "Basic Visualizations",
    "text": "Basic Visualizations\n\nProvider Trust\nFrom our causal model, we theorize three causes of provider trust:\n\nProvider quality\nRace/ethnicity\nCultural orientation (namely, trust in institutions)\n\nWe observe race, but we do not observe provider quality or cultural orientation."
  },
  {
    "objectID": "examples/applications_2_choropleths_for_spatial_data.html#provider-quality",
    "href": "examples/applications_2_choropleths_for_spatial_data.html#provider-quality",
    "title": "Application 2: Choropleths",
    "section": "Provider quality",
    "text": "Provider quality\nFor provider quality, we hypothesize the political and economic conditions with respect to healthcare will influence provider quality. We do not observe provider quality directly, but we do observe what state the provider is in.\nGiven all this, it may help us to see:\n\nA breakdown of race/ethnicity\nA breakdown of receipt of comprehensive care by race/ethnicity\nA breakdown of receipt of comprehensive care by state\nA breakdown of receipt of comprehensive care by race by state\n\n\nRace/ethnicity\nFor categorical variables, we typically first want to see a bar chart of the distribution of the variable.\n\n# Bar chart\nggplot(data, aes(race_ethnicity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n  ggplot(\n    data |&gt;\n      count(race_ethnicity) |&gt;\n      mutate(\n        proportion = n / sum(n),\n        race_ethnicity = fct_recode(\n          factor(race_ethnicity),\n          !!! race_ethnicity_labels\n        )\n      ) |&gt;\n      arrange(desc(proportion)) |&gt;\n      mutate(race_ethnicity = fct_inorder(race_ethnicity) |&gt; fct_rev())\n    ,\n    aes(x = proportion, y = race_ethnicity)\n  ) +\n  geom_bar(stat = \"identity\",\n           fill = colors$blue$`100`,\n           color = colors$blue$`400`) +\n  scale_x_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    expand = expansion(mult = c(0, 0.2))  # Add 20% padding on the right\n  ) +\n  labs(x = \"\", y = \"\", title = \"Patients by Race/Ethnicity\", caption = \"Source: Simulated Data\") +\n  theme_minimal() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n# RCP vs. Race/Ethnicity\nplot_data &lt;- data |&gt;\n  group_by(race_ethnicity) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care),\n    n = n()\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    race_ethnicity = fct_recode(factor(race_ethnicity), !!!race_ethnicity_labels),\n    race_ethnicity = fct_reorder(race_ethnicity, proportion)\n  )\n\n# Plot proportions by race\nggplot(plot_data, aes(x = proportion, y = race_ethnicity)) +\n  geom_bar(stat = \"identity\", fill = colors$blue$`100`, color = colors$blue$`400`) +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Receipt of Comprehensive Postnatal Care by Race/Ethnicity\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )"
  },
  {
    "objectID": "examples/applications_2_choropleths_for_spatial_data.html#choropleth",
    "href": "examples/applications_2_choropleths_for_spatial_data.html#choropleth",
    "title": "Application 2: Choropleths",
    "section": "Choropleth",
    "text": "Choropleth\nWe will now map the receipt of comprehensive postnatal care by state. This is a choropleth map.\nPlease note that you must think hard about whether it makes sense to map the data you are mapping. For example, if you are mapping the number of people who have received comprehensive postnatal care, you must think about whether it makes sense to compare states with different populations. In our case, we know that every state is evenly represented and every person has the same probability of being in the sample. However, these assumptions rarely hold with true to life research designs.\n\nus_map &lt;- us_map(regions = \"states\")\n\nrcp_by_state_data &lt;- data |&gt;\n  group_by(state) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care),\n    n = n()\n  ) |&gt;\n  ungroup()\n\n# Merge summarized data with map data\nrcp_by_state_data_mappable &lt;- us_map |&gt;\n  left_join(\n    rcp_by_state_data,\n    by = c(\"abbr\"=\"state\")\n  )\n\nggplot(data = rcp_by_state_data_mappable) +\n  geom_sf(aes(fill = proportion), color = \"white\") +\n  scale_fill_continuous(\n    low = colors$blue$`50`,\n    high = colors$blue$`900`,\n    name = \"Proportion\",\n    labels = scales::percent,\n  ) +\n  labs(\n    title = \"Proportion Receiving Comprehensive Postnatal Care By State\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )"
  },
  {
    "objectID": "examples/applications_2_choropleths_for_spatial_data.html#small-multiples",
    "href": "examples/applications_2_choropleths_for_spatial_data.html#small-multiples",
    "title": "Application 2: Choropleths",
    "section": "Small Multiples",
    "text": "Small Multiples\nEven choropleths benefit from faceting by small multiples.\n\n# Will it work? Maps  by state\nrace_ethnicity_care_data &lt;- data |&gt;\n  group_by(state, race_ethnicity) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care, na.rm = TRUE),\n    n = n(),\n        .groups = \"drop\"\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    race_ethnicity = fct_recode(race_ethnicity, !!!race_ethnicity_labels)\n  )\n\nrace_ethnicity_care_sf &lt;- us_map |&gt;\n  left_join(\n    race_ethnicity_care_data,\n    by = c(\"abbr\" = \"state\")\n  )\n\nggplot(race_ethnicity_care_sf) +\n  geom_sf(aes(geometry = geom, fill = proportion), color = \"white\", size = 0.1) +\n  scale_fill_continuous(\n    low = colors$blue$`100`,\n    high = colors$blue$`900`,\n    name = \"Proportion\",\n    labels = scales::percent,\n    na.value = \"grey90\"\n  ) +\n  facet_wrap(~ race_ethnicity, ncol = 3, nrow = 3) +\n  labs(\n    title = \"Proportion Receiving Comprehensive Postnatal Care by Race\",\n    fill = \"Proportion\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\"),\n    strip.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nChoropleths can be useful, but they can be misleading. In fact, they often end up encoding things like population density as much as the spatial variation we are trying to visualize."
  },
  {
    "objectID": "examples/applications_3_dot_plot_for_spatial_data.html",
    "href": "examples/applications_3_dot_plot_for_spatial_data.html",
    "title": "Application 3: Dot Plots",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"readr\",\n  \"dplyr\",\n  \"ggplot2\",\n  \"forcats\",\n  \"tidyr\",\n  \"kableExtra\",\n  \"stringr\",\n  \"ggrepel\",\n  \"maps\",\n  \"usmap\",\n  \"sf\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}\n\nsource(here::here(\"examples\", \"colors.R\"))"
  },
  {
    "objectID": "examples/applications_3_dot_plot_for_spatial_data.html#load-and-prepare",
    "href": "examples/applications_3_dot_plot_for_spatial_data.html#load-and-prepare",
    "title": "Application 3: Dot Plots",
    "section": "Load and Prepare",
    "text": "Load and Prepare\nAs usual, we start by loading and preparing the data.\n\ndata &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"simulated_data.csv\"))\nstate_data &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"state_data.csv\"))\n\ndata &lt;- data |&gt;\n  mutate(\n    state = as.factor(state),\n    received_comprehensive_postnatal_care = as.numeric(received_comprehensive_postnatal_care),\n    insurance = fct_relevel(insurance, \"no_insurance\"),\n    race_ethnicity = fct_relevel(race_ethnicity, \"white\"),\n    edu = fct_relevel(edu, \"hs\"),\n    job_type = fct_relevel(job_type, \"unemployed\"),\n  )\n\ndata$self_report_income &lt;- factor(data$self_report_income, levels = c(\n  \"$0–$24,999\",\n  \"$25,000–$49,999\",\n  \"$50,000–$74,999\",\n  \"$75,000–$99,999\",\n  \"$100,000–$124,999\",\n  \"$125,000–$149,999\",\n  \"$150,000–$174,999\",\n  \"$175,000+\"\n))\n\n# Set \"$50,000–$75,000\" as the reference category\ndata$self_report_income &lt;- fct_relevel(data$self_report_income, \"$50,000–$75,000\")\n\nWarning: 1 unknown level in `f`: $50,000–$75,000\n\nrace_ethnicity_labels &lt;- c(\"American Indian or Alaska Native\" = \"aian\",\n          \"White\" = \"white\",\n          \"Black\" = \"black\",\n          \"Asian\" = \"asian\",\n          \"Hispanic\" = \"hispanic\",\n          \"Native Hawaiian or Pacific Islander\" = \"nhpi\",\n          \"Other\" = \"other\")\n\ndata |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nprovider_id\nstate\nreceived_comprehensive_postnatal_care\nself_report_income\nage\nedu\nrace_ethnicity\ninsurance\njob_type\ndependents\ndistance_to_provider\nobesity\nmultiple_gestation\ndiabetes\nheart_disease\nplacenta_previa\nhypertension\ngest_hypertension\npreeclampsia\n\n\n\n\n1\n1\nAK\n0\n$25,000–$49,999\n40\nhs\nwhite\nno_insurance\nunskilled\n3\n18.856908\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\nAK\n1\n$75,000–$99,999\n21\nsome_college\nwhite\nno_insurance\nprofessional\n2\n4.096578\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\nAK\n0\n$50,000–$74,999\n29\nhs\nhispanic\nno_insurance\ntrade\n2\n2.841914\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\nAK\n0\n$25,000–$49,999\n31\nhs\nwhite\nno_insurance\nunskilled\n4\n6.459543\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n5\n1\nAK\n0\n$75,000–$99,999\n27\nhs\nwhite\nno_insurance\nunskilled\n3\n10.126509\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1\nAK\n0\n$50,000–$74,999\n17\nless_than_hs\nwhite\nno_insurance\nunemployed\n3\n5.235320\n1\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "examples/applications_3_dot_plot_for_spatial_data.html#dot-plots",
    "href": "examples/applications_3_dot_plot_for_spatial_data.html#dot-plots",
    "title": "Application 3: Dot Plots",
    "section": "Dot Plots",
    "text": "Dot Plots\nIn applications_2_choropleth.qmd we drew up a choropleth map of the receipt of comprehensive postnatal care by state. Let’s see if a dot plot can help us understand the data better.\n\n# Calculate overall proportions for all states\noverall_care_data &lt;- data |&gt;\n  group_by(state) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(race_ethnicity = \"Overall\")\n\n# Calculate proportions for each race\nrace_care_data &lt;- data |&gt;\n  filter(race_ethnicity %in% c(\"white\", \"black\", \"hispanic\", \"asian\")) |&gt;\n  group_by(state, race_ethnicity) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  )  |&gt;\n  mutate(\n    race_ethnicity = fct_recode(race_ethnicity, !!!race_ethnicity_labels)\n  ) ## will have warning because we dropped some race/eth cats\n\n\n# Combine overall and race-specific data\ndot_plot_data &lt;- bind_rows(overall_care_data, race_care_data)\n\n# Sort states by overall proportion (to ensure consistent order across facets)\nstate_order &lt;- dot_plot_data |&gt;\n  filter(race_ethnicity == \"Overall\") |&gt;\n  arrange(proportion) |&gt;\n  pull(state)\n\ndot_plot_data &lt;- dot_plot_data |&gt;\n  mutate(state = factor(state, levels = state_order)) |&gt;\n  mutate(\n    race_ethnicity = factor(\n      race_ethnicity,\n      levels = c(\"Overall\", \"White\", \"Black\", \"Hispanic\", \"Asian\")\n    )\n  )\n\nggplot(dot_plot_data, aes(x = proportion, y = state)) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  geom_segment(\n    aes(x = 0, xend = proportion, y = state, yend = state),\n    color = colors$blue$`500`,\n    linetype = \"dotted\"\n  ) +\n  facet_wrap(~race_ethnicity, ncol = 5, scales = \"free_y\") +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Proportion Receiving Comprehensive Postnatal Care\",\n    x = \"Proportion\",\n    y = \"State\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 8),\n    strip.text = element_text(size = 11, face = \"bold\"),\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    plot.caption = element_text(size = 10, hjust = 0)\n  )\n\n\n\n\n\n\n\n\nWe find that, in fact, the dot plot is superior to the choropleth map to visualize the spatial trends that exist in the data.\nI would encourage you to always try to visualize spatial data using non-map-based visualizations. In many cases, maps foreground non-causal phenomena like population density.\nFor comparison, compare the above to the choropleth map we made in applications_2_choropleth.qmd, which we reproduce below:"
  },
  {
    "objectID": "examples/applications_1_effective_and_honest_scales.html",
    "href": "examples/applications_1_effective_and_honest_scales.html",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "",
    "text": "This notebook demonstrates how axis scaling and data presentation can be used effectively or to mislead. We’ll explore how to use positions and scales to make data more understandable, particularly with respect to magnitudes.\nWe’ll explore two main techniques to mislead:\n\nTruncated axes in bar plots that exaggerate small differences\nSelective time windows in time series that create misleading trends"
  },
  {
    "objectID": "examples/applications_1_effective_and_honest_scales.html#introduction",
    "href": "examples/applications_1_effective_and_honest_scales.html#introduction",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "",
    "text": "This notebook demonstrates how axis scaling and data presentation can be used effectively or to mislead. We’ll explore how to use positions and scales to make data more understandable, particularly with respect to magnitudes.\nWe’ll explore two main techniques to mislead:\n\nTruncated axes in bar plots that exaggerate small differences\nSelective time windows in time series that create misleading trends"
  },
  {
    "objectID": "examples/applications_1_effective_and_honest_scales.html#position-scale-and-magnitude",
    "href": "examples/applications_1_effective_and_honest_scales.html#position-scale-and-magnitude",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "Position, Scale, and Magnitude",
    "text": "Position, Scale, and Magnitude\n\nset.seed(123)\nn &lt;- 1000\n\n# Create simulated patient data with more distinct patterns\npatient_data &lt;- data.frame(\n  patient_id = 1:n,\n  treatment = sample(c(\"Standard\", \"New\"), n, replace = TRUE),\n  age_group = sample(c(\"Young\", \"Elderly\"), n, replace = TRUE),\n  hospital = sample(c(\"City\", \"County\", \"University\", \"Private\"), n, replace = TRUE)\n)\n\n# Create more distinct base rates by hospital\nhospital_effects &lt;- c(\n  \"City\" = 0.25,      # Higher rates in city hospitals\n  \"County\" = 0.15,    # Moderate rates in county hospitals\n  \"University\" = 0.10, # Lower rates in university hospitals\n  \"Private\" = 0.08    # Lowest rates in private hospitals\n)\n\n# Add random variation and treatment/age effects\npatient_data$readmission_rate &lt;- vapply(1:n, function(i) {\n  h &lt;- patient_data$hospital[i]\n  base_rate &lt;- hospital_effects[h]\n  # Add some random variation\n  rate &lt;- base_rate + rnorm(1, 0, 0.02)\n  # Treatment effect (20% reduction for new treatment)\n  rate &lt;- rate * ifelse(patient_data$treatment[i] == \"New\", 0.8, 1)\n  # Age effect (30% increase for elderly)\n  rate &lt;- rate * ifelse(patient_data$age_group[i] == \"Elderly\", 1.3, 1)\n  return(rate)\n}, numeric(1))\n\n# Ensure rates stay between 0 and 1\npatient_data$readmission_rate &lt;- pmin(pmax(patient_data$readmission_rate, 0), 1)\n\n# Create summary data for bubble chart\nbubble_data &lt;- patient_data |&gt;\n  group_by(hospital, age_group) |&gt;\n  summarize(\n    readmission_rate = mean(readmission_rate),\n    patient_count = n(),\n    .groups = \"drop\"\n  )\n\n# 0. Worst Example: Bubble Chart (Area/Volume)\nggplot(bubble_data, aes(x = hospital, y = age_group)) +\n  geom_point(\n    aes(size = readmission_rate * 100),  # Scale up for better visibility\n    shape = 21,\n    fill = \"#34D399\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", readmission_rate * 100)),\n    size = 3,\n    vjust = -4.5\n  ) +\n  scale_size_continuous(\n    range = c(5, 20),\n    name = \"Readmission Rate (%)\"\n  ) +\n  labs(\n    title = \"Readmission Rates by Hospital and Age Group (Bubble Chart)\",\n    subtitle = \"Why bubble charts are bad: Humans are poor at comparing areas\",\n    x = \"Hospital\",\n    y = \"Age Group\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n# 1. Area/Volume (Why pie charts are bad)\npie_data &lt;- patient_data |&gt;\n  group_by(hospital, treatment) |&gt;\n  summarize(\n    count = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    total = sum(count),\n    proportion = count / total,\n    label = treatment\n  ) |&gt;\n  group_by(hospital) |&gt;\n  mutate(position = cumsum(proportion) - (proportion / 2)) |&gt;\n  ungroup()\n\n# Create pie charts\nstandard_pie &lt;- pie_data |&gt;\n  filter(hospital == \"City\") |&gt;\n  ggplot(aes(x = \"\", y = proportion, fill = treatment)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", fill = \"white\") +\n  coord_polar(theta = \"y\") +\n  geom_text(aes(y = position, label = label), size = 6) +\n  labs(\n    title = \"City Hospital\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)\n  )\n\nuniversity_pie &lt;- pie_data |&gt;\n  filter(hospital == \"University\") |&gt;\n  ggplot(aes(x = \"\", y = proportion, fill = treatment)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", fill = \"white\") +\n  coord_polar(theta = \"y\") +\n  geom_text(aes(y = position, label = label), size = 6) +\n  labs(\n    title = \"University Hospital\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)\n  )\n\n# Display pie charts\n(standard_pie + university_pie) +\n  plot_annotation(\n    title = \"Treatment Distribution by Hospital (Pie Charts)\",\n    subtitle = \"Why pie charts are bad: Humans are poor at comparing areas and angles\",\n    theme = theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 20),\n      plot.subtitle = element_text(hjust = 0.5, size = 14)\n    )\n  )\n\n\n\n\n\n\n\n# 2. Position on non-common scale\nggplot(patient_data, aes(x = hospital, y = readmission_rate)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", fill = \"#34D399\", color = \"black\") +\n  facet_wrap(~ age_group, nrow = 1, scales = \"free_y\") +\n  labs(\n    title = \"Readmission Rates by Hospital (Non-Common Scale)\",\n    subtitle = \"Different scales make comparisons difficult\",\n    x = \"Hospital\",\n    y = \"Readmission Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )\n\n\n\n\n\n\n\n# 3. Position on common scale\nggplot(patient_data, aes(x = hospital, y = readmission_rate)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", fill = \"#34D399\", color = \"black\") +\n  facet_wrap(~ age_group, nrow = 1) +\n  labs(\n    title = \"Readmission Rates by Hospital (Common Scale)\",\n    subtitle = \"Common scale makes comparisons easier and more accurate\",\n    x = \"Hospital\",\n    y = \"Readmission Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )"
  },
  {
    "objectID": "examples/applications_1_effective_and_honest_scales.html#naughty-axes",
    "href": "examples/applications_1_effective_and_honest_scales.html#naughty-axes",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "Naughty (?) Axes",
    "text": "Naughty (?) Axes\nThis section demonstrates how axis scaling can be used to mislead viewers about the magnitude of differences between groups in health outcomes. We’ll use a simulated dataset of treatment effects on adverse events.\nThe key technique here is truncating the y-axis to start above zero, which makes small differences appear much larger than they actually are. This is particularly effective when the actual differences are very small percentages.\nIn the below code, we simulate data of a rare adverse event that occurs in 0.001% of patients. This adverse event is in reality 3x more likely in the treatment group.\nWe use a logistic regression to estimate the treatment effect on the adverse event. We then plot the risk of the adverse event by treatment and age group.\n\n# Force regular decimal notation\noptions(scipen=999)\n\nset.seed(456)\nn &lt;- 1000000\n\ndata &lt;- data.frame(\n  treatment = rbinom(n, 1, 0.5)  # 1 = new treatment, 0 = standard care\n)\n\nbase_prob &lt;- 0.00001 # 0.001% baseline risk\n\ndata$adverse_event &lt;- rbinom(\n  n, 1, ifelse(data$treatment == 1, base_prob * 3, base_prob)\n)\n\nmodel &lt;- glm(adverse_event ~ treatment, data = data, family = \"binomial\")\nmodel |&gt; tidy(exponentiate = TRUE) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000010\n0.4472136\n-25.740124\n0.0000000\n\n\ntreatment\n2.990595\n0.5163997\n2.121365\n0.0338911\n\n\n\n\n\nBelow, we visualize the risk of the adverse event by treatment. We can see the risk of an adverse event is 3x higher in the treatment group, which may be alarming. But note the constrained y-axis.\n\n summary_data &lt;- data |&gt;\n  group_by(treatment) |&gt;\n  summarize(\n    risk = mean(adverse_event),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# Plot with constrained axes\nggplot(summary_data, aes(x = factor(treatment), y = risk, fill = factor(treatment))) +\n  geom_col(width = 0.7) +\n  scale_fill_brewer(palette = \"Set2\", labels = c(\"Standard Care\", \"New Treatment\")) +\n  labs(\n    title = \"Risk of Adverse Event by Treatment\",\n    x = \"Treatment\",\n    y = \"Proportion with Adverse Event\",\n    fill = \"Treatment\"\n  ) +\n  scale_y_continuous(limits = c(0, max(summary_data$risk) * 1.5)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, if we scale the axis to 0-1, we can see that the absolute risk is vanishingly rare.\n\n# Now add age group stratification\n# Plot with full scale\nggplot(summary_data, aes(x = factor(treatment), y = risk, fill = factor(treatment))) +\n  geom_col(width = 0.7) +\n  geom_text(\n    aes(label = paste0(\"p = \", sprintf(\"%.6f\", risk))),\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_fill_brewer(palette = \"Set2\", labels = c(\"Standard Care\", \"New Treatment\")) +\n  labs(\n    title = \"Risk of Adverse Event by Treatment\",\n    subtitle = \"Full scale (0 to 1) showing absolute risk\",\n    x = \"Treatment\",\n    y = \"Probability of Adverse Event\",\n    fill = \"Treatment\"\n  ) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 10),\n    axis.title = element_text(size = 12),\n    strip.text = element_text(size = 11, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nI do not believe that the plot with constrained axes is inherently misleading for certain audiences. For example, those who are familiar with medical research will likely understand that an odds ratio of “3” does mean 3x higher odds of the adverse event, but it does not mean that this is necessarily something the public needs to worry about. It is important to consider how an audience will interpret the plot."
  },
  {
    "objectID": "examples/applications_1_effective_and_honest_scales.html#naughty-time-series-axes",
    "href": "examples/applications_1_effective_and_honest_scales.html#naughty-time-series-axes",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "Naughty Time Series Axes",
    "text": "Naughty Time Series Axes\nThis section demonstrates how time series plots can be misleading when zooming in on specific time periods, using simulated data on Emergency Department bounceback rates as an example. The key technique here is selective windowing, or choosing a specific time period that supports a desired narrative while ignoring the broader context.\n\n# Create explicit data points\nbounceback_data &lt;- data.frame(\n  Year = 1:30,\n  BouncebackRate = c(\n    # First 20 years stable around 15% with noise\n    0.151, 0.149, 0.152, 0.148, 0.153, 0.147, 0.150, 0.152, 0.148, 0.151,\n    0.149, 0.153, 0.147, 0.150, 0.152, 0.148, 0.151, 0.149, 0.153, 0.147,\n    # Year 21-22: Still stable\n    0.151, 0.149,\n    # Year 23-25: Short dip to 12-13%\n    0.128, 0.127, 0.129,\n    # Year 26-30: Back to stable around 15%\n    0.151, 0.149, 0.152, 0.148, 0.153\n  )\n)\n\n# Plot showing full scale\nggplot(bounceback_data, aes(x = Year, y = BouncebackRate)) +\n  geom_line(color = viridis(1)) +\n  geom_point(color = viridis(1)) +\n  geom_text(\n    data = bounceback_data[c(1, 30), ],\n    aes(label = paste0(sprintf(\"%.1f\", BouncebackRate * 100), \"%\")),\n    nudge_y = 0.01,\n    size = 3\n  ) +\n  scale_y_continuous(\n    labels = scales::percent,\n    limits = c(0, 0.2),\n    breaks = seq(0, 0.2, 0.05)\n  ) +\n  labs(\n    title = \"ED Bounceback Rates Over 30 Years\",\n    x = \"Year\",\n    y = \"Bounceback Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n# Misleading plot starting at year 23\nggplot(bounceback_data[bounceback_data$Year &gt;= 23, ], aes(x = Year, y = BouncebackRate)) +\n  geom_line(color = viridis(1)) +\n  geom_point(color = viridis(1)) +\n  geom_text(\n    data = bounceback_data[bounceback_data$Year %in% c(23, 30), ],\n    aes(label = paste0(sprintf(\"%.1f\", BouncebackRate * 100), \"%\")),\n    nudge_y = 0.003,\n    size = 3\n  ) +\n  scale_y_continuous(\n    labels = scales::percent,\n    limits = c(0.12, 0.16),\n    breaks = seq(0.12, 0.16, 0.01)\n  ) +\n  labs(\n    title = \"ED Bounceback Rates Since My Enemy Took Power\",\n    x = \"Year\",\n    y = \"Bounceback Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )"
  },
  {
    "objectID": "presentation.html#purposes-of-visualization-exploration",
    "href": "presentation.html#purposes-of-visualization-exploration",
    "title": "Data Visualization",
    "section": "Purposes of Visualization: Exploration",
    "text": "Purposes of Visualization: Exploration\n\nGoal: Understand the data structure, data generating mechanism, relationships among variables, etc.\nStart here\nDon’t polish; these are for you\nStill consider good practices: you don’t want to misunderstand things because of scale, position, poor color separation, etc."
  },
  {
    "objectID": "presentation.html#purposes-of-visualization-sharing-results",
    "href": "presentation.html#purposes-of-visualization-sharing-results",
    "title": "Data Visualization",
    "section": "Purposes of Visualization: Sharing Results",
    "text": "Purposes of Visualization: Sharing Results\n\nGoal: share your findings\nGet the data and message right\nMake things look nice\nConsider text annotation\nStill don’t go too crazy, especially when the target is a journal that will redo your figures"
  },
  {
    "objectID": "presentation.html#proof-of-concept",
    "href": "presentation.html#proof-of-concept",
    "title": "Data Visualization",
    "section": "Proof of Concept",
    "text": "Proof of Concept\n\nLet’s take the mtcars dataset and see what we can do\n\nI have a plot of auto data. The data set is named \"auto.\" The variables I'm interested are: price, efficiency (variable name: mpg), weight, and manufacturer.  The data set only includes the variable \"make,\" which has both the manufacturer and the car name.  Can you first split \"make\" into \"manufacturer\" and \"model.\" Weight is a continuous measure, but it would be helpful to split this data into categories of small, medium, and large.\n\nI'd then like a plot comparing efficiency to weight. Facet this into small, medium, and large cars. On each figure, annotate the least efficient car with its make and model, one average car with its make and model, and the most efficient car with its make and model.\n\nPut the code for this in a new quarto notebook named llm-example.qmd"
  },
  {
    "objectID": "presentation.html#causal-inference-making-dags",
    "href": "presentation.html#causal-inference-making-dags",
    "title": "Data Visualization",
    "section": "Causal Inference & Making DAGs",
    "text": "Causal Inference & Making DAGs\n\nDAGs made easy\n\nView Example"
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html",
    "href": "examples/applications_7_cleveland_univariate_data.html",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\",\n  \"tukeyedar\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\nlibrary(tukeyedar)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#introduction",
    "href": "examples/applications_7_cleveland_univariate_data.html#introduction",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\",\n  \"tukeyedar\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\nlibrary(tukeyedar)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#simulated-example-data",
    "href": "examples/applications_7_cleveland_univariate_data.html#simulated-example-data",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Simulated Example Data",
    "text": "Simulated Example Data\nWe’ll use a simulated healthcare dataset to explore Cleveland’s concepts. Imagine we’re examining patient recovery times (in days) from a medical procedure across different hospitals.\n\nset.seed(1234)\n\n# Hospital A: Normal distribution, mean recovery time\nhospital_a &lt;- rnorm(200, mean = 14, sd = 3)\n\n# Hospital B: Normal distribution, longer recovery\nhospital_b &lt;- rnorm(200, mean = 18, sd = 3)\n\n# Hospital C: Skewed distribution with some very long recoveries\nhospital_c &lt;- c(\n  rnorm(180, mean = 14, sd = 2.5),\n  runif(20, min = 25, max = 40)  # Some outliers\n)\n\n# Hospital D: Bimodal distribution (two distinct patient groups)\nhospital_d &lt;- c(\n  rnorm(100, mean = 10, sd = 2),\n  rnorm(100, mean = 20, sd = 2)\n)\n\n# Combine into data frame\nrecovery_data &lt;- data.frame(\n  hospital = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 200),\n  recovery_days = c(hospital_a, hospital_b, hospital_c, hospital_d)\n)"
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#location-spread",
    "href": "examples/applications_7_cleveland_univariate_data.html#location-spread",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Location & Spread",
    "text": "Location & Spread\nCleveland emphasized understanding the “location” and “spread” of data, which in more common parlance just means measures of central tendency and variance/dispersoin. Different measures of location and spread tell different stories.\n\n# Calculate measures of location for each hospital\nlocation_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    mean = mean(recovery_days),\n    median = median(recovery_days),\n    q25 = quantile(recovery_days, 0.25),\n    q75 = quantile(recovery_days, 0.75),\n    n = n()\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nlocation_summary |&gt;\n  knitr::kable(caption = \"Measures of Location by Hospital\")\n\n\nMeasures of Location by Hospital\n\n\nhospital\nmean\nmedian\nq25\nq75\nn\n\n\n\n\nA\n13.8\n13.5\n11.7\n15.7\n200\n\n\nB\n18.2\n18.4\n16.3\n20.1\n200\n\n\nC\n15.6\n14.3\n12.0\n16.3\n200\n\n\nD\n14.9\n13.8\n9.8\n19.9\n200\n\n\n\n\n\nSummary statistics help:\n\nHospital A & B: Mean ≈ Median (symmetric distributions)\nHospital C: Mean &gt; Median (right-skewed due to outliers)\nHospital D: Bimodal pattern obscures single “center”"
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "href": "examples/applications_7_cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Visual Comparison: Mean vs. Median",
    "text": "Visual Comparison: Mean vs. Median\nVisual comparison gets us farther.\n\nggplot(recovery_data, aes(x = recovery_days, y = hospital)) +\n  geom_boxplot(outlier.alpha = 0.3, fill = \"lightblue\") +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = mean),\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 0.8\n  ) +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = median),\n    color = \"darkblue\",\n    linetype = \"solid\",\n    linewidth = 0.8\n  ) +\n  labs(\n    title = \"Recovery Times by Hospital\",\n    subtitle = \"Solid line = median (resistant), Dashed line = mean (sensitive to outliers)\",\n    x = \"Recovery Days\",\n    y = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nBox plots are great ways to get quick looks at location and spread. From here, you can see that the median is a skew-resistant measure. It is not affected by extreme values. The mean is sensitive to outliers. Hospital C shows outliers pulling the mean upward.\nThis is a rather elementary point, obviously, but for me, Cleveland’s Visualizing Data has a certain genius about showing the complexity hiding under simple concepts, while reinforcing that elementary summary and visualization tools are often the most important tools in our toolbox."
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#spread-measuring-variability",
    "href": "examples/applications_7_cleveland_univariate_data.html#spread-measuring-variability",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread: Measuring Variability",
    "text": "Spread: Measuring Variability\nUnderstanding how data varies around the center is as important and sometimes more important than knowing where the center is.\n\nspread_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    sd = sd(recovery_days),\n    iqr = IQR(recovery_days),\n    range = max(recovery_days) - min(recovery_days),\n    cv = sd / mean(recovery_days) * 100\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nspread_summary |&gt;\n  knitr::kable(\n    caption = \"Measures of Spread by Hospital\",\n    col.names = c(\"Hospital\", \"Std Dev\", \"IQR\", \"Range\", \"CV (%)\")\n  )\n\n\nMeasures of Spread by Hospital\n\n\nHospital\nStd Dev\nIQR\nRange\nCV (%)\n\n\n\n\nA\n3.1\n4.0\n17.7\n22.1\n\n\nB\n3.0\n3.8\n18.9\n16.6\n\n\nC\n6.1\n4.2\n32.1\n39.2\n\n\nD\n5.6\n10.1\n19.7\n37.2\n\n\n\n\n\nCommon metrics for understand spread are:\n\nIQR (Interquartile Range): Resistant to outliers, focuses on middle 50% of data\nStandard Deviation: Sensitive to outliers (see Hospital C)\nCoefficient of Variation: Standardized measure of spread relative to mean\n\nBelow we’ll explore how you can use spread and location to understand the structure of the data, check assumptions you may need for modeling (e.g., constant variance), and even get insights into the data generating mechanism of your data."
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#spread-location-s-l-plots",
    "href": "examples/applications_7_cleveland_univariate_data.html#spread-location-s-l-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread-Location (S-L) Plots",
    "text": "Spread-Location (S-L) Plots\nS-L plots help identify whether variability is constant or changes with the level of the data. This is fundamental to many modeling assumptions (e.g., for pooled t-tests, homoskedasticity).\nCleveland recommended comparing S-L relationships across groups to identify which groups exhibit constant spread vs. monotone spread (increasing variance with location).\n\n# For each hospital, calculate the spread for each observation\n# X-axis: Group median (location)\n# Y-axis: sqrt(|observation - group median|) for each individual observation\nspread_location_data &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    group_median = median(recovery_days),\n    spread = sqrt(abs(recovery_days - group_median))\n  ) |&gt;\n  ungroup()\n\n# Create spread-location plot with all hospitals\nggplot(spread_location_data, aes(x = group_median, y = spread, color = hospital)) +\n  geom_jitter(width = 0.3, alpha = 0.4, size = 2) +\n  stat_summary(fun = median, geom = \"line\", color = \"red\", linewidth = 1.2,\n               aes(group = 1)) +\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 3) +\n  labs(\n    title = \"Spread-Location Plot: All Hospitals\",\n    subtitle = \"Each point = one observation; Red line = median spread across locations\",\n    x = \"Location: Group Median (Days)\",\n    y = \"Spread: √|Value - Group Median|\",\n    color = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nHow to Read S-L Plots:\nEach point represents one observation, plotted at its group’s median on the x-axis. The y-axis shows how far that observation deviates from its group median (on the square root scale).\n\nVertical bands of points: Each vertical band corresponds to one hospital, positioned at that hospital’s median recovery time\nWidth of vertical band: Shows how variable the data is within that group (wider = more variability)\nRed line: Connects the median spread values across the four locations, revealing the spread-location relationship\n\nInterpretation:\n\nHospitals A & B: Narrow vertical bands at similar median locations (14 and 18 days) indicate low, consistent variability\nHospital C: Wider vertical band shows higher variability due to outliers\nHospital D: Bimodal distribution creates a different spread pattern\n\nThe red line is key: - Flat red line = constant spread (homoskedastic) → assumptions of pooled t-tests, ANOVA, OLS are satisfied - Upward sloping red line = monotone spread (heteroskedastic) → variance increases with location, violating homoskedasticity assumptions"
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#mean-difference-plot",
    "href": "examples/applications_7_cleveland_univariate_data.html#mean-difference-plot",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Mean-Difference Plot",
    "text": "Mean-Difference Plot\nWhen comparing two hospitals or measurement methods, Cleveland recommended plotting the difference vs. the average. This reveals whether differences are constant or depend on the magnitude.\n\n# Create comparison data with proper matching\n# Hospital B: constant offset (same SD as A, just shifted)\n# Hospital C: monotone spread (variance increases with magnitude)\nset.seed(789)\n\n# For Hospital B: pair each observation with a random one from Hospital A\nhospital_b_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"B\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital B vs A\"\n  )\n\n# For Hospital C: pair observations - this has outliers so will show monotone spread\nhospital_c_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital C vs A\"\n  )\n\ncomparison_pairs &lt;- bind_rows(hospital_b_pairs, hospital_c_pairs)\n\nggplot(comparison_pairs, aes(x = mean_value, y = difference)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\", linewidth = 1) +\n  facet_wrap(~ comparison, ncol = 2) +\n  labs(\n    title = \"Mean-Difference Plots: Comparing Hospitals to Hospital A\",\n    subtitle = \"Red line shows average difference; horizontal = constant spread\",\n    x = \"Average of Two Measurements\",\n    y = \"Difference (Hospital - Hospital A)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nHospital B vs A: Relatively flat red line around +4 days means constant spread (simple location shift)\nHospital C vs A: Upward-sloping red line → monotone spread (variability increases at higher values due to outliers)\n\nThis type of plot is essential in method comparison studies and reveals patterns that simple correlation or regression would miss."
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#residuals-actual-vs.-fitted-values",
    "href": "examples/applications_7_cleveland_univariate_data.html#residuals-actual-vs.-fitted-values",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Residuals: Actual vs. Fitted Values",
    "text": "Residuals: Actual vs. Fitted Values\nA fundamental concept in exploratory data analysis is the residual, the difference between what we observe and an expected value based on some model or summary.\nResidual = Observed Value - Fitted Value\nThe “fitted value” can come from:\n\nMean: Using the group mean as the fitted value\nMedian: Using the group median as the fitted value\nModel: Using predictions from a regression model\n\nResiduals help us understand:\n\nPatterns in variation: Are errors random or systematic?\nOutliers: Which observations deviate most from expectations?\nModel fit: Does our model capture the structure in the data?\n\nExamining residuals often provides insight into the data structure and the data generating mechanism. They are especially helpful for understanding whether data from different groups can be pooled: homogenous residuals can be pooled (e.g., in a t-test).\n\nBox Plots of Residuals from Group Means\nLet’s calculate residuals using each hospital’s mean as the fitted value:\n\n# Calculate residuals from group means\nresidual_data &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    group_mean = mean(recovery_days),\n    residual = recovery_days - group_mean\n  ) |&gt;\n  ungroup()\n\n# Create box plots of residuals\nggplot(residual_data, aes(x = hospital, y = residual, fill = hospital)) +\n  geom_boxplot(alpha = 0.7, outlier.size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Residuals from Group Means\",\n    subtitle = \"Residual = Observed - Mean; Symmetric around zero = good fit\",\n    x = \"Hospital\",\n    y = \"Residual (Days)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nInterpretation:\n\nHospital A & B: Residuals are roughly symmetric around zero (red dashed line)\nHospital C: Positive outlier residuals → some patients take much longer than the mean\nHospital D: Wider spread of residuals reflects the bimodal distribution\n\nWhen residuals show patterns (asymmetry, increasing spread, etc.), this suggests the simple mean doesn’t fully capture the data structure."
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#understanding-quantiles",
    "href": "examples/applications_7_cleveland_univariate_data.html#understanding-quantiles",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Understanding Quantiles",
    "text": "Understanding Quantiles\nThe quantile is a key tool for studying spread and location together. A quantile answers the question: “What value do X% of my observations fall below?”\nThe pth percentile is the value where p percent of the data falls below it. For example:\n\n25th percentile (p25): 25% of observations are below this value\n50th percentile (p50): 50% of observations are below this value (this is the median)\n75th percentile (p75): 75% of observations are below this value\n\nAgain, this is very elementary, but in practice proves very powerful.\n\nHow to Calculate Quantiles\nStep 1: Sort your data from smallest to largest\nStep 2: Find the position\nFor the pth percentile with n observations:\n\\[\\text{Position} = p \\times (n + 1)\\]\nStep 3: Get the value\n\nIf the position is a whole number (like 25), use that observation\nIf the position is fractional (like 25.25), interpolate between the two nearest values\n\nThere are a number of ways to calculate quantiles from Data. R’s quantile() function has 9 different ways to do it. Click this link to look through them.\n\n\nQuantiles Are…\n\nDistribution-free\nRobust to outliers\nEasy to compare: Comparing quantiles across groups reveals differences in location, spread, and shape."
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#residual-fitted-r-f-spread-plot",
    "href": "examples/applications_7_cleveland_univariate_data.html#residual-fitted-r-f-spread-plot",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Residual-Fitted (R-F) Spread Plot",
    "text": "Residual-Fitted (R-F) Spread Plot\nThe R-F spread plot visually decomposes your data into two parts:\n\nWhat the grouping explains (left panel)\nWhat remains unexplained (right panel)\n\nThink of it as a visual ANOVA. If the left panel is wide, grouping matters. If the right panel is wide, there’s lots of leftover variation.\n\n# Calculate TWO different percentile ranks:\n# 1. For left panel: sort by fitted values first (creates non-overlapping bands by group)\n# 2. For right panel: sort by residuals (pools all residuals together)\nrf_data &lt;- residual_data |&gt;\n  mutate(\n    fitted = group_mean,\n    grand_mean = mean(recovery_days),\n    fit_minus_mean = fitted - grand_mean\n  ) |&gt;\n  arrange(fitted, recovery_days) |&gt;\n  mutate(\n    f_value_fitted = (row_number() - 1) / (n() - 1)\n  ) |&gt;\n  arrange(residual) |&gt;\n  mutate(\n    f_value_residual = (row_number() - 1) / (n() - 1)\n  )\n\n# Create separate datasets for each panel\nfitted_panel &lt;- rf_data |&gt;\n  select(hospital, f_value = f_value_fitted, fit_minus_mean) |&gt;\n  ungroup()\n\nresidual_panel &lt;- rf_data |&gt;\n  select(f_value = f_value_residual, residual) |&gt;\n  ungroup()\n\n# Create r-f spread plot with improved aesthetics\nlibrary(patchwork)\n\n# Custom color palette for hospitals\nhospital_colors &lt;- c(\"#E64B35\", \"#4DBBD5\", \"#00A087\", \"#3C5488\")\n\np_fitted &lt;- ggplot(fitted_panel, aes(x = f_value, y = fit_minus_mean, color = hospital)) +\n  geom_point(alpha = 0.5, size = 2.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  scale_color_manual(values = hospital_colors) +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Between-Group Variation\",\n    subtitle = \"How much do the groups differ?\",\n    x = \"Percentile Rank (sorted by group)\",\n    y = \"Distance from Overall Mean (days)\",\n    color = \"Hospital\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray40\", size = 11),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, linewidth = 0.5)\n  )\n\np_residual &lt;- ggplot(residual_panel, aes(x = f_value, y = residual)) +\n  geom_point(alpha = 0.4, size = 2.5, color = \"#7570B3\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Within-Group Variation\",\n    subtitle = \"What's left unexplained?\",\n    x = \"Percentile Rank (pooled residuals)\",\n    y = \"Residual: Observed - Group Mean (days)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray40\", size = 11),\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, linewidth = 0.5)\n  )\n\n# Combine panels\np_fitted + p_residual +\n  plot_annotation(\n    title = \"Residual-Fitted (R-F) Spread Plot: Visual ANOVA\",\n    theme = theme(plot.title = element_text(face = \"bold\", size = 16))\n  )\n\n\n\n\n\n\n\n\n\nHow to Read This Plot\nLeft Panel - Between-Group Variation:\n\nEach colored band represents one hospital\nBand width = proportion of patients in that hospital\nBand height = how far that hospital’s average is from the overall average\nWide vertical spread → hospitals differ a lot → grouping explains variation\nNarrow vertical spread → hospitals are similar → grouping doesn’t help much\n\nRight Panel - Within-Group Variation:\n\nAll residuals pooled together (no colors)\nShows what’s left after accounting for hospital differences\nWide vertical spread → lots of unexplained variation remains\nNarrow vertical spread → hospital membership explains most of the variation\nPoints should scatter randomly around zero (horizontal line)\n\nThe Key Insight:\nCompare the vertical spread of the two panels. If the left panel is much wider than the right, the grouping (hospital) explains most of the variation. If the right panel is wider, there’s lots of individual variation that hospital membership doesn’t capture."
  },
  {
    "objectID": "examples/applications_7_cleveland_univariate_data.html#transformations",
    "href": "examples/applications_7_cleveland_univariate_data.html#transformations",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Transformations",
    "text": "Transformations\nTransformations can fix skewed data and stabilize variance. Cleveland emphasized using visualization (especially Q-Q plots) to diagnose problems and assess whether transformations help.\n\nExample 1: Log Transformation for Earnings\nWhy log transform earnings? Income and earnings follow multiplicative processes. A 10% raise means different dollar amounts for low vs high earners, but the same proportional change. Earnings data is typically right-skewed with a long tail.\n\n# Simulate earnings data with right skew (multiplicative process)\nset.seed(456)\nn &lt;- 500\n\n# Generate log-normal earnings (typical for income data)\nearnings_data &lt;- data.frame(\n  earnings = exp(rnorm(n, mean = log(50000), sd = 0.6)),  # Median ~$50k\n  health_score = rnorm(n, mean = 75, sd = 10)\n) |&gt;\n  mutate(\n    log_earnings = log(earnings)\n  )\n\n# Create side-by-side Q-Q plots\np1 &lt;- ggplot(earnings_data, aes(sample = earnings)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Earnings\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(earnings_data, aes(sample = log_earnings)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Log(Earnings)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np1 + p2 +\n  plot_annotation(\n    title = \"Log Transformation Fixes Right Skew in Earnings\",\n    subtitle = \"Left: Original data curves up (right skew) | Right: Log-transformed aligns with normal line\"\n  )\n\n\n\n\n\n\n\n\nInterpretation:\n\nLeft panel: Original earnings curve upward at upper quantiles → right skew\nRight panel: Log(earnings) points lie on the line → approximately normal\nWhy it works: Log transforms multiplicative relationships into additive ones\nOn log scale, a 10% raise is the same distance for everyone\n\n\n\nExample 2: Square Root Transformation for Count Data\nWhy square root for counts? Count data (e.g., number of events, accidents, infections) often has variance that increases with the mean. Square root transformation stabilizes this.\n\n# Simulate count data (Poisson-distributed)\nset.seed(789)\n\n# Generate data from multiple groups with different means\ncount_data &lt;- data.frame(\n  group = rep(c(\"Low\", \"Medium\", \"High\"), each = 200),\n  count = c(\n    rpois(200, lambda = 5),   # Low rate\n    rpois(200, lambda = 15),  # Medium rate\n    rpois(200, lambda = 30)   # High rate\n  )\n) |&gt;\n  mutate(\n    sqrt_count = sqrt(count + 0.5)  # Add 0.5 to handle zeros\n  )\n\n# Create side-by-side Q-Q plots\np1 &lt;- ggplot(count_data, aes(sample = count)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Counts\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(count_data, aes(sample = sqrt_count)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = expression(sqrt(Count + 0.5)),\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np1 + p2 +\n  plot_annotation(\n    title = \"Square Root Transformation Stabilizes Variance in Count Data\",\n    subtitle = expression(paste(\"Left: Original shows increasing variance. Right: \", sqrt(Count), \" transformation makes variance more constant\"))\n  )\n\n\n\n\n\n\n\n\nInterpretation:\n\nLeft panel: Original counts show a curve (variance increases with mean)\nRight panel: \\(\\sqrt{\\text{count}}\\) is closer to a straight line\nWhy it works: For Poisson data, variance = mean, so \\(\\sqrt{\\cdot}\\) helps stabilizes this relationship"
  },
  {
    "objectID": "presentation.html#recreating-minard-with-matplotlib",
    "href": "presentation.html#recreating-minard-with-matplotlib",
    "title": "Data Visualization",
    "section": "Recreating Minard with Matplotlib",
    "text": "Recreating Minard with Matplotlib\nSomeone has done a pretty impressive job recreating this visualization using Python’s matplotlib library:\n\nMinard’s Chart RecreationView the full tutorial"
  },
  {
    "objectID": "presentation.html#recreating-minard-with-matplotlib-cont.",
    "href": "presentation.html#recreating-minard-with-matplotlib-cont.",
    "title": "Data Visualization",
    "section": "Recreating Minard with Matplotlib (cont.)",
    "text": "Recreating Minard with Matplotlib (cont.)\n\nMinard’s Chart Recreated"
  }
]