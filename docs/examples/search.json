[
  {
    "objectID": "cleveland_univariate_data.html",
    "href": "cleveland_univariate_data.html",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "cleveland_univariate_data.html#introduction",
    "href": "cleveland_univariate_data.html#introduction",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "cleveland_univariate_data.html#simulated-example-data",
    "href": "cleveland_univariate_data.html#simulated-example-data",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Simulated Example Data",
    "text": "Simulated Example Data\nWe’ll use a simulated healthcare dataset to explore Cleveland’s concepts. Imagine we’re examining patient recovery times (in days) from a medical procedure across different hospitals.\n\nset.seed(1234)\n\n# Hospital A: Normal distribution, mean recovery time\nhospital_a &lt;- rnorm(200, mean = 14, sd = 3)\n\n# Hospital B: Normal distribution, longer recovery\nhospital_b &lt;- rnorm(200, mean = 18, sd = 3)\n\n# Hospital C: Skewed distribution with some very long recoveries\nhospital_c &lt;- c(\n  rnorm(180, mean = 14, sd = 2.5),\n  runif(20, min = 25, max = 40)  # Some outliers\n)\n\n# Hospital D: Bimodal distribution (two distinct patient groups)\nhospital_d &lt;- c(\n  rnorm(100, mean = 10, sd = 2),\n  rnorm(100, mean = 20, sd = 2)\n)\n\n# Combine into data frame\nrecovery_data &lt;- data.frame(\n  hospital = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 200),\n  recovery_days = c(hospital_a, hospital_b, hospital_c, hospital_d)\n)"
  },
  {
    "objectID": "cleveland_univariate_data.html#location-measuring-central-tendency",
    "href": "cleveland_univariate_data.html#location-measuring-central-tendency",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Location: Measuring Central Tendency",
    "text": "Location: Measuring Central Tendency\nCleveland emphasized understanding the “location” of data, but he just means meaasures of central tendency. Different measures of location tell different stories.\n\n# Calculate measures of location for each hospital\nlocation_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    mean = mean(recovery_days),\n    median = median(recovery_days),\n    q25 = quantile(recovery_days, 0.25),\n    q75 = quantile(recovery_days, 0.75),\n    n = n()\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nlocation_summary |&gt;\n  knitr::kable(caption = \"Measures of Location by Hospital\")\n\n\nMeasures of Location by Hospital\n\n\nhospital\nmean\nmedian\nq25\nq75\nn\n\n\n\n\nA\n13.8\n13.5\n11.7\n15.7\n200\n\n\nB\n18.2\n18.4\n16.3\n20.1\n200\n\n\nC\n15.6\n14.3\n12.0\n16.3\n200\n\n\nD\n14.9\n13.8\n9.8\n19.9\n200\n\n\n\n\n\n\nKey Insights:\n\nHospital A & B: Mean ≈ Median (symmetric distributions)\nHospital C: Mean &gt; Median (right-skewed due to outliers)\nHospital D: Bimodal pattern obscures single “center”"
  },
  {
    "objectID": "cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "href": "cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Visual Comparison: Mean vs. Median",
    "text": "Visual Comparison: Mean vs. Median\nVisual comparison gets us farther.\n\nggplot(recovery_data, aes(x = recovery_days, y = hospital)) +\n  geom_boxplot(outlier.alpha = 0.3, fill = \"lightblue\") +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = mean),\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 0.8\n  ) +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = median),\n    color = \"darkblue\",\n    linetype = \"solid\",\n    linewidth = 0.8\n  ) +\n  labs(\n    title = \"Recovery Times by Hospital\",\n    subtitle = \"Solid line = median (resistant), Dashed line = mean (sensitive to outliers)\",\n    x = \"Recovery Days\",\n    y = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nBox plots are great ways to get quick looks at location and spread. From here, you can see that the median is a skew-resistant measure. It is not affected by extreme values. The mean is sensitive to outliers. Hospital C shows outliers pulling the mean upward.\nThis is a rather elementary point, obviously, but for me, Cleveland’s Visualizing Data has a certain genius about showing the complexity hiding under simple concepts, while reinforcing that elementary summary and visualization tools are often the most important tools in our toolbox."
  },
  {
    "objectID": "cleveland_univariate_data.html#spread-measuring-variability",
    "href": "cleveland_univariate_data.html#spread-measuring-variability",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread: Measuring Variability",
    "text": "Spread: Measuring Variability\nUnderstanding how data varies around the center is as important and sometimes more important than knowing where the center is.\n\nspread_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    sd = sd(recovery_days),\n    iqr = IQR(recovery_days),\n    range = max(recovery_days) - min(recovery_days),\n    cv = sd / mean(recovery_days) * 100\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nspread_summary |&gt;\n  knitr::kable(\n    caption = \"Measures of Spread by Hospital\",\n    col.names = c(\"Hospital\", \"Std Dev\", \"IQR\", \"Range\", \"CV (%)\")\n  )\n\n\nMeasures of Spread by Hospital\n\n\nHospital\nStd Dev\nIQR\nRange\nCV (%)\n\n\n\n\nA\n3.1\n4.0\n17.7\n22.1\n\n\nB\n3.0\n3.8\n18.9\n16.6\n\n\nC\n6.1\n4.2\n32.1\n39.2\n\n\nD\n5.6\n10.1\n19.7\n37.2\n\n\n\n\n\nCommon metrics for understand spread are:\n\nIQR (Interquartile Range): Resistant to outliers, focuses on middle 50% of data\nStandard Deviation: Sensitive to outliers (see Hospital C)\nCoefficient of Variation: Standardized measure of spread relative to mean\n\nBelow we’ll explore how you can use spread and location to understand the structure of the data, check assumptions you may need for modeling (e.g., constant variance), and even get insights into the data generating mechanism of your data."
  },
  {
    "objectID": "cleveland_univariate_data.html#understanding-quantiles",
    "href": "cleveland_univariate_data.html#understanding-quantiles",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Understanding Quantiles",
    "text": "Understanding Quantiles\nThe quantile is a key tool for studying spread and location together. A quantile answers the question: “What value do X% of my observations fall below?”\nThe pth percentile is the value where p percent of the data falls below it. For example:\n\n25th percentile (p25): 25% of observations are below this value\n50th percentile (p50): 50% of observations are below this value (this is the median)\n75th percentile (p75): 75% of observations are below this value\n\nAgain, this is very elementary, but in practice proves very powerful.\n\nHow to Calculate Quantiles\nStep 1: Sort your data from smallest to largest\nStep 2: Find the position\nFor the pth percentile with n observations:\n\\[\\text{Position} = p \\times (n + 1)\\]\nStep 3: Get the value\n\nIf the position is a whole number (like 25), use that observation\nIf the position is fractional (like 25.25), interpolate between the two nearest values\n\nThere are a number of ways to calculate quantiles from Data. R’s quantile() function has 9 different ways to do it. Click this link to look through them.\n\n\nQuantiles Are…\n\nDistribution-free\nRobust to outliers\nEasy to compare: Comparing quantiles across groups reveals differences in location, spread, and shape."
  },
  {
    "objectID": "cleveland_univariate_data.html#spread-location-plots",
    "href": "cleveland_univariate_data.html#spread-location-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread-Location Plots",
    "text": "Spread-Location Plots\nCleveland emphasized visualizing the relationship between location (central tendency) and spread (dispersion). This helps identify whether variability is constant or changes with the level of the data. This is fundamental to many modeling assumptions (e.g., for pooled t-tests, homoskedasticity).\n\n# Create meaningful groups by binning recovery times\n# This gives us many more points to assess the relationship\nspread_location_data &lt;- recovery_data |&gt;\n  mutate(\n    recovery_bin = cut(recovery_days,\n                      breaks = seq(0, 50, by = 2.5),\n                      include.lowest = TRUE)\n  ) |&gt;\n  group_by(recovery_bin) |&gt;\n  filter(n() &gt;= 5) |&gt;  # Only keep bins with at least 5 observations\n  summarize(\n    median = median(recovery_days),\n    iqr = IQR(recovery_days),\n    mad = mad(recovery_days),\n    n = n()\n  ) |&gt;\n  filter(!is.na(median))\n\n# Create spread-location plot\nggplot(spread_location_data, aes(x = median, y = iqr)) +\n  geom_point(size = 3, color = \"steelblue\", alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Spread-Location Relationship\",\n    subtitle = \"Does variability increase with central tendency?\",\n    x = \"Location (Median Recovery Days)\",\n    y = \"Spread (IQR)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation: The plot shows the relationship between location and spread across recovery time bins. If the red line slopes upward (spread increases with location), this indicates *monotone spread or nonconstant variance. This violates assumptions of many statistical tests (e.g., pooled t-tests, OLS) and suggests a transformation (like log or square root) might stabilize the variance."
  },
  {
    "objectID": "cleveland_univariate_data.html#mean-difference-plot",
    "href": "cleveland_univariate_data.html#mean-difference-plot",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Mean-Difference Plot",
    "text": "Mean-Difference Plot\nWhen comparing two hospitals or measurement methods, Cleveland recommended plotting the difference vs. the average. This reveals whether differences are constant or depend on the magnitude.\n\n# Create comparison data with proper matching\n# Hospital B: constant offset (same SD as A, just shifted)\n# Hospital C: monotone spread (variance increases with magnitude)\nset.seed(789)\n\n# For Hospital B: pair each observation with a random one from Hospital A\nhospital_b_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"B\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital B vs A\"\n  )\n\n# For Hospital C: pair observations - this has outliers so will show monotone spread\nhospital_c_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital C vs A\"\n  )\n\ncomparison_pairs &lt;- bind_rows(hospital_b_pairs, hospital_c_pairs)\n\nggplot(comparison_pairs, aes(x = mean_value, y = difference)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\", linewidth = 1) +\n  facet_wrap(~ comparison, ncol = 2) +\n  labs(\n    title = \"Mean-Difference Plots: Comparing Hospitals to Hospital A\",\n    subtitle = \"Red line shows average difference; horizontal = constant spread\",\n    x = \"Average of Two Measurements\",\n    y = \"Difference (Hospital - Hospital A)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nHospital B vs A: Relatively flat red line around +4 days means constant spread (simple location shift)\nHospital C vs A: Upward-sloping red line → monotone spread (variability increases at higher values due to outliers)\n\nThis type of plot is essential in method comparison studies and reveals patterns that simple correlation or regression would miss."
  },
  {
    "objectID": "cleveland_univariate_data.html#power-transformations-for-stabilizing-variance",
    "href": "cleveland_univariate_data.html#power-transformations-for-stabilizing-variance",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Power Transformations for Stabilizing Variance",
    "text": "Power Transformations for Stabilizing Variance\nWhen spread-location plots reveal monotone spread (variance changes with level), Cleveland advocated for power transformations to stabilize variance and make distributions more symmetric.\n\nThe Power Transformation Family\nThe power transformation family takes the form:\n\\[y^{\\lambda} \\text{ where } \\lambda \\text{ is the power}\\]\nCommon transformations:\n\n\n\nλ\nTransformation\nName\nWhen to Use\n\n\n\n\n2\n\\(y^2\\)\nSquare\nCompress right tail\n\n\n1\n\\(y\\)\nIdentity\nNo transformation needed\n\n\n0.5\n\\(\\sqrt{y}\\)\nSquare root\nModerate right skew, count data\n\n\n0\n\\(\\log(y)\\)\nLog*\nStrong right skew, multiplicative processes\n\n\n-0.5\n\\(-1/\\sqrt{y}\\)\nInverse square root\nVery strong right skew\n\n\n-1\n\\(-1/y\\)\nInverse\nExtreme right skew\n\n\n\n\n\nWhen to Transform\n\nMonotone spread: Variance increases with the mean\nStrong skewness: Long tails distort analyses\nMultiplicative relationships: Percent changes, growth rates\nMeeting model assumptions: Many tests assume constant variance and normality\n\n\n\nExample: Log Transformation for Hospital C\nHospital C has outliers creating right skew and monotone spread. Let’s see if log transformation helps.\n\n# Create comparison of original vs log-transformed data for Hospital C\nhospital_c_data &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  mutate(\n    log_recovery = log(recovery_days),\n    sqrt_recovery = sqrt(recovery_days)\n  )\n\n# Create histograms\np1 &lt;- ggplot(hospital_c_data, aes(x = recovery_days)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Original Scale\",\n    x = \"Recovery Days\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np2 &lt;- ggplot(hospital_c_data, aes(x = log_recovery)) +\n  geom_histogram(bins = 30, fill = \"coral\", color = \"white\") +\n  labs(\n    title = \"Log-Transformed\",\n    x = \"Log(Recovery Days)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np3 &lt;- ggplot(hospital_c_data, aes(x = sqrt_recovery)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", color = \"white\") +\n  labs(\n    title = \"Square Root Transformed\",\n    x = \"√(Recovery Days)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n# Combine plots\n(p1 / p2 / p3) +\n  plot_annotation(\n    title = \"Effect of Power Transformations on Hospital C Data\",\n    subtitle = \"Log and √ transformations reduce right skew and stabilize variance\"\n  )\n\n\n\n\n\n\n\n\n\n\nAssessing the Transformation\n\n# Compare summary statistics before and after transformation\ntransformation_summary &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"SD\", \"IQR\", \"Skewness\"),\n  Original = c(\n    mean(hospital_c_data$recovery_days),\n    median(hospital_c_data$recovery_days),\n    sd(hospital_c_data$recovery_days),\n    IQR(hospital_c_data$recovery_days),\n    moments::skewness(hospital_c_data$recovery_days)\n  ),\n  Log = c(\n    mean(hospital_c_data$log_recovery),\n    median(hospital_c_data$log_recovery),\n    sd(hospital_c_data$log_recovery),\n    IQR(hospital_c_data$log_recovery),\n    moments::skewness(hospital_c_data$log_recovery)\n  ),\n  Sqrt = c(\n    mean(hospital_c_data$sqrt_recovery),\n    median(hospital_c_data$sqrt_recovery),\n    sd(hospital_c_data$sqrt_recovery),\n    IQR(hospital_c_data$sqrt_recovery),\n    moments::skewness(hospital_c_data$sqrt_recovery)\n  )\n) |&gt;\n  mutate(across(where(is.numeric), ~round(., 2)))\n\ntransformation_summary |&gt;\n  knitr::kable(caption = \"Comparing Transformations for Hospital C\")\n\n\nComparing Transformations for Hospital C\n\n\nMeasure\nOriginal\nLog\nSqrt\n\n\n\n\nMean\n15.59\n2.69\n3.89\n\n\nMedian\n14.29\n2.66\n3.78\n\n\nSD\n6.11\n0.32\n0.68\n\n\nIQR\n4.23\n0.30\n0.56\n\n\nSkewness\n2.12\n1.07\n1.64\n\n\n\n\n\nLog scale: Differences become ratios (additive turns to multiplicative); logs are typically preferred over other transformations because of this interpretive enefit\n\n\nChoosing the Right Transformation\nCleveland suggested using data-driven approaches:\n\nVisual inspection: Try several transformations, plot results\nSymmetry: Choose transformation that makes distribution most symmetric\nConstant variance: Choose transformation that stabilizes spread-location relationship\n\nThe goal is not mathematical purity but getting an intimate undrstanding of your data."
  },
  {
    "objectID": "cleveland_univariate_data.html#comparing-distributions-q-q-plots",
    "href": "cleveland_univariate_data.html#comparing-distributions-q-q-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Comparing Distributions: Q-Q Plots",
    "text": "Comparing Distributions: Q-Q Plots\nCleveland emphasized quantile-quantile (Q-Q) plots for comparing distributions. They plot quantiles of one distribution against another.\n\n# Compare each hospital to Hospital A\ncomparison_data &lt;- recovery_data |&gt;\n  filter(hospital != \"A\") |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    quantile_rank = rank(recovery_days) / (n() + 1)\n  ) |&gt;\n  ungroup()\n\n# Get corresponding quantiles from Hospital A\nhospital_a_quantiles &lt;- function(p) {\n  quantile(recovery_data$recovery_days[recovery_data$hospital == \"A\"], p)\n}\n\ncomparison_data &lt;- comparison_data |&gt;\n  mutate(\n    hospital_a_value = sapply(quantile_rank, hospital_a_quantiles)\n  )\n\nggplot(comparison_data, aes(x = hospital_a_value, y = recovery_days)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ hospital, ncol = 3) +\n  labs(\n    title = \"Q-Q Plots: Comparing Hospitals to Hospital A\",\n    subtitle = \"Points on red line = identical distributions\",\n    x = \"Hospital A Quantiles\",\n    y = \"Other Hospital Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nInterpreting Q-Q Plots:\n\nHospital B: Points above line → consistently longer recovery times\nHospital C: Curves up at high quantiles → longer tail (outliers)\nHospital D: S-shaped curve → different distribution shape (bimodal vs. unimodal)"
  },
  {
    "objectID": "cleveland_univariate_data.html#q-q-plots-for-assessing-normality",
    "href": "cleveland_univariate_data.html#q-q-plots-for-assessing-normality",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Q-Q Plots for Assessing Normality",
    "text": "Q-Q Plots for Assessing Normality\nA particularly important use of Q-Q plots is assessing whether data follows a normal distribution. This is done by plotting sample quantiles against theoretical normal quantiles.\n\nWhy This Matters\nMany statistical methods assume normality: - t-tests - ANOVA - Linear regression (assumes normal residuals) - Confidence intervals based on standard errors\nCleveland emphasized visual assessment over formal tests because: 1. Visual patterns reveal the type of departure (skew, outliers, heavy tails) 2. Sample size affects formal tests (large n makes trivial departures “significant”) 3. We can see if departures matter for our specific purpose\n\n\nHow It Works\nNormal Q-Q Plot: - X-axis: Theoretical quantiles from standard normal distribution - Y-axis: Observed quantiles from your data - If data is normal: Points fall on a straight line - If not normal: Systematic deviations reveal the problem\n\n# Create Q-Q plots for each hospital\nlibrary(patchwork)\n\n# Hospital A - approximately normal\np_a &lt;- ggplot(recovery_data |&gt; filter(hospital == \"A\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital A: Approximately Normal\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital B - normal but shifted\np_b &lt;- ggplot(recovery_data |&gt; filter(hospital == \"B\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital B: Normal (Different Mean)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital C - right-skewed with outliers\np_c &lt;- ggplot(recovery_data |&gt; filter(hospital == \"C\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital C: Right-Skewed + Outliers\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital D - bimodal\np_d &lt;- ggplot(recovery_data |&gt; filter(hospital == \"D\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital D: Bimodal (Non-Normal)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine\n(p_a + p_b) / (p_c + p_d) +\n  plot_annotation(\n    title = \"Normal Q-Q Plots: Diagnosing Departures from Normality\"\n  )\n\n\n\n\n\n\n\n\n\n\nReading the Patterns\n1. Points on the line (Hospitals A & B): - Data follows normal distribution - Parametric methods are safe to use\n2. Curve at upper end (Hospital C): - Right tail heavier than normal → right skew - Upper quantiles deviate upward from line - Common with positive data (income, recovery times) - Solution: Log or square root transformation\n3. Curve at both ends (Hospital C outliers): - Points at extremes deviate from line - Indicates outliers beyond what normal distribution would produce - Action: Investigate outliers, consider robust methods\n4. S-shaped curve (Hospital D): - Points curve below line at low end, above at high end - Distribution has shorter tails than normal (or is bimodal) - Indicates: Fundamentally different distribution shape - Action: Don’t assume normality, use distribution-free methods\n5. Points below line at low end, above at high end: - Distribution has heavier tails than normal - More extreme values than normal distribution predicts - Common in financial data, measurement errors\n\n\nBenefits of Q-Q Plots Over Formal Tests\nShapiro-Wilk test and similar tests have limitations:\n\n\n\n\n\n\n\nFormal Tests\nQ-Q Plots\n\n\n\n\nBinary answer (reject/fail to reject)\nShows what and how much departure\n\n\nVery sensitive with large n\nAssess practical significance\n\n\nDon’t reveal type of departure\nSee if it’s skew, outliers, heavy tails, etc.\n\n\nNo guidance on fix\nSuggests transformation strategy\n\n\nArbitrary α threshold\nProfessional judgment based on context\n\n\n\nExample: With n = 10,000, Shapiro-Wilk might reject normality for trivial departures that don’t affect t-test validity. Q-Q plot shows if departure matters for your analysis.\n\n\nPractical Workflow\nCleveland’s approach to checking normality:\n\nStart with histogram: Get overall shape sense\nCreate Q-Q plot: Diagnose specific departures\nAssess practical impact:\n\nSlight skew with large n? Probably fine.\nStrong bimodality? Serious problem.\nOutliers? Investigate before proceeding.\n\nConsider alternatives:\n\nTransform to achieve normality\nUse robust methods (median, bootstrap)\nUse distribution-free tests (Mann-Whitney, Kruskal-Wallis)\n\n\n\n\nWhen Normality Doesn’t Matter\nWith large samples, Central Limit Theorem means: - Means are approximately normal even if data isn’t - t-tests, ANOVA are robust to moderate departures - More concerned about outliers than mild skewness\nFocus Q-Q plot assessment on: - Small samples (n &lt; 30): Normality matters more - Extreme violations: Bimodality, severe skew, major outliers - Residual diagnostics: In regression, check if residuals are normal\n\n\nExample: Hospital C After Log Transformation\n\n# Compare original vs log-transformed Hospital C\nhospital_c_data &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  mutate(log_recovery = log(recovery_days))\n\np_orig &lt;- ggplot(hospital_c_data, aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Scale\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles (Days)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np_log &lt;- ggplot(hospital_c_data, aes(sample = log_recovery)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Log-Transformed\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles (Log Days)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np_orig + p_log +\n  plot_annotation(\n    title = \"Log Transformation Improves Normality\",\n    subtitle = \"Points align better with line after transformation\"\n  )\n\n\n\n\n\n\n\n\nKey observation: Log transformation brings the upper tail back toward the line, making distribution more symmetric and closer to normal.\n\n\nCleveland’s Message\n\n“The goal is not to prove normality, but to understand your data’s shape and choose appropriate methods.”\n\nQ-Q plots serve exploration and diagnosis, not hypothesis testing. Use them to: - Understand what you’re working with - Decide if assumptions are reasonable - Choose between parametric and robust methods - Transform when it aids interpretation"
  },
  {
    "objectID": "cleveland_univariate_data.html#references",
    "href": "cleveland_univariate_data.html#references",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "References",
    "text": "References\nCleveland, W. S. (1993). Visualizing Data. Hobart Press."
  },
  {
    "objectID": "cleveland_univariate_data.html#location-spread",
    "href": "cleveland_univariate_data.html#location-spread",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Location & Spread",
    "text": "Location & Spread\nCleveland emphasized understanding the “location” and “spread” of data, which in more common parlance just means measures of central tendency and variance/dispersoin. Different measures of location and spread tell different stories.\n\n# Calculate measures of location for each hospital\nlocation_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    mean = mean(recovery_days),\n    median = median(recovery_days),\n    q25 = quantile(recovery_days, 0.25),\n    q75 = quantile(recovery_days, 0.75),\n    n = n()\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nlocation_summary |&gt;\n  knitr::kable(caption = \"Measures of Location by Hospital\")\n\n\nMeasures of Location by Hospital\n\n\nhospital\nmean\nmedian\nq25\nq75\nn\n\n\n\n\nA\n13.8\n13.5\n11.7\n15.7\n200\n\n\nB\n18.2\n18.4\n16.3\n20.1\n200\n\n\nC\n15.6\n14.3\n12.0\n16.3\n200\n\n\nD\n14.9\n13.8\n9.8\n19.9\n200\n\n\n\n\n\nSummary statistics help:\n\nHospital A & B: Mean ≈ Median (symmetric distributions)\nHospital C: Mean &gt; Median (right-skewed due to outliers)\nHospital D: Bimodal pattern obscures single “center”"
  },
  {
    "objectID": "cleveland_univariate_data.html#spread-location-s-l-plots",
    "href": "cleveland_univariate_data.html#spread-location-s-l-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread-Location (S-L) Plots",
    "text": "Spread-Location (S-L) Plots\nS-L plots help identify whether variability is constant or changes with the level of the data. This is fundamental to many modeling assumptions (e.g., for pooled t-tests, homoskedasticity).\nCleveland recommended comparing S-L relationships across groups to identify which groups exhibit constant spread vs. monotone spread (increasing variance with location).\n\n# For each hospital, calculate the spread for each observation\n# X-axis: Group median (location)\n# Y-axis: sqrt(|observation - group median|) for each individual observation\nspread_location_data &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    group_median = median(recovery_days),\n    spread = sqrt(abs(recovery_days - group_median))\n  ) |&gt;\n  ungroup()\n\n# Create spread-location plot with all hospitals\nggplot(spread_location_data, aes(x = group_median, y = spread, color = hospital)) +\n  geom_jitter(width = 0.3, alpha = 0.4, size = 2) +\n  stat_summary(fun = median, geom = \"line\", color = \"red\", linewidth = 1.2,\n               aes(group = 1)) +\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 3) +\n  labs(\n    title = \"Spread-Location Plot: All Hospitals\",\n    subtitle = \"Each point = one observation; Red line = median spread across locations\",\n    x = \"Location: Group Median (Days)\",\n    y = \"Spread: √|Value - Group Median|\",\n    color = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nHow to Read S-L Plots:\nEach point represents one observation, plotted at its group’s median on the x-axis. The y-axis shows how far that observation deviates from its group median (on the square root scale).\n\nVertical bands of points: Each vertical band corresponds to one hospital, positioned at that hospital’s median recovery time\nWidth of vertical band: Shows how variable the data is within that group (wider = more variability)\nRed line: Connects the median spread values across the four locations, revealing the spread-location relationship\n\nInterpretation:\n\nHospitals A & B: Narrow vertical bands at similar median locations (14 and 18 days) indicate low, consistent variability\nHospital C: Wider vertical band shows higher variability due to outliers\nHospital D: Bimodal distribution creates a different spread pattern\n\nThe red line is key: - Flat red line = constant spread (homoskedastic) → assumptions of pooled t-tests, ANOVA, OLS are satisfied - Upward sloping red line = monotone spread (heteroskedastic) → variance increases with location, violating homoskedasticity assumptions"
  },
  {
    "objectID": "cleveland_univariate_data.html#residuals-actual-vs.-fitted-values",
    "href": "cleveland_univariate_data.html#residuals-actual-vs.-fitted-values",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Residuals: Actual vs. Fitted Values",
    "text": "Residuals: Actual vs. Fitted Values\nA fundamental concept in exploratory data analysis is the residual - the difference between what we observe and what we expect based on some model or summary.\nResidual = Observed Value - Fitted Value\nThe “fitted value” can come from:\n\nMean: Using the group mean as the fitted value\nMedian: Using the group median as the fitted value\nModel: Using predictions from a regression model\n\nResiduals help us understand:\n\nPatterns in variation: Are errors random or systematic?\nOutliers: Which observations deviate most from expectations?\nModel fit: Does our model capture the structure in the data?\n\nCleveland emphasized that examining residuals is often more informative than looking at raw data alone.\n\nBox Plots of Residuals from Group Means\nLet’s calculate residuals using each hospital’s mean as the fitted value:\n\n# Calculate residuals from group means\nresidual_data &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    group_mean = mean(recovery_days),\n    residual = recovery_days - group_mean\n  ) |&gt;\n  ungroup()\n\n# Create box plots of residuals\nggplot(residual_data, aes(x = hospital, y = residual, fill = hospital)) +\n  geom_boxplot(alpha = 0.7, outlier.size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Residuals from Group Means\",\n    subtitle = \"Residual = Observed - Mean; Symmetric around zero = good fit\",\n    x = \"Hospital\",\n    y = \"Residual (Days)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nInterpretation:\n\nHospital A & B: Residuals are roughly symmetric around zero (red dashed line)\nHospital C: Positive outlier residuals → some patients take much longer than the mean\nHospital D: Wider spread of residuals reflects the bimodal distribution\n\nWhen residuals show patterns (asymmetry, increasing spread, etc.), this suggests the simple mean doesn’t fully capture the data structure."
  },
  {
    "objectID": "cleveland_univariate_data.html#residual-fitted-r-f-spread-plot",
    "href": "cleveland_univariate_data.html#residual-fitted-r-f-spread-plot",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Residual-Fitted (R-F) Spread Plot",
    "text": "Residual-Fitted (R-F) Spread Plot\nThe r-f spread plot is a powerful diagnostic tool that shows the relationship between fitted values and residuals across quantiles of the data. This helps assess whether the model (in this case, using group means) adequately captures the data structure.\nCleveland recommended plotting: - Left panel: Fitted values vs. quantiles - Right panel: Residuals vs. quantiles\nThis side-by-side view reveals: - Whether fitted values match the data structure - Whether residuals show patterns or are randomly scattered - How spread changes across the distribution\n\n# Calculate quantiles for each hospital\nrf_data &lt;- residual_data |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    quantile_rank = percent_rank(recovery_days),\n    fitted = group_mean\n  ) |&gt;\n  ungroup()\n\n# Create data for both panels\nrf_long &lt;- rf_data |&gt;\n  select(hospital, quantile_rank, fitted, residual) |&gt;\n  pivot_longer(cols = c(fitted, residual),\n               names_to = \"type\",\n               values_to = \"value\") |&gt;\n  mutate(\n    type = factor(type,\n                  levels = c(\"fitted\", \"residual\"),\n                  labels = c(\"Fitted Values\", \"Residuals\"))\n  )\n\n# Create r-f spread plot\nggplot(rf_long, aes(x = quantile_rank, y = value, color = hospital)) +\n  geom_point(alpha = 0.4, size = 1.5) +\n  facet_wrap(~ type, ncol = 2, scales = \"free_y\") +\n  labs(\n    title = \"Residual-Fitted Spread Plot\",\n    subtitle = \"Left: Fitted values should track data quantiles | Right: Residuals should be random\",\n    x = \"Quantile of Observed Data\",\n    y = \"Value\",\n    color = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\", size = 11)\n  )\n\n\n\n\n\n\n\n\nHow to Read R-F Spread Plots:\nLeft panel (Fitted Values): - Points should form horizontal lines at each hospital’s mean - This shows that our “model” (group mean) is constant within each group\nRight panel (Residuals): - Points should scatter randomly around zero - Hospital A & B: Residuals spread evenly across quantiles (good fit) - Hospital C: Positive residuals concentrated at upper quantiles (outliers) - Hospital D: Bimodal pattern creates two clusters of residuals\nIf residuals showed a systematic pattern (curve, increasing spread), this would indicate the group mean model is inadequate."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html",
    "href": "applications_7_cleveland_univariate_data.html",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\nlibrary(tukeyedar)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#introduction",
    "href": "applications_7_cleveland_univariate_data.html#introduction",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "",
    "text": "This notebook follows William Cleveland’s systematic approach to understanding univariate data from his foundational book “Visualizing Data” (1993). Cleveland emphasized using visualization as an analytical tool for exploration, not just for presentation.\n\n“Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n— William S. Cleveland, Visualizing Data (1993)\n\nCleveland’s approach: before building models or making inferences, deeply understand your data through visual exploration of location, spread, and distribution shape.\n\n# Required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"tidyr\",\n  \"patchwork\",\n  \"moments\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(moments)\nlibrary(tukeyedar)\n\n# Set theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#simulated-example-data",
    "href": "applications_7_cleveland_univariate_data.html#simulated-example-data",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Simulated Example Data",
    "text": "Simulated Example Data\nWe’ll use a simulated healthcare dataset to explore Cleveland’s concepts. Imagine we’re examining patient recovery times (in days) from a medical procedure across different hospitals.\n\nset.seed(1234)\n\n# Hospital A: Normal distribution, mean recovery time\nhospital_a &lt;- rnorm(200, mean = 14, sd = 3)\n\n# Hospital B: Normal distribution, longer recovery\nhospital_b &lt;- rnorm(200, mean = 18, sd = 3)\n\n# Hospital C: Skewed distribution with some very long recoveries\nhospital_c &lt;- c(\n  rnorm(180, mean = 14, sd = 2.5),\n  runif(20, min = 25, max = 40)  # Some outliers\n)\n\n# Hospital D: Bimodal distribution (two distinct patient groups)\nhospital_d &lt;- c(\n  rnorm(100, mean = 10, sd = 2),\n  rnorm(100, mean = 20, sd = 2)\n)\n\n# Combine into data frame\nrecovery_data &lt;- data.frame(\n  hospital = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 200),\n  recovery_days = c(hospital_a, hospital_b, hospital_c, hospital_d)\n)"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#location-spread",
    "href": "applications_7_cleveland_univariate_data.html#location-spread",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Location & Spread",
    "text": "Location & Spread\nCleveland emphasized understanding the “location” and “spread” of data, which in more common parlance just means measures of central tendency and variance/dispersoin. Different measures of location and spread tell different stories.\n\n# Calculate measures of location for each hospital\nlocation_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    mean = mean(recovery_days),\n    median = median(recovery_days),\n    q25 = quantile(recovery_days, 0.25),\n    q75 = quantile(recovery_days, 0.75),\n    n = n()\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nlocation_summary |&gt;\n  knitr::kable(caption = \"Measures of Location by Hospital\")\n\n\nMeasures of Location by Hospital\n\n\nhospital\nmean\nmedian\nq25\nq75\nn\n\n\n\n\nA\n13.8\n13.5\n11.7\n15.7\n200\n\n\nB\n18.2\n18.4\n16.3\n20.1\n200\n\n\nC\n15.6\n14.3\n12.0\n16.3\n200\n\n\nD\n14.9\n13.8\n9.8\n19.9\n200\n\n\n\n\n\nSummary statistics help:\n\nHospital A & B: Mean ≈ Median (symmetric distributions)\nHospital C: Mean &gt; Median (right-skewed due to outliers)\nHospital D: Bimodal pattern obscures single “center”"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "href": "applications_7_cleveland_univariate_data.html#visual-comparison-mean-vs.-median",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Visual Comparison: Mean vs. Median",
    "text": "Visual Comparison: Mean vs. Median\nVisual comparison gets us farther.\n\nggplot(recovery_data, aes(x = recovery_days, y = hospital)) +\n  geom_boxplot(outlier.alpha = 0.3, fill = \"lightblue\") +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = mean),\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 0.8\n  ) +\n  geom_vline(\n    data = location_summary,\n    aes(xintercept = median),\n    color = \"darkblue\",\n    linetype = \"solid\",\n    linewidth = 0.8\n  ) +\n  labs(\n    title = \"Recovery Times by Hospital\",\n    subtitle = \"Solid line = median (resistant), Dashed line = mean (sensitive to outliers)\",\n    x = \"Recovery Days\",\n    y = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nBox plots are great ways to get quick looks at location and spread. From here, you can see that the median is a skew-resistant measure. It is not affected by extreme values. The mean is sensitive to outliers. Hospital C shows outliers pulling the mean upward.\nThis is a rather elementary point, obviously, but for me, Cleveland’s Visualizing Data has a certain genius about showing the complexity hiding under simple concepts, while reinforcing that elementary summary and visualization tools are often the most important tools in our toolbox."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#spread-measuring-variability",
    "href": "applications_7_cleveland_univariate_data.html#spread-measuring-variability",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread: Measuring Variability",
    "text": "Spread: Measuring Variability\nUnderstanding how data varies around the center is as important and sometimes more important than knowing where the center is.\n\nspread_summary &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  summarize(\n    sd = sd(recovery_days),\n    iqr = IQR(recovery_days),\n    range = max(recovery_days) - min(recovery_days),\n    cv = sd / mean(recovery_days) * 100\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 1)))\n\nspread_summary |&gt;\n  knitr::kable(\n    caption = \"Measures of Spread by Hospital\",\n    col.names = c(\"Hospital\", \"Std Dev\", \"IQR\", \"Range\", \"CV (%)\")\n  )\n\n\nMeasures of Spread by Hospital\n\n\nHospital\nStd Dev\nIQR\nRange\nCV (%)\n\n\n\n\nA\n3.1\n4.0\n17.7\n22.1\n\n\nB\n3.0\n3.8\n18.9\n16.6\n\n\nC\n6.1\n4.2\n32.1\n39.2\n\n\nD\n5.6\n10.1\n19.7\n37.2\n\n\n\n\n\nCommon metrics for understand spread are:\n\nIQR (Interquartile Range): Resistant to outliers, focuses on middle 50% of data\nStandard Deviation: Sensitive to outliers (see Hospital C)\nCoefficient of Variation: Standardized measure of spread relative to mean\n\nBelow we’ll explore how you can use spread and location to understand the structure of the data, check assumptions you may need for modeling (e.g., constant variance), and even get insights into the data generating mechanism of your data."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#spread-location-s-l-plots",
    "href": "applications_7_cleveland_univariate_data.html#spread-location-s-l-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Spread-Location (S-L) Plots",
    "text": "Spread-Location (S-L) Plots\nS-L plots help identify whether variability is constant or changes with the level of the data. This is fundamental to many modeling assumptions (e.g., for pooled t-tests, homoskedasticity).\nCleveland recommended comparing S-L relationships across groups to identify which groups exhibit constant spread vs. monotone spread (increasing variance with location).\n\n# For each hospital, calculate the spread for each observation\n# X-axis: Group median (location)\n# Y-axis: sqrt(|observation - group median|) for each individual observation\nspread_location_data &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    group_median = median(recovery_days),\n    spread = sqrt(abs(recovery_days - group_median))\n  ) |&gt;\n  ungroup()\n\n# Create spread-location plot with all hospitals\nggplot(spread_location_data, aes(x = group_median, y = spread, color = hospital)) +\n  geom_jitter(width = 0.3, alpha = 0.4, size = 2) +\n  stat_summary(fun = median, geom = \"line\", color = \"red\", linewidth = 1.2,\n               aes(group = 1)) +\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 3) +\n  labs(\n    title = \"Spread-Location Plot: All Hospitals\",\n    subtitle = \"Each point = one observation; Red line = median spread across locations\",\n    x = \"Location: Group Median (Days)\",\n    y = \"Spread: √|Value - Group Median|\",\n    color = \"Hospital\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nHow to Read S-L Plots:\nEach point represents one observation, plotted at its group’s median on the x-axis. The y-axis shows how far that observation deviates from its group median (on the square root scale).\n\nVertical bands of points: Each vertical band corresponds to one hospital, positioned at that hospital’s median recovery time\nWidth of vertical band: Shows how variable the data is within that group (wider = more variability)\nRed line: Connects the median spread values across the four locations, revealing the spread-location relationship\n\nInterpretation:\n\nHospitals A & B: Narrow vertical bands at similar median locations (14 and 18 days) indicate low, consistent variability\nHospital C: Wider vertical band shows higher variability due to outliers\nHospital D: Bimodal distribution creates a different spread pattern\n\nThe red line is key: - Flat red line = constant spread (homoskedastic) → assumptions of pooled t-tests, ANOVA, OLS are satisfied - Upward sloping red line = monotone spread (heteroskedastic) → variance increases with location, violating homoskedasticity assumptions"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#mean-difference-plot",
    "href": "applications_7_cleveland_univariate_data.html#mean-difference-plot",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Mean-Difference Plot",
    "text": "Mean-Difference Plot\nWhen comparing two hospitals or measurement methods, Cleveland recommended plotting the difference vs. the average. This reveals whether differences are constant or depend on the magnitude.\n\n# Create comparison data with proper matching\n# Hospital B: constant offset (same SD as A, just shifted)\n# Hospital C: monotone spread (variance increases with magnitude)\nset.seed(789)\n\n# For Hospital B: pair each observation with a random one from Hospital A\nhospital_b_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"B\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital B vs A\"\n  )\n\n# For Hospital C: pair observations - this has outliers so will show monotone spread\nhospital_c_pairs &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  slice_sample(n = 150) |&gt;\n  mutate(\n    hospital_a_value = sample(recovery_data$recovery_days[recovery_data$hospital == \"A\"],\n                             size = 150, replace = TRUE),\n    mean_value = (recovery_days + hospital_a_value) / 2,\n    difference = recovery_days - hospital_a_value,\n    comparison = \"Hospital C vs A\"\n  )\n\ncomparison_pairs &lt;- bind_rows(hospital_b_pairs, hospital_c_pairs)\n\nggplot(comparison_pairs, aes(x = mean_value, y = difference)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\", linewidth = 1) +\n  facet_wrap(~ comparison, ncol = 2) +\n  labs(\n    title = \"Mean-Difference Plots: Comparing Hospitals to Hospital A\",\n    subtitle = \"Red line shows average difference; horizontal = constant spread\",\n    x = \"Average of Two Measurements\",\n    y = \"Difference (Hospital - Hospital A)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nHospital B vs A: Relatively flat red line around +4 days means constant spread (simple location shift)\nHospital C vs A: Upward-sloping red line → monotone spread (variability increases at higher values due to outliers)\n\nThis type of plot is essential in method comparison studies and reveals patterns that simple correlation or regression would miss."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#residuals-actual-vs.-fitted-values",
    "href": "applications_7_cleveland_univariate_data.html#residuals-actual-vs.-fitted-values",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Residuals: Actual vs. Fitted Values",
    "text": "Residuals: Actual vs. Fitted Values\nA fundamental concept in exploratory data analysis is the residual, the difference between what we observe and an expected value based on some model or summary.\nResidual = Observed Value - Fitted Value\nThe “fitted value” can come from:\n\nMean: Using the group mean as the fitted value\nMedian: Using the group median as the fitted value\nModel: Using predictions from a regression model\n\nResiduals help us understand:\n\nPatterns in variation: Are errors random or systematic?\nOutliers: Which observations deviate most from expectations?\nModel fit: Does our model capture the structure in the data?\n\nExamining residuals often provides insight into the data structure and the data generating mechanism. They are especially helpful for understanding whether data from different groups can be pooled: homogenous residuals can be pooled (e.g., in a t-test).\n\nBox Plots of Residuals from Group Means\nLet’s calculate residuals using each hospital’s mean as the fitted value:\n\n# Calculate residuals from group means\nresidual_data &lt;- recovery_data |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    group_mean = mean(recovery_days),\n    residual = recovery_days - group_mean\n  ) |&gt;\n  ungroup()\n\n# Create box plots of residuals\nggplot(residual_data, aes(x = hospital, y = residual, fill = hospital)) +\n  geom_boxplot(alpha = 0.7, outlier.size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Residuals from Group Means\",\n    subtitle = \"Residual = Observed - Mean; Symmetric around zero = good fit\",\n    x = \"Hospital\",\n    y = \"Residual (Days)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nInterpretation:\n\nHospital A & B: Residuals are roughly symmetric around zero (red dashed line)\nHospital C: Positive outlier residuals → some patients take much longer than the mean\nHospital D: Wider spread of residuals reflects the bimodal distribution\n\nWhen residuals show patterns (asymmetry, increasing spread, etc.), this suggests the simple mean doesn’t fully capture the data structure."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#understanding-quantiles",
    "href": "applications_7_cleveland_univariate_data.html#understanding-quantiles",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Understanding Quantiles",
    "text": "Understanding Quantiles\nThe quantile is a key tool for studying spread and location together. A quantile answers the question: “What value do X% of my observations fall below?”\nThe pth percentile is the value where p percent of the data falls below it. For example:\n\n25th percentile (p25): 25% of observations are below this value\n50th percentile (p50): 50% of observations are below this value (this is the median)\n75th percentile (p75): 75% of observations are below this value\n\nAgain, this is very elementary, but in practice proves very powerful.\n\nHow to Calculate Quantiles\nStep 1: Sort your data from smallest to largest\nStep 2: Find the position\nFor the pth percentile with n observations:\n\\[\\text{Position} = p \\times (n + 1)\\]\nStep 3: Get the value\n\nIf the position is a whole number (like 25), use that observation\nIf the position is fractional (like 25.25), interpolate between the two nearest values\n\nThere are a number of ways to calculate quantiles from Data. R’s quantile() function has 9 different ways to do it. Click this link to look through them.\n\n\nQuantiles Are…\n\nDistribution-free\nRobust to outliers\nEasy to compare: Comparing quantiles across groups reveals differences in location, spread, and shape."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#residual-fitted-r-f-spread-plot",
    "href": "applications_7_cleveland_univariate_data.html#residual-fitted-r-f-spread-plot",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Residual-Fitted (R-F) Spread Plot",
    "text": "Residual-Fitted (R-F) Spread Plot\nThe R-F spread plot visually decomposes your data into two parts:\n\nWhat the grouping explains (left panel)\nWhat remains unexplained (right panel)\n\nThink of it as a visual ANOVA. If the left panel is wide, grouping matters. If the right panel is wide, there’s lots of leftover variation.\n\n# Calculate TWO different percentile ranks:\n# 1. For left panel: sort by fitted values first (creates non-overlapping bands by group)\n# 2. For right panel: sort by residuals (pools all residuals together)\nrf_data &lt;- residual_data |&gt;\n  mutate(\n    fitted = group_mean,\n    grand_mean = mean(recovery_days),\n    fit_minus_mean = fitted - grand_mean\n  ) |&gt;\n  arrange(fitted, recovery_days) |&gt;\n  mutate(\n    f_value_fitted = (row_number() - 1) / (n() - 1)\n  ) |&gt;\n  arrange(residual) |&gt;\n  mutate(\n    f_value_residual = (row_number() - 1) / (n() - 1)\n  )\n\n# Create separate datasets for each panel\nfitted_panel &lt;- rf_data |&gt;\n  select(hospital, f_value = f_value_fitted, fit_minus_mean) |&gt;\n  ungroup()\n\nresidual_panel &lt;- rf_data |&gt;\n  select(f_value = f_value_residual, residual) |&gt;\n  ungroup()\n\n# Create r-f spread plot with improved aesthetics\nlibrary(patchwork)\n\n# Custom color palette for hospitals\nhospital_colors &lt;- c(\"#E64B35\", \"#4DBBD5\", \"#00A087\", \"#3C5488\")\n\np_fitted &lt;- ggplot(fitted_panel, aes(x = f_value, y = fit_minus_mean, color = hospital)) +\n  geom_point(alpha = 0.5, size = 2.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  scale_color_manual(values = hospital_colors) +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Between-Group Variation\",\n    subtitle = \"How much do the groups differ?\",\n    x = \"Percentile Rank (sorted by group)\",\n    y = \"Distance from Overall Mean (days)\",\n    color = \"Hospital\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray40\", size = 11),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, linewidth = 0.5)\n  )\n\np_residual &lt;- ggplot(residual_panel, aes(x = f_value, y = residual)) +\n  geom_point(alpha = 0.4, size = 2.5, color = \"#7570B3\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Within-Group Variation\",\n    subtitle = \"What's left unexplained?\",\n    x = \"Percentile Rank (pooled residuals)\",\n    y = \"Residual: Observed - Group Mean (days)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray40\", size = 11),\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, linewidth = 0.5)\n  )\n\n# Combine panels\np_fitted + p_residual +\n  plot_annotation(\n    title = \"Residual-Fitted (R-F) Spread Plot: Visual ANOVA\",\n    theme = theme(plot.title = element_text(face = \"bold\", size = 16))\n  )\n\n\n\n\n\n\n\n\n\nHow to Read This Plot\nLeft Panel - Between-Group Variation:\n\nEach colored band represents one hospital\nBand width = proportion of patients in that hospital\nBand height = how far that hospital’s average is from the overall average\nWide vertical spread → hospitals differ a lot → grouping explains variation\nNarrow vertical spread → hospitals are similar → grouping doesn’t help much\n\nRight Panel - Within-Group Variation:\n\nAll residuals pooled together (no colors)\nShows what’s left after accounting for hospital differences\nWide vertical spread → lots of unexplained variation remains\nNarrow vertical spread → hospital membership explains most of the variation\nPoints should scatter randomly around zero (horizontal line)\n\nThe Key Insight:\nCompare the vertical spread of the two panels. If the left panel is much wider than the right, the grouping (hospital) explains most of the variation. If the right panel is wider, there’s lots of individual variation that hospital membership doesn’t capture."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#power-transformations-for-stabilizing-variance",
    "href": "applications_7_cleveland_univariate_data.html#power-transformations-for-stabilizing-variance",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Power Transformations for Stabilizing Variance",
    "text": "Power Transformations for Stabilizing Variance\nWhen spread-location plots reveal monotone spread (variance changes with level), Cleveland advocated for power transformations to stabilize variance and make distributions more symmetric.\n\nThe Power Transformation Family\nThe power transformation family takes the form:\n\\[y^{\\lambda} \\text{ where } \\lambda \\text{ is the power}\\]\nCommon transformations:\n\n\n\nλ\nTransformation\nName\nWhen to Use\n\n\n\n\n2\n\\(y^2\\)\nSquare\nCompress right tail\n\n\n1\n\\(y\\)\nIdentity\nNo transformation needed\n\n\n0.5\n\\(\\sqrt{y}\\)\nSquare root\nModerate right skew, count data\n\n\n0\n\\(\\log(y)\\)\nLog*\nStrong right skew, multiplicative processes\n\n\n-0.5\n\\(-1/\\sqrt{y}\\)\nInverse square root\nVery strong right skew\n\n\n-1\n\\(-1/y\\)\nInverse\nExtreme right skew\n\n\n\n\n\nWhen to Transform\n\nMonotone spread: Variance increases with the mean\nStrong skewness: Long tails distort analyses\nMultiplicative relationships: Percent changes, growth rates\nMeeting model assumptions: Many tests assume constant variance and normality\n\n\n\nExample: Log Transformation for Hospital C\nHospital C has outliers creating right skew and monotone spread. Let’s see if log transformation helps.\n\n# Create comparison of original vs log-transformed data for Hospital C\nhospital_c_data &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  mutate(\n    log_recovery = log(recovery_days),\n    sqrt_recovery = sqrt(recovery_days)\n  )\n\n# Create histograms\np1 &lt;- ggplot(hospital_c_data, aes(x = recovery_days)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Original Scale\",\n    x = \"Recovery Days\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np2 &lt;- ggplot(hospital_c_data, aes(x = log_recovery)) +\n  geom_histogram(bins = 30, fill = \"coral\", color = \"white\") +\n  labs(\n    title = \"Log-Transformed\",\n    x = \"Log(Recovery Days)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np3 &lt;- ggplot(hospital_c_data, aes(x = sqrt_recovery)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", color = \"white\") +\n  labs(\n    title = \"Square Root Transformed\",\n    x = \"√(Recovery Days)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n# Combine plots\n(p1 / p2 / p3) +\n  plot_annotation(\n    title = \"Effect of Power Transformations on Hospital C Data\",\n    subtitle = \"Log and √ transformations reduce right skew and stabilize variance\"\n  )\n\n\n\n\n\n\n\n\n\n\nAssessing the Transformation\n\n# Compare summary statistics before and after transformation\ntransformation_summary &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"SD\", \"IQR\", \"Skewness\"),\n  Original = c(\n    mean(hospital_c_data$recovery_days),\n    median(hospital_c_data$recovery_days),\n    sd(hospital_c_data$recovery_days),\n    IQR(hospital_c_data$recovery_days),\n    moments::skewness(hospital_c_data$recovery_days)\n  ),\n  Log = c(\n    mean(hospital_c_data$log_recovery),\n    median(hospital_c_data$log_recovery),\n    sd(hospital_c_data$log_recovery),\n    IQR(hospital_c_data$log_recovery),\n    moments::skewness(hospital_c_data$log_recovery)\n  ),\n  Sqrt = c(\n    mean(hospital_c_data$sqrt_recovery),\n    median(hospital_c_data$sqrt_recovery),\n    sd(hospital_c_data$sqrt_recovery),\n    IQR(hospital_c_data$sqrt_recovery),\n    moments::skewness(hospital_c_data$sqrt_recovery)\n  )\n) |&gt;\n  mutate(across(where(is.numeric), ~round(., 2)))\n\ntransformation_summary |&gt;\n  knitr::kable(caption = \"Comparing Transformations for Hospital C\")\n\n\nComparing Transformations for Hospital C\n\n\nMeasure\nOriginal\nLog\nSqrt\n\n\n\n\nMean\n15.59\n2.69\n3.89\n\n\nMedian\n14.29\n2.66\n3.78\n\n\nSD\n6.11\n0.32\n0.68\n\n\nIQR\n4.23\n0.30\n0.56\n\n\nSkewness\n2.12\n1.07\n1.64\n\n\n\n\n\nLog scale: Differences become ratios (additive turns to multiplicative); logs are typically preferred over other transformations because of this interpretive enefit\n\n\nChoosing the Right Transformation\nCleveland suggested using data-driven approaches:\n\nVisual inspection: Try several transformations, plot results\nSymmetry: Choose transformation that makes distribution most symmetric\nConstant variance: Choose transformation that stabilizes spread-location relationship\n\nThe goal is not mathematical purity but getting an intimate undrstanding of your data."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#comparing-distributions-q-q-plots",
    "href": "applications_7_cleveland_univariate_data.html#comparing-distributions-q-q-plots",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Comparing Distributions: Q-Q Plots",
    "text": "Comparing Distributions: Q-Q Plots\nCleveland emphasized quantile-quantile (Q-Q) plots for comparing distributions. They plot quantiles of one distribution against another.\n\n# Compare each hospital to Hospital A\ncomparison_data &lt;- recovery_data |&gt;\n  filter(hospital != \"A\") |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    quantile_rank = rank(recovery_days) / (n() + 1)\n  ) |&gt;\n  ungroup()\n\n# Get corresponding quantiles from Hospital A\nhospital_a_quantiles &lt;- function(p) {\n  quantile(recovery_data$recovery_days[recovery_data$hospital == \"A\"], p)\n}\n\ncomparison_data &lt;- comparison_data |&gt;\n  mutate(\n    hospital_a_value = sapply(quantile_rank, hospital_a_quantiles)\n  )\n\nggplot(comparison_data, aes(x = hospital_a_value, y = recovery_days)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ hospital, ncol = 3) +\n  labs(\n    title = \"Q-Q Plots: Comparing Hospitals to Hospital A\",\n    subtitle = \"Points on red line = identical distributions\",\n    x = \"Hospital A Quantiles\",\n    y = \"Other Hospital Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nInterpreting Q-Q Plots:\n\nHospital B: Points above line → consistently longer recovery times\nHospital C: Curves up at high quantiles → longer tail (outliers)\nHospital D: S-shaped curve → different distribution shape (bimodal vs. unimodal)"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#q-q-plots-for-assessing-normality",
    "href": "applications_7_cleveland_univariate_data.html#q-q-plots-for-assessing-normality",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Q-Q Plots for Assessing Normality",
    "text": "Q-Q Plots for Assessing Normality\nA particularly important use of Q-Q plots is assessing whether data follows a normal distribution. This is done by plotting sample quantiles against theoretical normal quantiles.\n\nWhy This Matters\nMany statistical methods assume normality: - t-tests - ANOVA - Linear regression (assumes normal residuals) - Confidence intervals based on standard errors\nCleveland emphasized visual assessment over formal tests because: 1. Visual patterns reveal the type of departure (skew, outliers, heavy tails) 2. Sample size affects formal tests (large n makes trivial departures “significant”) 3. We can see if departures matter for our specific purpose\n\n\nHow It Works\nNormal Q-Q Plot: - X-axis: Theoretical quantiles from standard normal distribution - Y-axis: Observed quantiles from your data - If data is normal: Points fall on a straight line - If not normal: Systematic deviations reveal the problem\n\n# Create Q-Q plots for each hospital\nlibrary(patchwork)\n\n# Hospital A - approximately normal\np_a &lt;- ggplot(recovery_data |&gt; filter(hospital == \"A\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital A: Approximately Normal\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital B - normal but shifted\np_b &lt;- ggplot(recovery_data |&gt; filter(hospital == \"B\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital B: Normal (Different Mean)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital C - right-skewed with outliers\np_c &lt;- ggplot(recovery_data |&gt; filter(hospital == \"C\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital C: Right-Skewed + Outliers\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Hospital D - bimodal\np_d &lt;- ggplot(recovery_data |&gt; filter(hospital == \"D\"),\n              aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Hospital D: Bimodal (Non-Normal)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine\n(p_a + p_b) / (p_c + p_d) +\n  plot_annotation(\n    title = \"Normal Q-Q Plots: Diagnosing Departures from Normality\"\n  )\n\n\n\n\n\n\n\n\n\n\nReading the Patterns\n1. Points on the line (Hospitals A & B): - Data follows normal distribution - Parametric methods are safe to use\n2. Curve at upper end (Hospital C): - Right tail heavier than normal → right skew - Upper quantiles deviate upward from line - Common with positive data (income, recovery times) - Solution: Log or square root transformation\n3. Curve at both ends (Hospital C outliers): - Points at extremes deviate from line - Indicates outliers beyond what normal distribution would produce - Action: Investigate outliers, consider robust methods\n4. S-shaped curve (Hospital D): - Points curve below line at low end, above at high end - Distribution has shorter tails than normal (or is bimodal) - Indicates: Fundamentally different distribution shape - Action: Don’t assume normality, use distribution-free methods\n5. Points below line at low end, above at high end: - Distribution has heavier tails than normal - More extreme values than normal distribution predicts - Common in financial data, measurement errors\n\n\nBenefits of Q-Q Plots Over Formal Tests\nShapiro-Wilk test and similar tests have limitations:\n\n\n\n\n\n\n\nFormal Tests\nQ-Q Plots\n\n\n\n\nBinary answer (reject/fail to reject)\nShows what and how much departure\n\n\nVery sensitive with large n\nAssess practical significance\n\n\nDon’t reveal type of departure\nSee if it’s skew, outliers, heavy tails, etc.\n\n\nNo guidance on fix\nSuggests transformation strategy\n\n\nArbitrary α threshold\nProfessional judgment based on context\n\n\n\nExample: With n = 10,000, Shapiro-Wilk might reject normality for trivial departures that don’t affect t-test validity. Q-Q plot shows if departure matters for your analysis.\n\n\nPractical Workflow\nCleveland’s approach to checking normality:\n\nStart with histogram: Get overall shape sense\nCreate Q-Q plot: Diagnose specific departures\nAssess practical impact:\n\nSlight skew with large n? Probably fine.\nStrong bimodality? Serious problem.\nOutliers? Investigate before proceeding.\n\nConsider alternatives:\n\nTransform to achieve normality\nUse robust methods (median, bootstrap)\nUse distribution-free tests (Mann-Whitney, Kruskal-Wallis)\n\n\n\n\nWhen Normality Doesn’t Matter\nWith large samples, Central Limit Theorem means: - Means are approximately normal even if data isn’t - t-tests, ANOVA are robust to moderate departures - More concerned about outliers than mild skewness\nFocus Q-Q plot assessment on: - Small samples (n &lt; 30): Normality matters more - Extreme violations: Bimodality, severe skew, major outliers - Residual diagnostics: In regression, check if residuals are normal\n\n\nExample: Hospital C After Log Transformation\n\n# Compare original vs log-transformed Hospital C\nhospital_c_data &lt;- recovery_data |&gt;\n  filter(hospital == \"C\") |&gt;\n  mutate(log_recovery = log(recovery_days))\n\np_orig &lt;- ggplot(hospital_c_data, aes(sample = recovery_days)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Scale\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles (Days)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np_log &lt;- ggplot(hospital_c_data, aes(sample = log_recovery)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Log-Transformed\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles (Log Days)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np_orig + p_log +\n  plot_annotation(\n    title = \"Log Transformation Improves Normality\",\n    subtitle = \"Points align better with line after transformation\"\n  )\n\n\n\n\n\n\n\n\nKey observation: Log transformation brings the upper tail back toward the line, making distribution more symmetric and closer to normal.\n\n\nCleveland’s Message\n\n“The goal is not to prove normality, but to understand your data’s shape and choose appropriate methods.”\n\nQ-Q plots serve exploration and diagnosis, not hypothesis testing. Use them to: - Understand what you’re working with - Decide if assumptions are reasonable - Choose between parametric and robust methods - Transform when it aids interpretation"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#references",
    "href": "applications_7_cleveland_univariate_data.html#references",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "References",
    "text": "References\nCleveland, W. S. (1993). Visualizing Data. Hobart Press."
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#power-transformations",
    "href": "applications_7_cleveland_univariate_data.html#power-transformations",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Power Transformations",
    "text": "Power Transformations\nPower transformations can fix skewed data and stabilize variance. Cleveland emphasized using visual tools (especially Q-Q plots) to diagnose problems and assess whether transformations help.\n\nExample 1: Log Transformation for Earnings\nWhy log transform earnings? Income and earnings follow multiplicative processes - a 10% raise means different dollar amounts for low vs high earners, but the same proportional change. Earnings data is typically right-skewed with a long tail.\n\n# Simulate earnings data with right skew (multiplicative process)\nset.seed(456)\nn &lt;- 500\n\n# Generate log-normal earnings (typical for income data)\nearnings_data &lt;- data.frame(\n  earnings = exp(rnorm(n, mean = log(50000), sd = 0.6)),  # Median ~$50k\n  health_score = rnorm(n, mean = 75, sd = 10)\n) |&gt;\n  mutate(\n    log_earnings = log(earnings)\n  )\n\n# Create side-by-side Q-Q plots\np1 &lt;- ggplot(earnings_data, aes(sample = earnings)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Earnings\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(earnings_data, aes(sample = log_earnings)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Log(Earnings)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np1 + p2 +\n  plot_annotation(\n    title = \"Log Transformation Fixes Right Skew in Earnings\",\n    subtitle = \"Left: Original data curves up (right skew) | Right: Log-transformed aligns with normal line\"\n  )\n\n\n\n\n\n\n\n\nInterpretation: - Left panel: Original earnings curve upward at upper quantiles → right skew - Right panel: Log(earnings) points lie on the line → approximately normal - Why it works: Log transforms multiplicative relationships into additive ones - On log scale, a 10% raise is the same distance for everyone\n\n\nExample 2: Square Root Transformation for Count Data\nWhy square root for counts? Count data (e.g., number of events, accidents, infections) often has variance that increases with the mean. Square root transformation stabilizes this.\n\n# Simulate count data (Poisson-distributed)\nset.seed(789)\n\n# Generate data from multiple groups with different means\ncount_data &lt;- data.frame(\n  group = rep(c(\"Low\", \"Medium\", \"High\"), each = 200),\n  count = c(\n    rpois(200, lambda = 5),   # Low rate\n    rpois(200, lambda = 15),  # Medium rate\n    rpois(200, lambda = 30)   # High rate\n  )\n) |&gt;\n  mutate(\n    sqrt_count = sqrt(count + 0.5)  # Add 0.5 to handle zeros\n  )\n\n# Create side-by-side Q-Q plots\np1 &lt;- ggplot(count_data, aes(sample = count)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Counts\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(count_data, aes(sample = sqrt_count)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"√(Count + 0.5)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np1 + p2 +\n  plot_annotation(\n    title = \"Square Root Transformation Stabilizes Variance in Count Data\",\n    subtitle = \"Left: Original shows increasing variance | Right: √ transformation makes variance more constant\"\n  )\n\n\n\n\n\n\n\n\nInterpretation: - Left panel: Original counts show a curve (variance increases with mean) - Right panel: √(count) is closer to a straight line - Why it works: For Poisson data, variance = mean, so √ stabilizes this relationship\n\n\nCleveland’s Transformation Strategy\n\nLook at Q-Q plots first - see if data deviates from normal\nIdentify the pattern:\n\nCurve up at high end → try log or √\nCurve down at high end → try square\n\nCheck the Q-Q plot after transformation - did it improve?\nConsider interpretability:\n\nLog: Multiplicative relationships, percent changes\n√: Count data, variance stabilization\n\nRemember: The goal is understanding your data, not forcing normality"
  },
  {
    "objectID": "applications_7_cleveland_univariate_data.html#transformations",
    "href": "applications_7_cleveland_univariate_data.html#transformations",
    "title": "Understanding Univariate Data: Location and Spread",
    "section": "Transformations",
    "text": "Transformations\nTransformations can fix skewed data and stabilize variance. Cleveland emphasized using visualization (especially Q-Q plots) to diagnose problems and assess whether transformations help.\n\nExample 1: Log Transformation for Earnings\nWhy log transform earnings? Income and earnings follow multiplicative processes. A 10% raise means different dollar amounts for low vs high earners, but the same proportional change. Earnings data is typically right-skewed with a long tail.\n\n# Simulate earnings data with right skew (multiplicative process)\nset.seed(456)\nn &lt;- 500\n\n# Generate log-normal earnings (typical for income data)\nearnings_data &lt;- data.frame(\n  earnings = exp(rnorm(n, mean = log(50000), sd = 0.6)),  # Median ~$50k\n  health_score = rnorm(n, mean = 75, sd = 10)\n) |&gt;\n  mutate(\n    log_earnings = log(earnings)\n  )\n\n# Create side-by-side Q-Q plots\np1 &lt;- ggplot(earnings_data, aes(sample = earnings)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Original Earnings\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(earnings_data, aes(sample = log_earnings)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Log(Earnings)\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\np1 + p2 +\n  plot_annotation(\n    title = \"Log Transformation Fixes Right Skew in Earnings\",\n    subtitle = \"Left: Original data curves up (right skew) | Right: Log-transformed aligns with normal line\"\n  )\n\n\n\n\n\n\n\n\nInterpretation:\n\nLeft panel: Original earnings curve upward at upper quantiles → right skew\nRight panel: Log(earnings) points lie on the line → approximately normal\nWhy it works: Log transforms multiplicative relationships into additive ones\nOn log scale, a 10% raise is the same distance for everyone\n\n\n\nExample 2: The Ladder of Powers\nTry several transformations from the ladder of powers and visually compare them using Q-Q plots.\nFor example:\n\n\\(x^3\\) (cube)\n\\(x^2\\) (square)\n\\(x^1\\) (original, no transformation)\n\\(x^{1/2}\\) (square root)\n\\(x^{-1}\\) (reciprocal)\n\\(x^{-2}\\) (reciprocal square)\n\\(x^{-3}\\) (reciprocal cube)\n\nI add \\(\\log(x)\\) in there too. You can think of it as the limit of \\(x^p\\) as \\(p\\) approaches 0, since for small \\(p\\), \\(x^p\\) behaves similarly to \\(\\log(x)\\). Cleveland (1993, p. 56) explains that the derivative of \\(\\log(x)\\) is \\(\\frac{1}{x}\\), and the derivative of \\(x^{0.001}\\) is proportional to \\(x^{-0.999}\\). The intuition is:\n\nFor large \\(p\\), you get steep curves (\\(x^2\\), \\(x^3\\)).\nFor small \\(p\\), they flatten out and start to look like \\(\\log(x)\\).\n\n\nlibrary(tidyr)\n\n# Use the earnings data from Example 1\nset.seed(456)\nn &lt;- 500\nearnings &lt;- exp(rnorm(n, mean = log(50000), sd = 0.6))\n\n# Apply different power transformations\ntransformation_data &lt;- data.frame(\n  x = earnings\n) |&gt;\n  mutate(\n    x_cubed = x^3,\n    x_squared = x^2,\n    x_original = x,\n    x_log = log(x),\n    x_sqrt = sqrt(x),\n    x_inv = 1/x,\n    x_inv_squared = 1/(x^2),\n    x_inv_cubed = 1/(x^3)\n  ) |&gt;\n  pivot_longer(\n    cols = starts_with(\"x_\"),\n    names_to = \"transformation\",\n    values_to = \"value\"\n  ) |&gt;\n  mutate(\n    transformation = factor(\n      transformation,\n      levels = c(\"x_cubed\", \"x_squared\", \"x_original\", \"x_log\", \"x_sqrt\", \"x_inv\", \"x_inv_squared\", \"x_inv_cubed\"),\n      labels = c(\n        \"x³\",\n        \"x²\",\n        \"x (original)\",\n        \"log\",\n        \"√x\",\n        \"1/x\",\n        \"1/x²\",\n        \"1/x³\"\n      )\n    )\n  )\n\n# Create trellis of Q-Q plots\nggplot(transformation_data, aes(sample = value)) +\n  stat_qq(size = 0.5, alpha = 0.6) +\n  stat_qq_line(color = \"red\", linewidth = 0.8) +\n  facet_wrap(~ transformation, scales = \"free\", ncol = 4) +\n  labs(\n    title = \"Ladder of Powers: Comparing Transformations for Earnings Data\",\n    subtitle = \"Which transformation makes the data most closely follow the line?\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 12),\n    strip.background = element_rect(fill = \"gray90\", color = NA),\n    panel.border = element_rect(color = \"gray80\", fill = NA, linewidth = 0.5),\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\nHow to read this:\n\nEach panel shows the same data with a different transformation\nLook for which Q-Q plot has points closest to the red line\n\nStrategy: Create this trellis for your data, compare visually. The goal isn’t necessarily to find a fix, but to understand the structure of the data. One tip: when the log is the best or close to the others – pick it. You can then use multiplicative interpretation."
  },
  {
    "objectID": "causal_1_making_dags.html",
    "href": "causal_1_making_dags.html",
    "title": "Making DAGs with ggdag",
    "section": "",
    "text": "Directed Acyclic Graphs (DAGs) are visual representations of causal relationships. They help us:\n\nMake our assumptions about causality explicit\nIdentify which variables to control for\nUnderstand potential sources of bias\nCommunicate causal models clearly\n\nThe ggdag package makes it easy to create beautiful DAGs using familiar ggplot2 syntax."
  },
  {
    "objectID": "causal_1_making_dags.html#introduction",
    "href": "causal_1_making_dags.html#introduction",
    "title": "Making DAGs with ggdag",
    "section": "",
    "text": "Directed Acyclic Graphs (DAGs) are visual representations of causal relationships. They help us:\n\nMake our assumptions about causality explicit\nIdentify which variables to control for\nUnderstand potential sources of bias\nCommunicate causal models clearly\n\nThe ggdag package makes it easy to create beautiful DAGs using familiar ggplot2 syntax."
  },
  {
    "objectID": "causal_1_making_dags.html#basic-dag-structure",
    "href": "causal_1_making_dags.html#basic-dag-structure",
    "title": "Making DAGs with ggdag",
    "section": "Basic DAG Structure",
    "text": "Basic DAG Structure\n\nSimple DAG: X → Y\nThe simplest DAG shows one variable causing another.\n\n# Create a basic DAG: X causes Y\nsimple_dag &lt;- dagify(\n  Y ~ X\n)\n\n# Visualize it\nggdag(simple_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\nThe arrow shows that X causes Y (X → Y)."
  },
  {
    "objectID": "causal_1_making_dags.html#customizing-dag-appearance",
    "href": "causal_1_making_dags.html#customizing-dag-appearance",
    "title": "Making DAGs with ggdag",
    "section": "Customizing DAG Appearance",
    "text": "Customizing DAG Appearance\n\nAdding Labels\nUse descriptive names instead of single letters:\n\n# Create DAG with meaningful labels\nlabeled_dag &lt;- dagify(\n  outcome ~ exposure + confounder,\n  exposure ~ confounder,\n  labels = c(\n    \"exposure\" = \"Treatment\",\n    \"outcome\" = \"Health\",\n    \"confounder\" = \"Age\"\n  )\n)\n\nggdag(labeled_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nCustom Positioning\nControl where nodes appear using coordinates:\n\n# Specify exact coordinates for nodes\ncoords &lt;- list(\n  x = c(exposure = 1, outcome = 3, confounder = 2),\n  y = c(exposure = 1, outcome = 1, confounder = 2)\n)\n\npositioned_dag &lt;- dagify(\n  outcome ~ exposure + confounder,\n  exposure ~ confounder,\n  coords = coords,\n  labels = c(\n    \"exposure\" = \"Treatment\",\n    \"outcome\" = \"Health\",\n    \"confounder\" = \"Age\"\n  )\n)\n\nggdag(positioned_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nStyling with ggplot2\nSince ggdag uses ggplot2, you can customize with familiar syntax:\n\nggdag(positioned_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  ) +\n  labs(title = \"Causal Model: Effect of Treatment on Health\")"
  },
  {
    "objectID": "causal_1_making_dags.html#common-causal-structures",
    "href": "causal_1_making_dags.html#common-causal-structures",
    "title": "Making DAGs with ggdag",
    "section": "Common Causal Structures",
    "text": "Common Causal Structures\n\nThe Fork (Common Cause)\n\nfork_dag &lt;- dagify(\n  Y ~ Z,\n  X ~ Z,\n  coords = list(\n    x = c(X = 1, Y = 3, Z = 2),\n    y = c(X = 1, Y = 1, Z = 2)\n  )\n)\n\nggdag(fork_dag) +\n  theme_dag() +\n  labs(title = \"Fork: Z is a common cause of X and Y\")\n\n\n\n\n\n\n\n\nStrategy: Control for Z to block the non-causal path.\n\n\nThe Chain (Mediation)\n\nchain_dag &lt;- dagify(\n  Y ~ M,\n  M ~ X,\n  coords = list(\n    x = c(X = 1, M = 2, Y = 3),\n    y = c(X = 1, M = 1, Y = 1)\n  )\n)\n\nggdag(chain_dag) +\n  theme_dag() +\n  labs(title = \"Chain: M mediates the effect of X on Y\")\n\n\n\n\n\n\n\n\nStrategy: Don’t control for M if you want the total effect of X on Y.\n\n\nThe Inverted Fork (Collider)\n\ninverted_fork_dag &lt;- dagify(\n  C ~ X + Y,\n  coords = list(\n    x = c(X = 1, Y = 3, C = 2),\n    y = c(X = 2, Y = 2, C = 1)\n  )\n)\n\nggdag(inverted_fork_dag) +\n  theme_dag() +\n  labs(title = \"Inverted Fork: C is a collider\")\n\n\n\n\n\n\n\n\nStrategy: Never control for C - it opens a non-causal path!"
  },
  {
    "objectID": "causal_1_making_dags.html#practical-example-healthcare-access",
    "href": "causal_1_making_dags.html#practical-example-healthcare-access",
    "title": "Making DAGs with ggdag",
    "section": "Practical Example: Healthcare Access",
    "text": "Practical Example: Healthcare Access\nLet’s build a realistic DAG for healthcare access and outcomes:\n\nhealthcare_dag &lt;- dagify(\n  health_outcome ~ healthcare_access + age + comorbidities + insurance,\n  healthcare_access ~ income + insurance + distance + provider_quality,\n  insurance ~ income + job_type,\n  income ~ education + age,\n  comorbidities ~ age + income,\n  provider_quality ~ geography,\n  distance ~ geography,\n\n  coords = list(\n    x = c(\n      health_outcome = 5,\n      healthcare_access = 3,\n      age = 1,\n      comorbidities = 3,\n      insurance = 2,\n      income = 1,\n      education = 0.5,\n      job_type = 0.5,\n      distance = 2.5,\n      provider_quality = 2.5,\n      geography = 1.5\n    ),\n    y = c(\n      health_outcome = 3,\n      healthcare_access = 3,\n      age = 4,\n      comorbidities = 4,\n      insurance = 2,\n      income = 2,\n      education = 1,\n      job_type = 3,\n      distance = 1,\n      provider_quality = 2,\n      geography = 1\n    )\n  ),\n\n  labels = c(\n    \"health_outcome\" = \"Health\\nOutcome\",\n    \"healthcare_access\" = \"Healthcare\\nAccess\",\n    \"age\" = \"Age\",\n    \"comorbidities\" = \"Comorbidities\",\n    \"insurance\" = \"Insurance\",\n    \"income\" = \"Income\",\n    \"education\" = \"Education\",\n    \"job_type\" = \"Job Type\",\n    \"distance\" = \"Distance to\\nProvider\",\n    \"provider_quality\" = \"Provider\\nQuality\",\n    \"geography\" = \"Geography\"\n  ),\n\n  exposure = \"healthcare_access\",\n  outcome = \"health_outcome\"\n)\n\nggdag(healthcare_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11)\n  ) +\n  labs(\n    title = \"Causal Model: Healthcare Access and Health Outcomes\",\n    subtitle = \"Red = Exposure | Blue = Outcome\"\n  )"
  },
  {
    "objectID": "causal_1_making_dags.html#identifying-adjustment-sets",
    "href": "causal_1_making_dags.html#identifying-adjustment-sets",
    "title": "Making DAGs with ggdag",
    "section": "Identifying Adjustment Sets",
    "text": "Identifying Adjustment Sets\nThe ggdag package can automatically identify which variables to control for:\n\n# Show what to adjust for\nggdag_adjustment_set(healthcare_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  labs(title = \"What variables should we control for?\")\n\n\n\n\n\n\n\n\nGreen variables are in the adjustment set - control for these to estimate the causal effect."
  },
  {
    "objectID": "causal_1_making_dags.html#dag-analysis-paths",
    "href": "causal_1_making_dags.html#dag-analysis-paths",
    "title": "Making DAGs with ggdag",
    "section": "DAG Analysis: Paths",
    "text": "DAG Analysis: Paths\nVisualize all paths from exposure to outcome:\n\n# Show all paths\nggdag_paths(healthcare_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  labs(title = \"All Paths from Healthcare Access to Health Outcome\")"
  },
  {
    "objectID": "causal_1_making_dags.html#dag-analysis-backdoor-paths",
    "href": "causal_1_making_dags.html#dag-analysis-backdoor-paths",
    "title": "Making DAGs with ggdag",
    "section": "DAG Analysis: Backdoor Paths",
    "text": "DAG Analysis: Backdoor Paths\nIdentify confounding (backdoor) paths:\n\n# Show backdoor paths that create confounding\nggdag_parents(healthcare_dag, \"health_outcome\", text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  labs(title = \"What directly affects Health Outcome?\")"
  },
  {
    "objectID": "causal_1_making_dags.html#tips-for-making-dags",
    "href": "causal_1_making_dags.html#tips-for-making-dags",
    "title": "Making DAGs with ggdag",
    "section": "Tips for Making DAGs",
    "text": "Tips for Making DAGs\n\nStart simple: Begin with just exposure -&gt; outcome\nAdd confounders: What affects both?\nConsider mediators: What’s on the causal pathway?\nWatch for colliders: Don’t condition on them!\nGet feedback: Show your DAG to domain experts\nIterate: DAGs evolve as you learn more\n\nBooks:\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press.\n\nLecture videos on YouTube\n\nHernán, M. A., & Robins, J. M. Causal Inference: What If. Free online\n\nSoftware:\n\nggdag documentation - R package for drawing DAGs\ndagitty.net - Browser-based interactive DAG tool"
  },
  {
    "objectID": "causal_1_making_dags.html#common-mistakes",
    "href": "causal_1_making_dags.html#common-mistakes",
    "title": "Making DAGs with ggdag",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\nMistake 1: Controlling for a Mediator\n\n# Don't control for M if you want the total effect!\nchain_dag &lt;- dagify(\n  Y ~ M + X,\n  M ~ X\n)\n\nggdag(chain_dag) +\n  theme_dag() +\n  labs(title = \"Don't control for M (mediator) for total effect\")\n\n\n\n\n\n\n\n\n\n\nMistake 2: Controlling for a Collider\n\n# Controlling for C opens a non-causal path!\ncollider_dag &lt;- dagify(\n  C ~ X + Y\n)\n\nggdag(collider_dag) +\n  theme_dag() +\n  labs(title = \"Never control for C (collider)\")\n\n\n\n\n\n\n\n\n\n\nMistake 3: Missing a Confounder\n\n# If you don't control for Z, X→Y is biased\nconfounder_dag &lt;- dagify(\n  Y ~ X + Z,\n  X ~ Z\n)\n\nggdag(confounder_dag) +\n  theme_dag() +\n  labs(title = \"Must control for Z (confounder)\")"
  },
  {
    "objectID": "causal_1_making_dags.html#resources",
    "href": "causal_1_making_dags.html#resources",
    "title": "Making DAGs with ggdag",
    "section": "Resources",
    "text": "Resources\nBooks:\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press.\n\nLecture videos on YouTube\n\nHernán, M. A., & Robins, J. M. Causal Inference: What If. Free online\nPearl, J., Glymour, M., & Jewell, N. P. (2016). Causal Inference in Statistics: A Primer. Wiley.\n\nSoftware:\n\nggdag documentation - R package for drawing DAGs\ndagitty.net - Browser-based interactive DAG tool"
  },
  {
    "objectID": "causal_1_making_dags.html#summary",
    "href": "causal_1_making_dags.html#summary",
    "title": "Making DAGs with ggdag",
    "section": "Summary",
    "text": "Summary\nKey Concepts:\n\nConfounders: Affect both exposure and outcome → Control for them\nMediators: On the causal pathway → Don’t control (for total effect)\nColliders: Caused by exposure and outcome → Never control\nAdjustment sets: Minimal set of variables to control for unbiased estimates\n\nggdag workflow:\n# 1. Define structure\nmy_dag &lt;- dagify(\n  outcome ~ exposure + confounder,\n  exposure ~ confounder\n)\n\n# 2. Visualize\nggdag(my_dag) + theme_dag()\n\n# 3. Identify adjustment set\nggdag_adjustment_set(my_dag)\nDAGs make causal thinking explicit and visual. Use them early and often!"
  },
  {
    "objectID": "causal_1_making_dags.html#the-three-elemental-confounds",
    "href": "causal_1_making_dags.html#the-three-elemental-confounds",
    "title": "Making DAGs with ggdag",
    "section": "The Three Elemental Confounds",
    "text": "The Three Elemental Confounds\nRichard McElreath’s Statistical Rethinking (2nd ed., 2020) is a great resource on modeling in general, but also on causal inference. He breaks DAGs down into three fundamental causal structures that create confounding. Every complex DAG is built from combinations of these three patterns.\nThese examples follow McElreath’s framework, also covered in his excellent lecture series on YouTube.\n\n1. The Fork (Common Cause)\n\nfork_dag &lt;- dagify(\n  Y ~ Z,\n  X ~ Z,\n  coords = list(\n    x = c(X = 1, Y = 3, Z = 2),\n    y = c(X = 1, Y = 1, Z = 2)\n  )\n)\n\nggdag(fork_dag) +\n  theme_dag() +\n  labs(title = \"Fork: Z is a common cause of X and Y\")\n\n\n\n\n\n\n\n\nStrategy: Control for Z to block the non-causal path.\nZ is a confounder: it causes both X and Y. If you don’t control for Z, the association between X and Y includes both the direct causal effect AND the indirect path through Z.\n\n\n2. The Chain (Mediation)\n\nchain_dag &lt;- dagify(\n  Y ~ M,\n  M ~ X,\n  coords = list(\n    x = c(X = 1, M = 2, Y = 3),\n    y = c(X = 1, M = 1, Y = 1)\n  )\n)\n\nggdag(chain_dag) +\n  theme_dag() +\n  labs(title = \"Chain: M mediates the effect of X on Y\")\n\n\n\n\n\n\n\n\nStrategy: Don’t control for M if you want the total effect of X on Y.\nM is a mediator. It’s on the causal pathway from X to Y. Controlling for M blocks this path and gives you only the direct effect of X on Y, ignoring the indirect effect through M.\n\n\n3. The Inverted Fork (Collider)\n\ninverted_fork_dag &lt;- dagify(\n  C ~ X + Y,\n  coords = list(\n    x = c(X = 1, Y = 3, C = 2),\n    y = c(X = 2, Y = 2, C = 1)\n  )\n)\n\nggdag(inverted_fork_dag) +\n  theme_dag() +\n  labs(title = \"Inverted Fork: C is a collider\")\n\n\n\n\n\n\n\n\nStrategy: Never control for C. It introduces an association between X and Y that isn’t “real.”\nC is a collider: both X and Y cause it. Controlling for C creates a spurious association between X and Y even when there’s no causal relationship."
  },
  {
    "objectID": "ai_llm_illustration.html",
    "href": "ai_llm_illustration.html",
    "title": "AI & LLM Example",
    "section": "",
    "text": "We’ll use an LLM to help us prepare data for a visualization\nLet’s use the CDC’s data on influenza vaccination coverage for all ages"
  },
  {
    "objectID": "ai_llm_illustration.html#overview",
    "href": "ai_llm_illustration.html#overview",
    "title": "AI & LLM Example",
    "section": "",
    "text": "We’ll use an LLM to help us prepare data for a visualization\nLet’s use the CDC’s data on influenza vaccination coverage for all ages"
  },
  {
    "objectID": "ai_llm_illustration.html#data-preparation",
    "href": "ai_llm_illustration.html#data-preparation",
    "title": "AI & LLM Example",
    "section": "Data Preparation",
    "text": "Data Preparation"
  },
  {
    "objectID": "ai_llm_illustration.html#overview-1",
    "href": "ai_llm_illustration.html#overview-1",
    "title": "AI & LLM Example",
    "section": "Overview",
    "text": "Overview\n\nWe’ll use an LLM to help us prepare data for a visualization\nLet’s use the CDC’s data on influenza vaccination coverage for all ages"
  },
  {
    "objectID": "ai_llm_illustration.html#data-preparation-1",
    "href": "ai_llm_illustration.html#data-preparation-1",
    "title": "AI & LLM Example",
    "section": "Data Preparation",
    "text": "Data Preparation\n\ndf &lt;- read_csv(here::here(\"data\", \"raw\", \"cdc_Influenza_Vaccination_Coverage_for_All_Ages__6__Months__20250610.csv\"))\n\ndf |&gt; glimpse()\n\nRows: 220,729\nColumns: 11\n$ Vaccine              &lt;chr&gt; \"Seasonal Influenza\", \"Seasonal Influenza\", \"Seas…\n$ `Geography Type`     &lt;chr&gt; \"Counties\", \"Counties\", \"Counties\", \"Counties\", \"…\n$ Geography            &lt;chr&gt; \"New Haven\", \"New Haven\", \"New Haven\", \"New Haven…\n$ FIPS                 &lt;chr&gt; \"09009\", \"09009\", \"09009\", \"09009\", \"09009\", \"090…\n$ `Season/Survey Year` &lt;chr&gt; \"2018\", \"2021\", \"2020\", \"2021\", \"2018\", \"2019\", \"…\n$ Month                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 5, 4, 11, 10, 9, 8, 9, 1, 12, 1…\n$ `Dimension Type`     &lt;chr&gt; \"&gt;=18 Years\", \"&gt;=18 Years\", \"Age\", \"Age\", \"Age\", …\n$ Dimension            &lt;chr&gt; \"Non-Medical Setting\", \"Non-Medical Setting\", \"&gt;=…\n$ `Estimate (%)`       &lt;chr&gt; \"45.5\", \"53.0\", \"52.4\", \"50.2\", \"34.0\", \"49.5\", \"…\n$ `95% CI (%)`         &lt;chr&gt; \"43.9 to 47.2\", \"46.0 to 60.9\", \"50.6 to 54.3\", \"…\n$ `Sample Size`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 48, 48, 552, 552, 552, 55…"
  },
  {
    "objectID": "ai_llm_illustration.html#chat-with-the-llm",
    "href": "ai_llm_illustration.html#chat-with-the-llm",
    "title": "AI & LLM Example",
    "section": "Chat with the LLM",
    "text": "Chat with the LLM\n\nData Preparation\nFrom here, we can chat with the LLM to help us prepare the data for a visualization.\nWe’ll start with the most lazy prompt we can, and see how it goes.\nI am completely unqualified for my job. Can you look at the below notebook, clean up the data, and create me a plot of influenze vaccine coverage by age group?\n\nMy job depends on it and I only have five minutes, please help. I just saw the boss's car pull into the parking lot."
  },
  {
    "objectID": "applications_1_effective_and_honest_scales.html",
    "href": "applications_1_effective_and_honest_scales.html",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "",
    "text": "This notebook demonstrates how axis scaling and data presentation can be used effectively or to mislead. We’ll explore how to use positions and scales to make data more understandable, particularly with respect to magnitudes.\nWe’ll explore two main techniques to mislead:\n\nTruncated axes in bar plots that exaggerate small differences\nSelective time windows in time series that create misleading trends"
  },
  {
    "objectID": "applications_1_effective_and_honest_scales.html#introduction",
    "href": "applications_1_effective_and_honest_scales.html#introduction",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "",
    "text": "This notebook demonstrates how axis scaling and data presentation can be used effectively or to mislead. We’ll explore how to use positions and scales to make data more understandable, particularly with respect to magnitudes.\nWe’ll explore two main techniques to mislead:\n\nTruncated axes in bar plots that exaggerate small differences\nSelective time windows in time series that create misleading trends"
  },
  {
    "objectID": "applications_1_effective_and_honest_scales.html#position-scale-and-magnitude",
    "href": "applications_1_effective_and_honest_scales.html#position-scale-and-magnitude",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "Position, Scale, and Magnitude",
    "text": "Position, Scale, and Magnitude\n\nset.seed(123)\nn &lt;- 1000\n\n# Create simulated patient data with more distinct patterns\npatient_data &lt;- data.frame(\n  patient_id = 1:n,\n  treatment = sample(c(\"Standard\", \"New\"), n, replace = TRUE),\n  age_group = sample(c(\"Young\", \"Elderly\"), n, replace = TRUE),\n  hospital = sample(c(\"City\", \"County\", \"University\", \"Private\"), n, replace = TRUE)\n)\n\n# Create more distinct base rates by hospital\nhospital_effects &lt;- c(\n  \"City\" = 0.25,      # Higher rates in city hospitals\n  \"County\" = 0.15,    # Moderate rates in county hospitals\n  \"University\" = 0.10, # Lower rates in university hospitals\n  \"Private\" = 0.08    # Lowest rates in private hospitals\n)\n\n# Add random variation and treatment/age effects\npatient_data$readmission_rate &lt;- vapply(1:n, function(i) {\n  h &lt;- patient_data$hospital[i]\n  base_rate &lt;- hospital_effects[h]\n  # Add some random variation\n  rate &lt;- base_rate + rnorm(1, 0, 0.02)\n  # Treatment effect (20% reduction for new treatment)\n  rate &lt;- rate * ifelse(patient_data$treatment[i] == \"New\", 0.8, 1)\n  # Age effect (30% increase for elderly)\n  rate &lt;- rate * ifelse(patient_data$age_group[i] == \"Elderly\", 1.3, 1)\n  return(rate)\n}, numeric(1))\n\n# Ensure rates stay between 0 and 1\npatient_data$readmission_rate &lt;- pmin(pmax(patient_data$readmission_rate, 0), 1)\n\n# Create summary data for bubble chart\nbubble_data &lt;- patient_data |&gt;\n  group_by(hospital, age_group) |&gt;\n  summarize(\n    readmission_rate = mean(readmission_rate),\n    patient_count = n(),\n    .groups = \"drop\"\n  )\n\n# 0. Worst Example: Bubble Chart (Area/Volume)\nggplot(bubble_data, aes(x = hospital, y = age_group)) +\n  geom_point(\n    aes(size = readmission_rate * 100),  # Scale up for better visibility\n    shape = 21,\n    fill = \"#34D399\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", readmission_rate * 100)),\n    size = 3,\n    vjust = -4.5\n  ) +\n  scale_size_continuous(\n    range = c(5, 20),\n    name = \"Readmission Rate (%)\"\n  ) +\n  labs(\n    title = \"Readmission Rates by Hospital and Age Group (Bubble Chart)\",\n    subtitle = \"Why bubble charts are bad: Humans are poor at comparing areas\",\n    x = \"Hospital\",\n    y = \"Age Group\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n# 1. Area/Volume (Why pie charts are bad)\npie_data &lt;- patient_data |&gt;\n  group_by(hospital, treatment) |&gt;\n  summarize(\n    count = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  group_by(hospital) |&gt;\n  mutate(\n    total = sum(count),\n    proportion = count / total,\n    label = treatment\n  ) |&gt;\n  group_by(hospital) |&gt;\n  mutate(position = cumsum(proportion) - (proportion / 2)) |&gt;\n  ungroup()\n\n# Create pie charts\nstandard_pie &lt;- pie_data |&gt;\n  filter(hospital == \"City\") |&gt;\n  ggplot(aes(x = \"\", y = proportion, fill = treatment)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", fill = \"white\") +\n  coord_polar(theta = \"y\") +\n  geom_text(aes(y = position, label = label), size = 6) +\n  labs(\n    title = \"City Hospital\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)\n  )\n\nuniversity_pie &lt;- pie_data |&gt;\n  filter(hospital == \"University\") |&gt;\n  ggplot(aes(x = \"\", y = proportion, fill = treatment)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", fill = \"white\") +\n  coord_polar(theta = \"y\") +\n  geom_text(aes(y = position, label = label), size = 6) +\n  labs(\n    title = \"University Hospital\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)\n  )\n\n# Display pie charts\n(standard_pie + university_pie) +\n  plot_annotation(\n    title = \"Treatment Distribution by Hospital (Pie Charts)\",\n    subtitle = \"Why pie charts are bad: Humans are poor at comparing areas and angles\",\n    theme = theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 20),\n      plot.subtitle = element_text(hjust = 0.5, size = 14)\n    )\n  )\n\n\n\n\n\n\n\n# 2. Position on non-common scale\nggplot(patient_data, aes(x = hospital, y = readmission_rate)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", fill = \"#34D399\", color = \"black\") +\n  facet_wrap(~ age_group, nrow = 1, scales = \"free_y\") +\n  labs(\n    title = \"Readmission Rates by Hospital (Non-Common Scale)\",\n    subtitle = \"Different scales make comparisons difficult\",\n    x = \"Hospital\",\n    y = \"Readmission Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )\n\n\n\n\n\n\n\n# 3. Position on common scale\nggplot(patient_data, aes(x = hospital, y = readmission_rate)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", fill = \"#34D399\", color = \"black\") +\n  facet_wrap(~ age_group, nrow = 1) +\n  labs(\n    title = \"Readmission Rates by Hospital (Common Scale)\",\n    subtitle = \"Common scale makes comparisons easier and more accurate\",\n    x = \"Hospital\",\n    y = \"Readmission Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )"
  },
  {
    "objectID": "applications_1_effective_and_honest_scales.html#naughty-axes",
    "href": "applications_1_effective_and_honest_scales.html#naughty-axes",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "Naughty (?) Axes",
    "text": "Naughty (?) Axes\nThis section demonstrates how axis scaling can be used to mislead viewers about the magnitude of differences between groups in health outcomes. We’ll use a simulated dataset of treatment effects on adverse events.\nThe key technique here is truncating the y-axis to start above zero, which makes small differences appear much larger than they actually are. This is particularly effective when the actual differences are very small percentages.\nIn the below code, we simulate data of a rare adverse event that occurs in 0.001% of patients. This adverse event is in reality 3x more likely in the treatment group.\nWe use a logistic regression to estimate the treatment effect on the adverse event. We then plot the risk of the adverse event by treatment and age group.\n\n# Force regular decimal notation\noptions(scipen=999)\n\nset.seed(456)\nn &lt;- 1000000\n\ndata &lt;- data.frame(\n  treatment = rbinom(n, 1, 0.5)  # 1 = new treatment, 0 = standard care\n)\n\nbase_prob &lt;- 0.00001 # 0.001% baseline risk\n\ndata$adverse_event &lt;- rbinom(\n  n, 1, ifelse(data$treatment == 1, base_prob * 3, base_prob)\n)\n\nmodel &lt;- glm(adverse_event ~ treatment, data = data, family = \"binomial\")\nmodel |&gt; tidy(exponentiate = TRUE) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000010\n0.4472136\n-25.740124\n0.0000000\n\n\ntreatment\n2.990595\n0.5163997\n2.121365\n0.0338911\n\n\n\n\n\nBelow, we visualize the risk of the adverse event by treatment. We can see the risk of an adverse event is 3x higher in the treatment group, which may be alarming. But note the constrained y-axis.\n\n summary_data &lt;- data |&gt;\n  group_by(treatment) |&gt;\n  summarize(\n    risk = mean(adverse_event),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# Plot with constrained axes\nggplot(summary_data, aes(x = factor(treatment), y = risk, fill = factor(treatment))) +\n  geom_col(width = 0.7) +\n  scale_fill_brewer(palette = \"Set2\", labels = c(\"Standard Care\", \"New Treatment\")) +\n  labs(\n    title = \"Risk of Adverse Event by Treatment\",\n    x = \"Treatment\",\n    y = \"Proportion with Adverse Event\",\n    fill = \"Treatment\"\n  ) +\n  scale_y_continuous(limits = c(0, max(summary_data$risk) * 1.5)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, if we scale the axis to 0-1, we can see that the absolute risk is vanishingly rare.\n\n# Now add age group stratification\n# Plot with full scale\nggplot(summary_data, aes(x = factor(treatment), y = risk, fill = factor(treatment))) +\n  geom_col(width = 0.7) +\n  geom_text(\n    aes(label = paste0(\"p = \", sprintf(\"%.6f\", risk))),\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_fill_brewer(palette = \"Set2\", labels = c(\"Standard Care\", \"New Treatment\")) +\n  labs(\n    title = \"Risk of Adverse Event by Treatment\",\n    subtitle = \"Full scale (0 to 1) showing absolute risk\",\n    x = \"Treatment\",\n    y = \"Probability of Adverse Event\",\n    fill = \"Treatment\"\n  ) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 10),\n    axis.title = element_text(size = 12),\n    strip.text = element_text(size = 11, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nI do not believe that the plot with constrained axes is inherently misleading for certain audiences. For example, those who are familiar with medical research will likely understand that an odds ratio of “3” does mean 3x higher odds of the adverse event, but it does not mean that this is necessarily something the public needs to worry about. It is important to consider how an audience will interpret the plot."
  },
  {
    "objectID": "applications_1_effective_and_honest_scales.html#naughty-time-series-axes",
    "href": "applications_1_effective_and_honest_scales.html#naughty-time-series-axes",
    "title": "Application 1: Effective Use of Position and Scales",
    "section": "Naughty Time Series Axes",
    "text": "Naughty Time Series Axes\nThis section demonstrates how time series plots can be misleading when zooming in on specific time periods, using simulated data on Emergency Department bounceback rates as an example. The key technique here is selective windowing, or choosing a specific time period that supports a desired narrative while ignoring the broader context.\n\n# Create explicit data points\nbounceback_data &lt;- data.frame(\n  Year = 1:30,\n  BouncebackRate = c(\n    # First 20 years stable around 15% with noise\n    0.151, 0.149, 0.152, 0.148, 0.153, 0.147, 0.150, 0.152, 0.148, 0.151,\n    0.149, 0.153, 0.147, 0.150, 0.152, 0.148, 0.151, 0.149, 0.153, 0.147,\n    # Year 21-22: Still stable\n    0.151, 0.149,\n    # Year 23-25: Short dip to 12-13%\n    0.128, 0.127, 0.129,\n    # Year 26-30: Back to stable around 15%\n    0.151, 0.149, 0.152, 0.148, 0.153\n  )\n)\n\n# Plot showing full scale\nggplot(bounceback_data, aes(x = Year, y = BouncebackRate)) +\n  geom_line(color = viridis(1)) +\n  geom_point(color = viridis(1)) +\n  geom_text(\n    data = bounceback_data[c(1, 30), ],\n    aes(label = paste0(sprintf(\"%.1f\", BouncebackRate * 100), \"%\")),\n    nudge_y = 0.01,\n    size = 3\n  ) +\n  scale_y_continuous(\n    labels = scales::percent,\n    limits = c(0, 0.2),\n    breaks = seq(0, 0.2, 0.05)\n  ) +\n  labs(\n    title = \"ED Bounceback Rates Over 30 Years\",\n    x = \"Year\",\n    y = \"Bounceback Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n# Misleading plot starting at year 23\nggplot(bounceback_data[bounceback_data$Year &gt;= 23, ], aes(x = Year, y = BouncebackRate)) +\n  geom_line(color = viridis(1)) +\n  geom_point(color = viridis(1)) +\n  geom_text(\n    data = bounceback_data[bounceback_data$Year %in% c(23, 30), ],\n    aes(label = paste0(sprintf(\"%.1f\", BouncebackRate * 100), \"%\")),\n    nudge_y = 0.003,\n    size = 3\n  ) +\n  scale_y_continuous(\n    labels = scales::percent,\n    limits = c(0.12, 0.16),\n    breaks = seq(0.12, 0.16, 0.01)\n  ) +\n  labs(\n    title = \"ED Bounceback Rates Since My Enemy Took Power\",\n    x = \"Year\",\n    y = \"Bounceback Rate\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )"
  },
  {
    "objectID": "applications_2_choropleths_for_spatial_data.html",
    "href": "applications_2_choropleths_for_spatial_data.html",
    "title": "Application 2: Choropleths",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"readr\",\n  \"dplyr\",\n  \"ggplot2\",\n  \"forcats\",\n  \"tidyr\",\n  \"kableExtra\",\n  \"stringr\",\n  \"ggrepel\",\n  \"maps\",\n  \"usmap\",\n  \"sf\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}\nsource(here::here(\"examples\", \"colors.R\"))"
  },
  {
    "objectID": "applications_2_choropleths_for_spatial_data.html#load-and-prepare",
    "href": "applications_2_choropleths_for_spatial_data.html#load-and-prepare",
    "title": "Application 2: Choropleths",
    "section": "Load and Prepare",
    "text": "Load and Prepare\nAs usual, we start by loading and preparing the data.\n\ndata &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"simulated_data.csv\"))\nstate_data &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"state_data.csv\"))\n\ndata &lt;- data |&gt;\n  mutate(\n    state = as.factor(state),\n    received_comprehensive_postnatal_care = as.numeric(received_comprehensive_postnatal_care),\n    insurance = fct_relevel(insurance, \"no_insurance\"),\n    race_ethnicity = fct_relevel(race_ethnicity, \"white\"),\n    edu = fct_relevel(edu, \"hs\"),\n    job_type = fct_relevel(job_type, \"unemployed\"),\n  )\n\ndata$self_report_income &lt;- factor(data$self_report_income, levels = c(\n  \"$0–$24,999\",\n  \"$25,000–$49,999\",\n  \"$50,000–$74,999\",\n  \"$75,000–$99,999\",\n  \"$100,000–$124,999\",\n  \"$125,000–$149,999\",\n  \"$150,000–$174,999\",\n  \"$175,000+\"\n))\n\n# Set \"$50,000–$75,000\" as the reference category\ndata$self_report_income &lt;- fct_relevel(data$self_report_income, \"$50,000–$75,000\")\n\nWarning: 1 unknown level in `f`: $50,000–$75,000\n\nrace_ethnicity_labels &lt;- c(\"American Indian or Alaska Native\" = \"aian\",\n          \"White\" = \"white\",\n          \"Black\" = \"black\",\n          \"Asian\" = \"asian\",\n          \"Hispanic\" = \"hispanic\",\n          \"Native Hawaiian or Pacific Islander\" = \"nhpi\",\n          \"Other\" = \"other\")\n\ndata |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nprovider_id\nstate\nreceived_comprehensive_postnatal_care\nself_report_income\nage\nedu\nrace_ethnicity\ninsurance\njob_type\ndependents\ndistance_to_provider\nobesity\nmultiple_gestation\ndiabetes\nheart_disease\nplacenta_previa\nhypertension\ngest_hypertension\npreeclampsia\n\n\n\n\n1\n1\nAK\n0\n$25,000–$49,999\n40\nhs\nwhite\nno_insurance\nunskilled\n3\n18.856908\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\nAK\n1\n$75,000–$99,999\n21\nsome_college\nwhite\nno_insurance\nprofessional\n2\n4.096578\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\nAK\n0\n$50,000–$74,999\n29\nhs\nhispanic\nno_insurance\ntrade\n2\n2.841914\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\nAK\n0\n$25,000–$49,999\n31\nhs\nwhite\nno_insurance\nunskilled\n4\n6.459543\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n5\n1\nAK\n0\n$75,000–$99,999\n27\nhs\nwhite\nno_insurance\nunskilled\n3\n10.126509\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1\nAK\n0\n$50,000–$74,999\n17\nless_than_hs\nwhite\nno_insurance\nunemployed\n3\n5.235320\n1\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "applications_2_choropleths_for_spatial_data.html#basic-visualizations",
    "href": "applications_2_choropleths_for_spatial_data.html#basic-visualizations",
    "title": "Application 2: Choropleths",
    "section": "Basic Visualizations",
    "text": "Basic Visualizations\n\nProvider Trust\nFrom our causal model, we theorize three causes of provider trust:\n\nProvider quality\nRace/ethnicity\nCultural orientation (namely, trust in institutions)\n\nWe observe race, but we do not observe provider quality or cultural orientation."
  },
  {
    "objectID": "applications_2_choropleths_for_spatial_data.html#provider-quality",
    "href": "applications_2_choropleths_for_spatial_data.html#provider-quality",
    "title": "Application 2: Choropleths",
    "section": "Provider quality",
    "text": "Provider quality\nFor provider quality, we hypothesize the political and economic conditions with respect to healthcare will influence provider quality. We do not observe provider quality directly, but we do observe what state the provider is in.\nGiven all this, it may help us to see:\n\nA breakdown of race/ethnicity\nA breakdown of receipt of comprehensive care by race/ethnicity\nA breakdown of receipt of comprehensive care by state\nA breakdown of receipt of comprehensive care by race by state\n\n\nRace/ethnicity\nFor categorical variables, we typically first want to see a bar chart of the distribution of the variable.\n\n# Bar chart\nggplot(data, aes(race_ethnicity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n  ggplot(\n    data |&gt;\n      count(race_ethnicity) |&gt;\n      mutate(\n        proportion = n / sum(n),\n        race_ethnicity = fct_recode(\n          factor(race_ethnicity),\n          !!! race_ethnicity_labels\n        )\n      ) |&gt;\n      arrange(desc(proportion)) |&gt;\n      mutate(race_ethnicity = fct_inorder(race_ethnicity) |&gt; fct_rev())\n    ,\n    aes(x = proportion, y = race_ethnicity)\n  ) +\n  geom_bar(stat = \"identity\",\n           fill = colors$blue$`100`,\n           color = colors$blue$`400`) +\n  scale_x_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    expand = expansion(mult = c(0, 0.2))  # Add 20% padding on the right\n  ) +\n  labs(x = \"\", y = \"\", title = \"Patients by Race/Ethnicity\", caption = \"Source: Simulated Data\") +\n  theme_minimal() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n# RCP vs. Race/Ethnicity\nplot_data &lt;- data |&gt;\n  group_by(race_ethnicity) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care),\n    n = n()\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    race_ethnicity = fct_recode(factor(race_ethnicity), !!!race_ethnicity_labels),\n    race_ethnicity = fct_reorder(race_ethnicity, proportion)\n  )\n\n# Plot proportions by race\nggplot(plot_data, aes(x = proportion, y = race_ethnicity)) +\n  geom_bar(stat = \"identity\", fill = colors$blue$`100`, color = colors$blue$`400`) +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Receipt of Comprehensive Postnatal Care by Race/Ethnicity\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )"
  },
  {
    "objectID": "applications_2_choropleths_for_spatial_data.html#choropleth",
    "href": "applications_2_choropleths_for_spatial_data.html#choropleth",
    "title": "Application 2: Choropleths",
    "section": "Choropleth",
    "text": "Choropleth\nWe will now map the receipt of comprehensive postnatal care by state. This is a choropleth map.\nPlease note that you must think hard about whether it makes sense to map the data you are mapping. For example, if you are mapping the number of people who have received comprehensive postnatal care, you must think about whether it makes sense to compare states with different populations. In our case, we know that every state is evenly represented and every person has the same probability of being in the sample. However, these assumptions rarely hold with true to life research designs.\n\nus_map &lt;- us_map(regions = \"states\")\n\nrcp_by_state_data &lt;- data |&gt;\n  group_by(state) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care),\n    n = n()\n  ) |&gt;\n  ungroup()\n\n# Merge summarized data with map data\nrcp_by_state_data_mappable &lt;- us_map |&gt;\n  left_join(\n    rcp_by_state_data,\n    by = c(\"abbr\"=\"state\")\n  )\n\nggplot(data = rcp_by_state_data_mappable) +\n  geom_sf(aes(fill = proportion), color = \"white\") +\n  scale_fill_continuous(\n    low = colors$blue$`50`,\n    high = colors$blue$`900`,\n    name = \"Proportion\",\n    labels = scales::percent,\n  ) +\n  labs(\n    title = \"Proportion Receiving Comprehensive Postnatal Care By State\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )"
  },
  {
    "objectID": "applications_2_choropleths_for_spatial_data.html#small-multiples",
    "href": "applications_2_choropleths_for_spatial_data.html#small-multiples",
    "title": "Application 2: Choropleths",
    "section": "Small Multiples",
    "text": "Small Multiples\nEven choropleths benefit from faceting by small multiples.\n\n# Will it work? Maps  by state\nrace_ethnicity_care_data &lt;- data |&gt;\n  group_by(state, race_ethnicity) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care, na.rm = TRUE),\n    n = n(),\n        .groups = \"drop\"\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    race_ethnicity = fct_recode(race_ethnicity, !!!race_ethnicity_labels)\n  )\n\nrace_ethnicity_care_sf &lt;- us_map |&gt;\n  left_join(\n    race_ethnicity_care_data,\n    by = c(\"abbr\" = \"state\")\n  )\n\nggplot(race_ethnicity_care_sf) +\n  geom_sf(aes(geometry = geom, fill = proportion), color = \"white\", size = 0.1) +\n  scale_fill_continuous(\n    low = colors$blue$`100`,\n    high = colors$blue$`900`,\n    name = \"Proportion\",\n    labels = scales::percent,\n    na.value = \"grey90\"\n  ) +\n  facet_wrap(~ race_ethnicity, ncol = 3, nrow = 3) +\n  labs(\n    title = \"Proportion Receiving Comprehensive Postnatal Care by Race\",\n    fill = \"Proportion\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\"),\n    strip.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nChoropleths can be useful, but they can be misleading. In fact, they often end up encoding things like population density as much as the spatial variation we are trying to visualize."
  },
  {
    "objectID": "applications_3_dot_plot_for_spatial_data.html",
    "href": "applications_3_dot_plot_for_spatial_data.html",
    "title": "Application 3: Dot Plots",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"readr\",\n  \"dplyr\",\n  \"ggplot2\",\n  \"forcats\",\n  \"tidyr\",\n  \"kableExtra\",\n  \"stringr\",\n  \"ggrepel\",\n  \"maps\",\n  \"usmap\",\n  \"sf\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}\n\nsource(here::here(\"examples\", \"colors.R\"))"
  },
  {
    "objectID": "applications_3_dot_plot_for_spatial_data.html#load-and-prepare",
    "href": "applications_3_dot_plot_for_spatial_data.html#load-and-prepare",
    "title": "Application 3: Dot Plots",
    "section": "Load and Prepare",
    "text": "Load and Prepare\nAs usual, we start by loading and preparing the data.\n\ndata &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"simulated_data.csv\"))\nstate_data &lt;- readr::read_csv(here::here(\"data\", \"processed\", \"state_data.csv\"))\n\ndata &lt;- data |&gt;\n  mutate(\n    state = as.factor(state),\n    received_comprehensive_postnatal_care = as.numeric(received_comprehensive_postnatal_care),\n    insurance = fct_relevel(insurance, \"no_insurance\"),\n    race_ethnicity = fct_relevel(race_ethnicity, \"white\"),\n    edu = fct_relevel(edu, \"hs\"),\n    job_type = fct_relevel(job_type, \"unemployed\"),\n  )\n\ndata$self_report_income &lt;- factor(data$self_report_income, levels = c(\n  \"$0–$24,999\",\n  \"$25,000–$49,999\",\n  \"$50,000–$74,999\",\n  \"$75,000–$99,999\",\n  \"$100,000–$124,999\",\n  \"$125,000–$149,999\",\n  \"$150,000–$174,999\",\n  \"$175,000+\"\n))\n\n# Set \"$50,000–$75,000\" as the reference category\ndata$self_report_income &lt;- fct_relevel(data$self_report_income, \"$50,000–$75,000\")\n\nWarning: 1 unknown level in `f`: $50,000–$75,000\n\nrace_ethnicity_labels &lt;- c(\"American Indian or Alaska Native\" = \"aian\",\n          \"White\" = \"white\",\n          \"Black\" = \"black\",\n          \"Asian\" = \"asian\",\n          \"Hispanic\" = \"hispanic\",\n          \"Native Hawaiian or Pacific Islander\" = \"nhpi\",\n          \"Other\" = \"other\")\n\ndata |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nprovider_id\nstate\nreceived_comprehensive_postnatal_care\nself_report_income\nage\nedu\nrace_ethnicity\ninsurance\njob_type\ndependents\ndistance_to_provider\nobesity\nmultiple_gestation\ndiabetes\nheart_disease\nplacenta_previa\nhypertension\ngest_hypertension\npreeclampsia\n\n\n\n\n1\n1\nAK\n0\n$25,000–$49,999\n40\nhs\nwhite\nno_insurance\nunskilled\n3\n18.856908\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\nAK\n1\n$75,000–$99,999\n21\nsome_college\nwhite\nno_insurance\nprofessional\n2\n4.096578\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\nAK\n0\n$50,000–$74,999\n29\nhs\nhispanic\nno_insurance\ntrade\n2\n2.841914\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\nAK\n0\n$25,000–$49,999\n31\nhs\nwhite\nno_insurance\nunskilled\n4\n6.459543\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n5\n1\nAK\n0\n$75,000–$99,999\n27\nhs\nwhite\nno_insurance\nunskilled\n3\n10.126509\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1\nAK\n0\n$50,000–$74,999\n17\nless_than_hs\nwhite\nno_insurance\nunemployed\n3\n5.235320\n1\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "applications_3_dot_plot_for_spatial_data.html#dot-plots",
    "href": "applications_3_dot_plot_for_spatial_data.html#dot-plots",
    "title": "Application 3: Dot Plots",
    "section": "Dot Plots",
    "text": "Dot Plots\nIn applications_2_choropleth.qmd we drew up a choropleth map of the receipt of comprehensive postnatal care by state. Let’s see if a dot plot can help us understand the data better.\n\n# Calculate overall proportions for all states\noverall_care_data &lt;- data |&gt;\n  group_by(state) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(race_ethnicity = \"Overall\")\n\n# Calculate proportions for each race\nrace_care_data &lt;- data |&gt;\n  filter(race_ethnicity %in% c(\"white\", \"black\", \"hispanic\", \"asian\")) |&gt;\n  group_by(state, race_ethnicity) |&gt;\n  summarize(\n    proportion = mean(received_comprehensive_postnatal_care, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  )  |&gt;\n  mutate(\n    race_ethnicity = fct_recode(race_ethnicity, !!!race_ethnicity_labels)\n  ) ## will have warning because we dropped some race/eth cats\n\n\n# Combine overall and race-specific data\ndot_plot_data &lt;- bind_rows(overall_care_data, race_care_data)\n\n# Sort states by overall proportion (to ensure consistent order across facets)\nstate_order &lt;- dot_plot_data |&gt;\n  filter(race_ethnicity == \"Overall\") |&gt;\n  arrange(proportion) |&gt;\n  pull(state)\n\ndot_plot_data &lt;- dot_plot_data |&gt;\n  mutate(state = factor(state, levels = state_order)) |&gt;\n  mutate(\n    race_ethnicity = factor(\n      race_ethnicity,\n      levels = c(\"Overall\", \"White\", \"Black\", \"Hispanic\", \"Asian\")\n    )\n  )\n\nggplot(dot_plot_data, aes(x = proportion, y = state)) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  geom_segment(\n    aes(x = 0, xend = proportion, y = state, yend = state),\n    color = colors$blue$`500`,\n    linetype = \"dotted\"\n  ) +\n  facet_wrap(~race_ethnicity, ncol = 5, scales = \"free_y\") +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Proportion Receiving Comprehensive Postnatal Care\",\n    x = \"Proportion\",\n    y = \"State\",\n    caption = \"Source: Simulated Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 8),\n    strip.text = element_text(size = 11, face = \"bold\"),\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    plot.caption = element_text(size = 10, hjust = 0)\n  )\n\n\n\n\n\n\n\n\nWe find that, in fact, the dot plot is superior to the choropleth map to visualize the spatial trends that exist in the data.\nI would encourage you to always try to visualize spatial data using non-map-based visualizations. In many cases, maps foreground non-causal phenomena like population density.\nFor comparison, compare the above to the choropleth map we made in applications_2_choropleth.qmd, which we reproduce below:"
  },
  {
    "objectID": "applications_4_distribution_plots.html",
    "href": "applications_4_distribution_plots.html",
    "title": "Application 4: Distribution Plots",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"ggridges\",\n  \"kableExtra\",\n  \"purrr\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}"
  },
  {
    "objectID": "applications_4_distribution_plots.html#distribution",
    "href": "applications_4_distribution_plots.html#distribution",
    "title": "Application 4: Distribution Plots",
    "section": "Distribution",
    "text": "Distribution\nThe distribution or spread of data is often of comparable importance to measures of central tendency. There are numerous visualizations that can be used to understand distributions. Below we will explore box plots, violin plots, and ridgeline plots."
  },
  {
    "objectID": "applications_4_distribution_plots.html#simulate-physical-activity-data",
    "href": "applications_4_distribution_plots.html#simulate-physical-activity-data",
    "title": "Application 4: Distribution Plots",
    "section": "Simulate Physical Activity Data",
    "text": "Simulate Physical Activity Data\nLet’s simulate data on weekly minutes of moderate-to-vigorous physical activity (MVPA) across different job types in New England states.\n\nset.seed(123)\n\n# Number of observations\nn &lt;- 5000\n\n# Function to generate data with outliers\ngenerate_with_outliers &lt;- function(n, mean, sd, outlier_prob = 0.1, outlier_sd = 3) {\n  # Generate main distribution\n  main_data &lt;- rnorm(n, mean, sd)\n\n  # Add outliers\n  is_outlier &lt;- runif(n) &lt; outlier_prob\n  outliers &lt;- rnorm(n, mean, sd * outlier_sd)\n\n  # Combine\n  ifelse(is_outlier, outliers, main_data)\n}\n\n# Function to generate bimodal distribution\ngenerate_bimodal &lt;- function(n, mean1, mean2, sd1, sd2, prob1 = 0.5) {\n  # Generate two normal distributions\n  dist1 &lt;- rnorm(n, mean1, sd1)\n  dist2 &lt;- rnorm(n, mean2, sd2)\n\n  # Randomly choose between the two distributions\n  is_dist1 &lt;- runif(n) &lt; prob1\n\n  # Combine\n  ifelse(is_dist1, dist1, dist2)\n}\n\n# Simulate data\nactivity_data &lt;- tibble(\n  # Job types\n  job_type = sample(c(\"Office Work\", \"Service Industry\", \"Healthcare\", \"Construction\", \"Education\", \"Professional Athlete\"),\n                   size = n,\n                   replace = TRUE,\n                   prob = c(0.25, 0.25, 0.2, 0.15, 0.1, 0.05))\n) |&gt;\n  # Generate MVPA minutes based on job type\n  mutate(\n    # Base MVPA with job-specific variance and outliers\n    mvpa = case_when(\n      job_type == \"Office Work\" ~ rnorm(n, mean = 80, sd = 30),\n      job_type == \"Service Industry\" ~ generate_with_outliers(n, mean = 130, sd = 35, outlier_prob = 0.15),\n      job_type == \"Healthcare\" ~ generate_bimodal(n,\n                                                 mean1 = 120, sd1 = 30,  # First mode (more sedentary)\n                                                 mean2 = 280, sd2 = 40,  # Second mode (more active)\n                                                 prob1 = 0.6),           # 60% in first mode\n      job_type == \"Education\" ~ rnorm(n, mean = 160, sd = 40),\n      job_type == \"Construction\" ~ rnorm(n, mean = 250, sd = 20),\n      job_type == \"Professional Athlete\" ~ rnorm(n, mean = 450, sd = 30)\n    ),\n    # Ensure non-negative values\n    mvpa = pmax(0, mvpa)\n  )\n\n# Show first few rows\nactivity_data |&gt; head() |&gt; kable()\n\n\n\n\njob_type\nmvpa\n\n\n\n\nOffice Work\n59.63577\n\n\nConstruction\n274.26397\n\n\nOffice Work\n58.86456\n\n\nEducation\n177.27274\n\n\nEducation\n194.10753\n\n\nService Industry\n86.05837"
  },
  {
    "objectID": "applications_4_distribution_plots.html#box-plots",
    "href": "applications_4_distribution_plots.html#box-plots",
    "title": "Application 4: Distribution Plots",
    "section": "Box Plots",
    "text": "Box Plots\nBox plots are great for showing the distribution of a continuous variable across different groups. They show: - Median (middle line) - Interquartile range (box) - Range (whiskers) - Outliers (points)\n\n# Create box plots by job type\np_box &lt;- ggplot(activity_data, aes(x = job_type, y = mvpa, fill = job_type)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_viridis_d(option = \"plasma\") +\n  labs(\n    title = \"Weekly Minutes of Moderate-to-Vigorous Physical Activity by Job Type\",\n    x = \"Job Type\",\n    y = \"Minutes per Week\",\n    fill = \"Job Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 8, face = \"bold\"),\n    legend.text = element_text(size = 8)\n  )\n\np_box"
  },
  {
    "objectID": "applications_4_distribution_plots.html#violin-plots",
    "href": "applications_4_distribution_plots.html#violin-plots",
    "title": "Application 4: Distribution Plots",
    "section": "Violin Plots",
    "text": "Violin Plots\nViolin plots show the full distribution of the data, not just the summary statistics. They’re particularly useful for: - Seeing the shape of the distribution - Identifying multiple modes - Comparing distributions across groups\n\n# Create violin plots by job type\np_violin &lt;- ggplot(activity_data, aes(x = job_type, y = mvpa, fill = job_type)) +\n  geom_violin(alpha = 0.7) +\n  scale_fill_viridis_d(option = \"plasma\") +\n  labs(\n    title = \"Weekly Minutes of Moderate-to-Vigorous Physical Activity by Job Type\",\n    x = \"Job Type\",\n    y = \"Minutes per Week\",\n    fill = \"Job Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 8, face = \"bold\"),\n    legend.text = element_text(size = 8)\n  )\n\np_violin"
  },
  {
    "objectID": "applications_4_distribution_plots.html#violin-box-plot",
    "href": "applications_4_distribution_plots.html#violin-box-plot",
    "title": "Application 4: Distribution Plots",
    "section": "Violin-Box Plot",
    "text": "Violin-Box Plot\n\n# Create violin-box plot\np_violin_box &lt;- ggplot(activity_data, aes(x = mvpa, y = reorder(job_type, mvpa, FUN = median), fill = job_type)) +\n  # Add violin plot\n  geom_violin(\n    alpha = 0.7,\n    scale = \"width\"\n  ) +\n  # Add boxplot\n  geom_boxplot(\n    width = 0.2,\n    alpha = 0.7,\n    fill = \"white\",\n    outlier.shape = NA\n  ) +\n  # Customize scales\n  scale_x_continuous(\n    name = \"Weekly MVPA Minutes\",\n    limits = c(0, 600),\n    breaks = seq(0, 600, 100)\n  ) +\n  scale_fill_viridis_d(option = \"magma\") +\n  # Add labels\n  labs(\n    title = \"Distribution of Weekly Physical Activity by Job Type\",\n    caption = \"Box shows median and quartiles, violin shows full distribution\",\n    y = \"Job Type\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 10, hjust = 0.5, margin = margin(t = 20)),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"none\"\n  )\n\np_violin_box"
  },
  {
    "objectID": "applications_4_distribution_plots.html#ridgeline-plots",
    "href": "applications_4_distribution_plots.html#ridgeline-plots",
    "title": "Application 4: Distribution Plots",
    "section": "Ridgeline Plots",
    "text": "Ridgeline Plots\nRidgeline plots are great for comparing distributions across many groups. They’re particularly useful when: - You have many groups to compare - You want to see the full distribution for each group - You want to identify patterns across groups\n\n# Create ridgeline plot\np_ridge &lt;- ggplot(activity_data, aes(x = mvpa, y = reorder(job_type, mvpa, FUN = median), fill = after_stat(x))) +\n  # Add density ridges\n  geom_density_ridges_gradient(\n    scale = 3,\n    alpha = 0.7,\n    quantile_lines = TRUE,\n    quantiles = 2,\n    show.legend = TRUE\n  ) +\n  # Customize scales\n  scale_x_continuous(\n    name = \"Weekly MVPA Minutes\",\n    limits = c(0, 600),\n    breaks = seq(0, 600, 100)\n  ) +\n  scale_fill_viridis_c(option = \"magma\", name = \"Minutes\") +\n  # Add labels\n  labs(\n    title = \"Distribution of Weekly Physical Activity by Job Type\",\n    caption = \"Vertical lines show median minutes of activity for each job type\",\n    y = \"Job Type\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 10, hjust = 0.5, margin = margin(t = 20)),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.title = element_text(size = 8, face = \"bold\"),\n    legend.text = element_text(size = 8)\n  )\n\np_ridge"
  },
  {
    "objectID": "applications_5_visualizing_time_trends.html",
    "href": "applications_5_visualizing_time_trends.html",
    "title": "Application 5: Time Trends",
    "section": "",
    "text": "Time series plots are a common way to visualize data over time. They are useful for showing trends and patterns in data over time.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate time series data\nn_weeks &lt;- 104  # 2 years of weekly data\nintervention_week &lt;- 52  # Intervention at 1 year\n\n# Create base data\ntime_data &lt;- tibble(\n  week = 1:n_weeks,\n  date = as.Date(\"2023-01-01\") + weeks(week - 1),\n  # Base level (flat)\n  base_level = 100,\n  # Random noise\n  noise = rnorm(n_weeks, mean = 0, sd = 10),\n  # Intervention effect with decay\n  intervention = ifelse(\n    week &gt;= intervention_week,\n    30 * exp(-0.02 * (week - intervention_week)),  # Exponential decay\n    0\n  ),\n  # Combine components\n  value = base_level + noise + intervention\n)\n\n# Add gender effect for later use\ntime_data &lt;- time_data |&gt;\n  crossing(gender = c(\"Male\", \"Female\")) |&gt;\n  mutate(\n    # Add gender-specific effects\n    gender_effect = ifelse(gender == \"Female\", -15, 15),\n    # Add gender-specific intervention effects with decay\n    gender_intervention = ifelse(\n      week &gt;= intervention_week,\n      ifelse(\n        gender == \"Female\",\n        40 * exp(-0.02 * (week - intervention_week)),  # Larger effect for females\n        20 * exp(-0.02 * (week - intervention_week))\n      ),\n      0\n    ),\n    # Combine all effects\n    value_gender = value + gender_effect + gender_intervention\n  )\n\n\n# Create basic time series plot\np_time &lt;-\n\n\nggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add line\n  geom_line(\n    color = \"gray30\",\n    linewidth = 0.5\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"red\",\n    alpha = 0.5\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time\",\n    subtitle = \"Intervention implemented at 1 year (dashed line)\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  )\n\np_time"
  },
  {
    "objectID": "applications_5_visualizing_time_trends.html#time-series-plots",
    "href": "applications_5_visualizing_time_trends.html#time-series-plots",
    "title": "Application 5: Time Trends",
    "section": "",
    "text": "Time series plots are a common way to visualize data over time. They are useful for showing trends and patterns in data over time.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate time series data\nn_weeks &lt;- 104  # 2 years of weekly data\nintervention_week &lt;- 52  # Intervention at 1 year\n\n# Create base data\ntime_data &lt;- tibble(\n  week = 1:n_weeks,\n  date = as.Date(\"2023-01-01\") + weeks(week - 1),\n  # Base level (flat)\n  base_level = 100,\n  # Random noise\n  noise = rnorm(n_weeks, mean = 0, sd = 10),\n  # Intervention effect with decay\n  intervention = ifelse(\n    week &gt;= intervention_week,\n    30 * exp(-0.02 * (week - intervention_week)),  # Exponential decay\n    0\n  ),\n  # Combine components\n  value = base_level + noise + intervention\n)\n\n# Add gender effect for later use\ntime_data &lt;- time_data |&gt;\n  crossing(gender = c(\"Male\", \"Female\")) |&gt;\n  mutate(\n    # Add gender-specific effects\n    gender_effect = ifelse(gender == \"Female\", -15, 15),\n    # Add gender-specific intervention effects with decay\n    gender_intervention = ifelse(\n      week &gt;= intervention_week,\n      ifelse(\n        gender == \"Female\",\n        40 * exp(-0.02 * (week - intervention_week)),  # Larger effect for females\n        20 * exp(-0.02 * (week - intervention_week))\n      ),\n      0\n    ),\n    # Combine all effects\n    value_gender = value + gender_effect + gender_intervention\n  )\n\n\n# Create basic time series plot\np_time &lt;-\n\n\nggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add line\n  geom_line(\n    color = \"gray30\",\n    linewidth = 0.5\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"red\",\n    alpha = 0.5\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time\",\n    subtitle = \"Intervention implemented at 1 year (dashed line)\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  )\n\np_time"
  },
  {
    "objectID": "applications_5_visualizing_time_trends.html#time-series-plots-with-smoothers",
    "href": "applications_5_visualizing_time_trends.html#time-series-plots-with-smoothers",
    "title": "Application 5: Time Trends",
    "section": "Time Series Plots with Smoothers",
    "text": "Time Series Plots with Smoothers\nSmoothers are a common way to visualize trends in data over time. Below, we use a loess smoother to visualize the trend in the data.\n\n# Create time series plot with smoother\np_time_smooth &lt;- ggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add smoother\n  geom_smooth(\n    method = \"loess\",\n    span = 0.3,\n    color = \"blue\",\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"red\",\n    alpha = 0.5\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time with Smoother\",\n    subtitle = \"Intervention implemented at 1 year (dashed line)\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  )\n\np_time_smooth"
  },
  {
    "objectID": "applications_5_visualizing_time_trends.html#interrupted-time-series-plots",
    "href": "applications_5_visualizing_time_trends.html#interrupted-time-series-plots",
    "title": "Application 5: Time Trends",
    "section": "Interrupted Time Series Plots",
    "text": "Interrupted Time Series Plots\nInterrupted time series plots are a common way to visualize the effect of an intervention on a time series. Below, we fit a linear model to the data before and after the intervention and visualize the trend in the data before and after the intervention. We annotate the plot with the average weight before and after the intervention. This specific example a “regression discontinuity” design.\n\n# Calculate averages for annotation\navg_before &lt;- time_data |&gt;\n  filter(date &lt; as.Date(\"2024-01-01\")) |&gt;\n  summarise(avg = mean(value)) |&gt;\n  mutate(\n    x = as.Date(\"2023-07-01\"),\n    label = sprintf(\"Avg: %.0f min\", avg)\n  )\n\navg_after &lt;- time_data |&gt;\n  filter(date &gt;= as.Date(\"2024-01-01\")) |&gt;\n  summarise(avg = mean(value)) |&gt;\n  mutate(\n    x = as.Date(\"2024-07-01\"),\n    label = sprintf(\"Avg: %.0f min\", avg)\n  )\n\n# Create time series plot with separate linear models\np_time_lm &lt;- ggplot(time_data, aes(x = date, y = value)) +\n  # Add points\n  geom_point(\n    size = 2,\n    alpha = 0.5,\n    color = \"gray50\"\n  ) +\n  # Add linear model for pre-intervention\n  geom_smooth(\n    data = filter(time_data, date &lt; as.Date(\"2024-01-01\")),\n    method = \"lm\",\n    color = \"blue\",\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add linear model for post-intervention\n  geom_smooth(\n    data = filter(time_data, date &gt;= as.Date(\"2024-01-01\")),\n    method = \"lm\",\n    color = \"red\",\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add vertical line for intervention\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dashed\",\n    color = \"black\",\n    alpha = 0.5\n  ) +\n  # Add arrow and annotation\n  annotate(\n    \"segment\",\n    x = as.Date(\"2023-11-15\"),\n    xend = as.Date(\"2024-01-01\"),\n    y = 150,\n    yend = 150,\n    arrow = arrow(length = unit(0.2, \"cm\"), ends = \"last\", type = \"closed\"),\n    color = \"red\"\n  ) +\n  annotate(\n    \"text\",\n    x = as.Date(\"2023-11-15\"),\n    y = 150,\n    label = \"Intervention\\nStarted\",\n    hjust = 1.1,\n    color = \"red\",\n    fontface = \"bold\"\n  ) +\n  # Add average annotations\n  geom_text(\n    data = avg_before,\n    aes(x = x, y = avg, label = label),\n    hjust = 0.5,\n    vjust = -6,\n    fontface = \"bold\",\n    color = \"black\"\n  ) +\n  geom_text(\n    data = avg_after,\n    aes(x = x, y = avg, label = label),\n    hjust = 0.5,\n    vjust = -6,\n    fontface = \"bold\",\n    color = \"black\"\n  ) +\n  # Add labels\n  labs(\n    title = \"Weekly Physical Activity Over Time\",\n    subtitle = \"Linear trends before and after intervention\",\n    x = NULL,\n    y = \"Minutes of Activity\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_y_continuous(\n    limits = c(0, 180)\n  )\n\np_time_lm"
  },
  {
    "objectID": "applications_5_visualizing_time_trends.html#using-labels-shading-and-annotations",
    "href": "applications_5_visualizing_time_trends.html#using-labels-shading-and-annotations",
    "title": "Application 5: Time Trends",
    "section": "Using Labels, Shading, and Annotations",
    "text": "Using Labels, Shading, and Annotations\nWe can use labels, shading, and annotations to add additional information to our plots.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate dates for 4 years\ndates &lt;- seq.Date(\n  from = as.Date(\"2020-01-01\"),\n  to = as.Date(\"2023-12-31\"),\n  by = \"day\"\n)\n\n# Generate person IDs and their base characteristics\nn_people &lt;- 300\nperson_data &lt;- tibble(\n  person_id = 1:n_people,\n  gender = sample(c(\"Male\", \"Female\"), n_people, replace = TRUE),\n  # Base weight varies by person (in pounds)\n  base_weight = case_when(\n    gender == \"Male\" ~ rnorm(n_people, mean = 176, sd = 15),  # Mean 176 lbs, SD 15\n    TRUE ~ rnorm(n_people, mean = 143, sd = 12)               # Mean 143 lbs, SD 12\n  ),\n  # Individual seasonal sensitivity\n  seasonal_sensitivity = rnorm(n_people, mean = 1, sd = 0.2)  # Some people more sensitive to seasons\n)\n\n# Create base data frame with all person-dates\nweight_data &lt;- crossing(\n  date = dates,\n  person_id = 1:n_people\n) |&gt;\n  left_join(person_data, by = \"person_id\") |&gt;\n  mutate(\n    month = month(date),\n    day_of_year = yday(date),\n    # Small seasonal effect (2-3 pounds total)\n    seasonal_effect = case_when(\n      month %in% c(12, 1, 2) ~ 2.5 * sin((day_of_year - 15) * 2 * pi / 365),  # Winter peak\n      month %in% c(3, 4, 5) ~ 1.5 * sin((day_of_year - 15) * 2 * pi / 365),   # Spring\n      month %in% c(6, 7, 8) ~ 0,                                              # Summer\n      TRUE ~ 2.0 * sin((day_of_year - 15) * 2 * pi / 365)                     # Fall\n    ),\n    # Apply individual sensitivity to seasonal effect\n    seasonal_effect = seasonal_effect * seasonal_sensitivity,\n    # Add very small random noise (0.2 lbs)\n    noise = rnorm(n(), 0, 0.2),\n    # Calculate final weight\n    weight = base_weight + seasonal_effect + noise\n  )\n\n# Calculate daily averages by gender\ndaily_averages &lt;- weight_data |&gt;\n  group_by(date, gender) |&gt;\n  summarise(\n    avg_weight = mean(weight),\n    .groups = \"drop\"\n  )\n\n# Create annotation data frame\nannotation_dates &lt;- c(\n  as.Date(\"2020-11-01\"), as.Date(\"2020-12-31\"),\n  as.Date(\"2021-11-01\"), as.Date(\"2021-12-31\"),\n  as.Date(\"2022-11-01\"), as.Date(\"2022-12-31\"),\n  as.Date(\"2023-11-01\"), as.Date(\"2023-12-31\")\n)\n\nannotations &lt;- daily_averages |&gt;\n  filter(date %in% annotation_dates) |&gt;\n  mutate(\n    label = sprintf(\"%.1f\", avg_weight),\n    text_x = case_when(\n      month(date) == 11 ~ date - 56,  # Dodge left for November\n      TRUE ~ date + 56                # Dodge right for December\n    ),\n    text_y = case_when(\n      gender == \"Female\" ~ 150,  # Fixed position for men (lowered from 155)\n      TRUE ~ 185               # Fixed position for women\n    )\n  )\n\n# Create time series plot\np_weight &lt;- ggplot(daily_averages, aes(x = date, y = avg_weight, color = gender)) +\n  # Add holiday season shading\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2020-11-01\"),\n    xmax = as.Date(\"2020-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2021-11-01\"),\n    xmax = as.Date(\"2021-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2022-11-01\"),\n    xmax = as.Date(\"2022-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  annotate(\n    \"rect\",\n    xmin = as.Date(\"2023-11-01\"),\n    xmax = as.Date(\"2023-12-31\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"gray90\",\n    alpha = 0.5\n  ) +\n  # Add vertical lines for Nov 1 and Dec 31\n  geom_vline(\n    data = annotations,\n    aes(xintercept = date),\n    linewidth = 0.3,\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  # Add smoothed lines\n  geom_smooth(\n    method = \"loess\",\n    span = 0.1,\n    linewidth = 1,\n    se = TRUE,\n    alpha = 0.2\n  ) +\n  # Add text labels\n  geom_text(\n    data = annotations,\n    aes(\n      x = text_x,\n      y = text_y,\n      label = label,\n      color = gender\n    ),\n    size = 3,\n    show.legend = FALSE\n  ) +\n  # Add labels\n  labs(\n    title = \"Average Weight Over Time by Gender\",\n    x = NULL,\n    y = \"Weight (lbs)\",\n    caption = \"Shaded area indicates holiday season (November 1 - December 31).\\nText annotations show average weights on November 1 and December 31.\"\n  ) +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.title.position = \"plot\",\n    plot.caption = element_text(size = 8, hjust = 0, margin = margin(t = 10)),\n    axis.title = element_text(size = 10, face = \"bold\"),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.title = element_blank()\n  ) +\n  # Extend y-axis range\n  scale_y_continuous(\n    limits = c(120, 220),\n    breaks = seq(120, 220, by = 20)\n  ) +\n  # Use different colors for genders\n  scale_color_manual(\n    values = c(\n      \"Male\" = \"red\",\n      \"Female\" = \"darkblue\"\n    ),\n    labels = c(\"Women\", \"Men\")\n  ) +\n  # Remove rect from legend and fix the \"a\" issue\n  guides(color = guide_legend(override.aes = list(shape = NA, fill = NA, linetype = 1, label = NULL, text = NULL)))\n\np_weight"
  },
  {
    "objectID": "applications_5_visualizing_time_trends.html#sankey-diagrams",
    "href": "applications_5_visualizing_time_trends.html#sankey-diagrams",
    "title": "Application 5: Time Trends",
    "section": "Sankey Diagrams",
    "text": "Sankey Diagrams\nSankey diagrams are a common way to visualize the flow of data. Below, we create a Sankey diagram to visualize simulated data of patients with hypertension who change treatments over time.\n\n# Create treatment pattern data\nset.seed(123)\n\n# Define possible treatments (common hypertension medications)\ntreatments &lt;- c(\"ACE Inhibitor\", \"ARB\", \"CCB\", \"No Treatment\")\n\n# Generate patient data with treatment changes\nn_patients &lt;- 1000\npatient_data &lt;- tibble(\n  patient_id = 1:n_patients,\n  initial_treatment = sample(treatments, n_patients, replace = TRUE,\n                           prob = c(0.4, 0.3, 0.2, 0.1)),  # ACE inhibitors most common first-line\n  # 60% of patients change treatment at least once\n  has_change = rbinom(n_patients, 1, 0.6),\n  # For those who change, determine second treatment\n  second_treatment = case_when(\n    has_change == 1 ~ sample(treatments, n_patients, replace = TRUE,\n                           prob = c(0.3, 0.3, 0.3, 0.1)),\n    TRUE ~ initial_treatment\n  ),\n  # 30% of patients who changed once change again\n  has_second_change = case_when(\n    has_change == 1 ~ rbinom(n_patients, 1, 0.3),\n    TRUE ~ 0\n  ),\n  # For those who change twice, determine third treatment\n  third_treatment = case_when(\n    has_second_change == 1 ~ sample(treatments, n_patients, replace = TRUE,\n                                  prob = c(0.2, 0.2, 0.2, 0.4)),\n    TRUE ~ second_treatment\n  )\n)\n\n# Create Sankey diagram data\n# Create nodes data frame\nnodes &lt;- data.frame(\n  name = c(\n    treatments,  # First column\n    treatments,  # Second column\n    treatments   # Third column\n  )\n)\n\n# Create links data frame\nlinks &lt;- patient_data |&gt;\n  count(initial_treatment, second_treatment, third_treatment) |&gt;\n  filter(n &gt; 0) |&gt;\n  mutate(\n    source = match(initial_treatment, nodes$name) - 1,\n    target = match(second_treatment, nodes$name) - 1 + length(treatments),\n    value = n\n  ) |&gt;\n  select(source, target, value)\n\n# Add second stage links\nlinks_second &lt;- patient_data |&gt;\n  count(second_treatment, third_treatment) |&gt;\n  filter(n &gt; 0) |&gt;\n  mutate(\n    source = match(second_treatment, nodes$name) - 1 + length(treatments),\n    target = match(third_treatment, nodes$name) - 1 + 2 * length(treatments),\n    value = n\n  ) |&gt;\n  select(source, target, value)\n\nlinks &lt;- bind_rows(links, links_second)\n\n# Create Sankey diagram\np_sankey &lt;- sankeyNetwork(\n  Links = links,\n  Nodes = nodes,\n  Source = \"source\",\n  Target = \"target\",\n  Value = \"value\",\n  NodeID = \"name\",\n  fontSize = 12,\n  nodeWidth = 30,\n  sinksRight = FALSE\n)\n\n# Add title using HTML\np_sankey &lt;- htmlwidgets::prependContent(\n  p_sankey,\n  htmltools::tags$h3(\"Hypertension Treatment Pattern Flow Over Time\")\n)\n\np_sankey\n\nHypertension Treatment Pattern Flow Over Time"
  },
  {
    "objectID": "applications_5_visualizing_time_trends.html#sunburst-plots",
    "href": "applications_5_visualizing_time_trends.html#sunburst-plots",
    "title": "Application 5: Time Trends",
    "section": "Sunburst Plots",
    "text": "Sunburst Plots\nSunburst plots are another common way to visualize the flow of data. Below, we create a Sunburst plot to visualize the distribution of patients with hypertension who change treatments over time.\nThese show the distribution of patients by treatment at each stage of the process, using color to indicate the treatment at each stage.\n\n# Create Sunburst plot data\n# Create properly formatted sunburst data with three levels\nsunburst_data &lt;- patient_data |&gt;\n  # Create paths for each treatment stage\n  mutate(\n    ids = paste0(\"initial_\", initial_treatment),\n    labels = initial_treatment,\n    parents = \"\",\n    values = 1,\n    level = \"initial\"\n  ) |&gt;\n  bind_rows(\n    patient_data |&gt;\n      filter(has_change == 1) |&gt;\n      mutate(\n        ids = paste0(\"second_\", initial_treatment, \"_\", second_treatment),\n        labels = second_treatment,\n        parents = paste0(\"initial_\", initial_treatment),\n        values = 1,\n        level = \"second\"\n      )\n  ) |&gt;\n  bind_rows(\n    patient_data |&gt;\n      filter(has_second_change == 1) |&gt;\n      mutate(\n        ids = paste0(\"third_\", initial_treatment, \"_\", second_treatment, \"_\", third_treatment),\n        labels = third_treatment,\n        parents = paste0(\"second_\", initial_treatment, \"_\", second_treatment),\n        values = 1,\n        level = \"third\"\n      )\n  ) |&gt;\n  group_by(ids, labels, parents, level) |&gt;\n  summarise(values = sum(values), .groups = \"drop\")\n\n# Create color scales for each level\ninitial_colors &lt;- viridis::viridis(4)\nsecond_colors &lt;- viridis::viridis(4, begin = 0.3, end = 0.7)\nthird_colors &lt;- viridis::viridis(4, begin = 0.6, end = 1)\n\n# Create Sunburst plot using plotly\np_sunburst &lt;- plot_ly(\n  sunburst_data,\n  ids = ~ids,\n  labels = ~labels,\n  parents = ~parents,\n  values = ~values,\n  type = \"sunburst\",\n  branchvalues = \"total\",\n  marker = list(\n    colors = case_when(\n      sunburst_data$level == \"initial\" ~ initial_colors[match(sunburst_data$labels, treatments)],\n      sunburst_data$level == \"second\" ~ second_colors[match(sunburst_data$labels, treatments)],\n      sunburst_data$level == \"third\" ~ third_colors[match(sunburst_data$labels, treatments)]\n    )\n  )\n) |&gt;\n  layout(\n    title = \"Hypertension Treatment Pattern Distribution\",\n    width = 800,\n    height = 800\n  )\n\n# Display plots\np_sunburst"
  },
  {
    "objectID": "applications_6_visualizing_correlations_and_models.html",
    "href": "applications_6_visualizing_correlations_and_models.html",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "",
    "text": "# List of required packages\nrequired_packages &lt;- c(\n  \"dplyr\",\n  \"ggplot2\",\n  \"broom.mixed\",\n  \"kableExtra\",\n  \"lme4\",\n  \"readr\",\n  \"emmeans\"\n)\n\n# Install missing packages\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load all packages\nfor (package in required_packages) {\n  library(package, character.only = TRUE)\n}\n\nsource(here::here(\"examples\", \"colors.R\"))\nset.seed(123)"
  },
  {
    "objectID": "applications_6_visualizing_correlations_and_models.html#looking-at-data-correlations",
    "href": "applications_6_visualizing_correlations_and_models.html#looking-at-data-correlations",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Looking At Data: Correlations",
    "text": "Looking At Data: Correlations\nCorrelations are indisposable for understanding the relationship between two variables, but they can be misleading as we show below.\nThis is adapted directly from Jan Vanhove.\n\nplot_r &lt;- function(df, showSmoother = TRUE, smoother = \"lm\") {\n  p &lt;- ggplot(df, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7)\n  \n  if(showSmoother) {\n    p &lt;- p +\n    geom_smooth(\n      formula = y ~ x,\n      method = smoother,\n      color = colors$green$`500`,\n      fill = colors$slate$`300`,\n      alpha = 0.3,\n    )\n  }\n\n  p &lt;- p +\n    facet_wrap(~title, scales = \"free\", ncol = 2) +\n    theme_minimal(base_size = 12) +\n    theme(\n      strip.text = element_text(face = \"bold\", size = 14),\n      axis.title = element_blank(),\n      plot.title = element_text(size = 24, face = \"bold\", hjust = 0.5),\n      panel.grid.minor = element_blank(),\n      axis.text = element_text(size = 10),\n      panel.spacing = unit(2, \"lines\")\n    )\n  \n  p\n}\n\ncorr_r &lt;- function(r = 0.6, n = 50) {\n  \n  compute.y &lt;- function(x, y, r) {\n    theta &lt;- acos(r)\n    X &lt;- cbind(x, y)\n    Xctr &lt;- scale(X, center = TRUE, scale = FALSE)    # Centered variables (mean 0)\n    Id &lt;- diag(n)                                     # Identity matrix\n    Q &lt;- qr.Q(qr(Xctr[, 1, drop = FALSE]))            # QR decomposition\n    P &lt;- tcrossprod(Q)                                # Projection onto space defined by x1\n    x2o &lt;- (Id - P) %*% Xctr[, 2]                     # x2ctr made orthogonal to x1ctr\n    Xc2 &lt;- cbind(Xctr[, 1], x2o)\n    Y &lt;- Xc2 %*% diag(1 / sqrt(colSums(Xc2 ^ 2)))\n    y &lt;- Y[, 2] + (1 / tan(theta)) * Y[, 1]\n    return(y)\n  }\n  \n  cases &lt;- list(\n    list(id = 1, title = \"(1) Normal x, normal residuals\", x = rnorm(n), y = rnorm(n)),\n    list(id = 2, title = \"(2) Uniform x, normal residuals\", x = runif(n, 0, 1), y = rnorm(n)),\n    list(id = 3, title = \"(3) +-skewed x, normal residuals\", x = rlnorm(n, 5), y = rnorm(n)),\n    list(id = 4, title = \"(4) --skewed x, normal residuals\", x = rlnorm(n, 5) * -1 + 5000, y = rnorm(n)),\n    list(id = 5, title = \"(5) Normal x, +-skewed residuals\", x = rnorm(n), y = rlnorm(n, 5)),\n    list(id = 6, title = \"(6) Normal x, --skewed residuals\", x = rnorm(n), y = -rlnorm(n, 5)),\n    list(id = 7, title = \"(7) Increasing spread\", \n         x = sort(rnorm(n)) + abs(min(rnorm(n))), \n         y = rnorm(n, 0, sqrt(abs(10 * sort(rnorm(n)))))),\n    list(id = 8, title = \"(8) Decreasing spread\", \n         x = sort(rnorm(n)) + abs(min(rnorm(n))), \n         y = rnorm(n, 0, sqrt(pmax(0.1, abs(10 * max(sort(rnorm(n))) - 10 * sort(rnorm(n))))))),\n    list(id = 9, title = \"(9) Quadratic trend\", x = rnorm(n), y = rnorm(n) ^ 2),\n    list(id = 10, title = \"(10) Sinusoid relationship\", x = runif(n, -2 * pi, 2 * pi), y = sin(runif(n, -2 * pi, 2 * pi))),\n    list(id = 11, title = \"(11) A single positive outlier\", x = c(rnorm(n - 1), 10), y = c(rnorm(n - 1), 15)),\n    list(id = 12, title = \"(12) A single negative outlier\", x = c(rnorm(n - 1), 10), y = c(rnorm(n - 1), -15)),\n    list(id = 13, title = \"(13) Bimodal residuals\", x = rnorm(n), y = c(rnorm(floor(n / 2), mean = -3), rnorm(ceiling(n / 2), 3))),\n    list(id = 14, title = \"(14) Two groups\", \n         x = c(rnorm(floor(n / 2), -3), rnorm(ceiling(n / 2), 3)), \n         y = c(rnorm(floor(n / 2), mean = 3), rnorm(ceiling(n / 2), mean = -3))),\n    list(id = 15, title = \"(15) Sampling at the extremes\", \n         x = c(rnorm(floor(n / 2)), rnorm(ceiling(n / 2), mean = 10)), \n         y = rnorm(n)),\n    list(id = 16, title = \"(16) Categorical data\", \n         x = sample(1:5, n, replace = TRUE), \n         y = sample(1:7, n, replace = TRUE))\n  )\n  \n  df &lt;- bind_rows(lapply(cases, function(case) {\n    id = case$id\n    x &lt;- case$x\n    y &lt;- compute.y(x, case$y, r)\n    data.frame(id = id, x = x, y = y, title = case$title)\n  }))\n  \n  df$title &lt;- factor(df$title, levels = paste0(\"(\", 1:16, \") \", c(\n    \"Normal x, normal residuals\",\n    \"Uniform x, normal residuals\",\n    \"+-skewed x, normal residuals\",\n    \"--skewed x, normal residuals\",\n    \"Normal x, +-skewed residuals\",\n    \"Normal x, --skewed residuals\",\n    \"Increasing spread\",\n    \"Decreasing spread\",\n    \"Quadratic trend\",\n    \"Sinusoid relationship\",\n    \"A single positive outlier\",\n    \"A single negative outlier\",\n    \"Bimodal residuals\",\n    \"Two groups\",\n    \"Sampling at the extremes\",\n    \"Categorical data\"\n  )))\n  \n\n  return(df)\n}\n\n\ndata &lt;- corr_r(r=0.3, n=100)\n\nanalysis_data &lt;- data |&gt; filter(id == 1)\nmodel &lt;- lm(y ~ x, data = analysis_data)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = analysis_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.19848 -0.07113 -0.00910  0.06042  0.34241 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.00313    0.01015  -0.308  0.75846   \nx            0.03463    0.01112   3.113  0.00243 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.101 on 98 degrees of freedom\nMultiple R-squared:   0.09, Adjusted R-squared:  0.08071 \nF-statistic: 9.692 on 1 and 98 DF,  p-value: 0.002426\n\ncor(analysis_data$x, analysis_data$y)\n\n[1] 0.3\n\nplot_r(data, showSmoother=FALSE)\n\n\n\n\n\n\n\nplot_r(data, showSmoother=TRUE)"
  },
  {
    "objectID": "applications_6_visualizing_correlations_and_models.html#visualizing-model-outputs",
    "href": "applications_6_visualizing_correlations_and_models.html#visualizing-model-outputs",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Visualizing Model Outputs",
    "text": "Visualizing Model Outputs\nVisualization of model outputs is often neglected, but it can be a powerful way both to undrestand and to communicate the results of a model. A number of packages exist to help with this, including broom.mixed, emmeans, and ggeffects.\nHere, we fit a logistic mixed-effects model using the glmer() function to estimate the odds of receiving comprehensive postnatal care. This model accounts for both fixed effects (individual-level predictors like race or insurance status) and a random intercept for provider, which captures unobserved heterogeneity across providers.\n\n# Load data\ndata &lt;- read_csv(here::here(\"data\", \"processed\", \"simulated_data.csv\"))\n\nRows: 50000 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): state, self_report_income, edu, race_ethnicity, insurance, job_type\ndbl (14): id, provider_id, received_comprehensive_postnatal_care, age, depen...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Sample 1/4 of the sites\nset.seed(123)\nunique_sites &lt;- unique(data$provider_id)\nreduced_sites &lt;- sample(unique_sites, length(unique_sites) * 0.10)\ndata &lt;- data[data$provider_id %in% reduced_sites, ]\n\n# Fit the model\nmodel &lt;- glmer(\n  received_comprehensive_postnatal_care ~ \n    race_ethnicity +\n    log(distance_to_provider) +\n    insurance +\n    multiple_gestation + \n    placenta_previa + \n    gest_hypertension + \n    preeclampsia +\n    (1 | provider_id),  \n  data = data, \n  family = binomial(link = \"logit\"),\n  control = glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 1e5))\n)\n\n# Create variable labels for better visualization\nvariable_labels &lt;- c(\n  \"race_ethnicityaian\" = \"AIAN\",\n  \"race_ethnicityasian\" = \"Asian\",\n  \"race_ethnicityblack\" = \"Black\",\n  \"race_ethnicityhispanic\" = \"Hispanic\",\n  \"race_ethnicitynhpi\" = \"NHPI\",\n  \"race_ethnicityother\" = \"Other\",\n  \"log(distance_to_provider)\" = \"Log(Distance to Provider)\",\n  \"insuranceprivate\" = \"Insurance: Private\",\n  \"insurancestate_provided\" = \"Insurance: State-Provided\",\n  \"multiple_gestation\" = \"Multiple Gestation\",\n  \"placenta_previa\" = \"Placenta Previa\",\n  \"gest_hypertension\" = \"Gestational Hypertension\",\n  \"preeclampsia\" = \"Preeclampsia\",\n  \"(Intercept)\" = \"Intercept\"\n)\n\nBelow, we extract the fixed-effect estimates using broom.mixed::tidy(), including 95% confidence intervals, and plot them as a coefficient plot (a forest plot for regression coefficients).\nEach point shows the estimated log-odds of receiving comprehensive care associated with a given covariate, holding other variables constant. The dashed vertical line at 0 represents no effect.\n\n# Get fixed effects with confidence intervals\nfixed_effects &lt;- tidy(model, conf.int = TRUE) |&gt;\n  filter(effect == \"fixed\") |&gt;\n  # Remove any NA values\n  filter(!is.na(estimate)) |&gt;\n  # Create a more readable term name\n  mutate(term = case_when(\n    term == \"(Intercept)\" ~ \"Intercept\",\n    term == \"age\" ~ \"Age\",\n    term == \"sexMale\" ~ \"Male Sex\",\n    term == \"raceBlack\" ~ \"Black Race\",\n    term == \"raceHispanic\" ~ \"Hispanic Race\",\n    term == \"raceOther\" ~ \"Other Race\",\n    term == \"insurancePrivate\" ~ \"Private Insurance\",\n    term == \"insuranceMedicaid\" ~ \"Medicaid\",\n    term == \"insuranceOther\" ~ \"Other Insurance\",\n    term == \"comorbidity_score\" ~ \"Comorbidity Score\",\n    term == \"emergency_admissionTRUE\" ~ \"Emergency Admission\",\n    term == \"weekend_admissionTRUE\" ~ \"Weekend Admission\",\n    term == \"night_admissionTRUE\" ~ \"Night Admission\",\n    TRUE ~ term\n  ))\n\n# Create the forest plot\nggplot(fixed_effects, aes(x = estimate, y = reorder(term, estimate))) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), \n                 height = 0.2, color = colors$slate$`300`) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = colors$red$`600`) +\n  labs(\n    title = \"Fixed Effects from GLMM\",\n    subtitle = \"Each point represents the estimated effect on log-odds of receiving comprehensive care\",\n    x = \"Effect Size (95% CI)\",\n    y = \"Predictor\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    axis.text.y = element_text(size = 10)\n  )"
  },
  {
    "objectID": "applications_6_visualizing_correlations_and_models.html#predicted-probabilities-by-covariate-groups",
    "href": "applications_6_visualizing_correlations_and_models.html#predicted-probabilities-by-covariate-groups",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Predicted Probabilities by Covariate Groups",
    "text": "Predicted Probabilities by Covariate Groups\nHere, we use the emmeans package to estimate marginal predicted probabilities for selected covariates. These represent the expected probability of receiving comprehensive care for each level of a covariate, averaging over the distribution of other covariates in the model. This approach helps isolate the relationship between each covariate and the outcome while controlling for confounding. We visualize these estimates and their 95% confidence intervals, grouped by domain (e.g., race/ethnicity, insurance, medical conditions).\n\n# Calculate predicted probabilities for each covariate group\nlibrary(emmeans)\n\n# Race/Ethnicity\nrace_probs &lt;- emmeans(model, ~ race_ethnicity, type = \"response\")\nrace_probs_df &lt;- as.data.frame(race_probs) |&gt;\n  mutate(\n    group = \"Race/Ethnicity\",\n    category = case_when(\n      race_ethnicity == \"aian\" ~ \"AIAN\",\n      race_ethnicity == \"asian\" ~ \"Asian\",\n      race_ethnicity == \"black\" ~ \"Black\",\n      race_ethnicity == \"hispanic\" ~ \"Hispanic\",\n      race_ethnicity == \"nhpi\" ~ \"NHPI\",\n      race_ethnicity == \"other\" ~ \"Other\",\n      race_ethnicity == \"white\" ~ \"White\"\n    )\n  )\n\n# Insurance\ninsurance_probs &lt;- emmeans(model, ~ insurance, type = \"response\")\ninsurance_probs_df &lt;- as.data.frame(insurance_probs) |&gt;\n  mutate(\n    group = \"Insurance\",\n    category = case_when(\n      insurance == \"medicaid\" ~ \"Medicaid\",\n      insurance == \"private\" ~ \"Private\",\n      insurance == \"state_provided\" ~ \"State-Provided\",\n      insurance == \"uninsured\" ~ \"Uninsured\",\n      TRUE ~ \"Other\"\n    )\n  )\n\n# Medical Conditions\nconditions_probs &lt;- emmeans(model, ~ multiple_gestation + placenta_previa + gest_hypertension + preeclampsia, type = \"response\")\nconditions_probs_df &lt;- as.data.frame(conditions_probs) |&gt;\n  mutate(\n    group = \"Medical Conditions\",\n    category = case_when(\n      multiple_gestation == 1 ~ \"Multiple Gestation\",\n      placenta_previa == 1 ~ \"Placenta Previa\",\n      gest_hypertension == 1 ~ \"Gestational Hypertension\",\n      preeclampsia == 1 ~ \"Preeclampsia\",\n      TRUE ~ \"No Medical Conditions\"\n    )\n  ) |&gt;\n  # Remove duplicates where multiple conditions are true\n  distinct(category, .keep_all = TRUE)\n\n# Combine all predictions\nall_probs &lt;- bind_rows(\n  race_probs_df |&gt; select(group, category, prob, asymp.LCL, asymp.UCL),\n  insurance_probs_df |&gt; select(group, category, prob, asymp.LCL, asymp.UCL),\n  conditions_probs_df |&gt; select(group, category, prob, asymp.LCL, asymp.UCL)\n)\n\n# Create faceted plot\nggplot(all_probs, aes(x = prob, y = reorder(category, prob))) +\n  geom_errorbarh(aes(xmin = asymp.LCL, xmax = asymp.UCL), \n                 height = 0.2, color = colors$slate$`300`) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  facet_wrap(~ group, scales = \"free_y\", ncol = 1) +\n  labs(\n    title = \"Predicted Probability of Receiving Comprehensive Care\",\n    subtitle = \"By Covariate Groups\",\n    x = \"Predicted Probability (95% CI)\",\n    y = \"Category\"\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    axis.text.y = element_text(size = 10),\n    strip.text = element_text(size = 12, face = \"bold\")\n  )"
  },
  {
    "objectID": "applications_6_visualizing_correlations_and_models.html#provider-level-random-effects",
    "href": "applications_6_visualizing_correlations_and_models.html#provider-level-random-effects",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Provider-Level Random Effects",
    "text": "Provider-Level Random Effects\nWe visualize the provider-level random intercepts, which represent the estimated deviation of each provider from the overall mean (intercept) after accounting for the fixed effects in the model. This helps reveal between-provider variability and can identify providers whose outcomes are systematically higher or lower than expected. The dashed vertical line at zero indicates the overall average; providers to the right have higher-than-average effects, and those to the left are lower.\n\n# Get random effects\nrandom_effects &lt;- ranef(model, condVar = TRUE)\nrandom_effects_data &lt;- as.data.frame(random_effects)\n\n# Create the forest plot of random effects\nggplot(random_effects_data, aes(x = condval, y = reorder(grp, condval))) +\n  geom_errorbarh(aes(xmin = condval - 1.96*sqrt(attr(random_effects$provider_id, \"postVar\")[1,1,]), \n                     xmax = condval + 1.96*sqrt(attr(random_effects$provider_id, \"postVar\")[1,1,])), \n                 height = 0.2, color = colors$slate$`300`) +\n  geom_point(size = 3, color = colors$blue$`500`) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = colors$red$`600`) +\n  labs(\n    title = \"Random Effects by Provider\",\n    subtitle = \"Each point represents the provider-specific deviation from the overall intercept\",\n    x = \"Provider-Specific Effect (95% CI)\",\n    y = \"Provider ID\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    axis.text.y = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\n# Print model summary\nsummary(model)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nreceived_comprehensive_postnatal_care ~ race_ethnicity + log(distance_to_provider) +  \n    insurance + multiple_gestation + placenta_previa + gest_hypertension +  \n    preeclampsia + (1 | provider_id)\n   Data: data\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 1e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   5679.3    5776.5   -2824.7    5649.3      4782 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.3504 -0.6684 -0.5330  1.1475  3.0147 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n provider_id (Intercept) 0.2053   0.4531  \nNumber of obs: 4797, groups:  provider_id, 50\n\nFixed effects:\n                          Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)               -1.19650    0.40041  -2.988  0.00281 **\nrace_ethnicityasian        0.36808    0.41330   0.891  0.37315   \nrace_ethnicityblack       -0.21309    0.40567  -0.525  0.59939   \nrace_ethnicityhispanic     0.11511    0.39780   0.289  0.77230   \nrace_ethnicitynhpi         0.07320    0.80323   0.091  0.92739   \nrace_ethnicityother        0.54133    0.52931   1.023  0.30645   \nrace_ethnicitywhite        0.23508    0.39261   0.599  0.54933   \nlog(distance_to_provider) -0.02094    0.02519  -0.831  0.40585   \ninsuranceprivate           0.18922    0.07774   2.434  0.01493 * \ninsurancestate_provided    0.15031    0.08768   1.714  0.08648 . \nmultiple_gestation         0.22298    0.20185   1.105  0.26930   \nplacenta_previa            0.10400    0.30208   0.344  0.73063   \ngest_hypertension          0.23839    0.14268   1.671  0.09477 . \npreeclampsia               0.30954    0.22275   1.390  0.16463   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCorrelation matrix not shown by default, as p = 14 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it"
  },
  {
    "objectID": "applications_6_visualizing_correlations_and_models.html#interaction-effects",
    "href": "applications_6_visualizing_correlations_and_models.html#interaction-effects",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "Interaction Effects",
    "text": "Interaction Effects\nThe emmeans package can also be used to help visualize interaction effects. Let’s simulate some data with an interaction effect.\n\nInteraction Between Categorical and Continuous Variable\nIn this example, we simulate data to study how different exercise programs might be more or less effective depending on a person’s starting fitness level. We have:\n\nA categorical predictor: Type of exercise program\n\nHigh-Intensity Interval Training (HIIT): Short bursts of intense exercise followed by rest\nStrength Training: Weight lifting and resistance exercises\nCardio: Steady-state aerobic exercise like jogging or cycling\n\nA continuous predictor: Baseline fitness level (on a 1-10 scale)\nAn outcome: Weight loss in pounds\n\nThe key question is whether the effectiveness of each program depends on how fit someone is when they start. For example, HIIT might be more effective for people who are already somewhat fit, while Cardio might be better for beginners. The relationship between program type and weight loss changes depends on baseline fitness.\nEach program has:\n\nA baseline effectiveness (how much weight loss we expect at average fitness levels)\nA different rate of effectiveness based on baseline fitness (how much more or less effective it becomes as fitness changes)\nSome random variation to account for individual differences\n\n\n# Simulate data with an interaction effect\nset.seed(123)\nn &lt;- 500\n\n# Simulate data for program × fitness interaction\nix_data &lt;- data.frame(\n  # Categorical predictor: Exercise program type\n  program = sample(c(\"HIIT\", \"Strength Training\", \"Cardio\"), \n                  size = n, replace = TRUE),\n  # Continuous predictor: Baseline fitness level (1-10 scale)\n  baseline_fitness = runif(n, min = 1, max = 10),\n  # Add some noise\n  error = rnorm(n, 0, 2)\n) |&gt;\n  # Create interaction effect\n  mutate(\n    # Center the baseline fitness\n    baseline_fitness_centered = baseline_fitness - mean(baseline_fitness),\n    # Different slopes for different programs\n    fitness_effect = case_when(\n      program == \"HIIT\" ~ 0.8,\n      program == \"Strength Training\" ~ 0.5,\n      program == \"Cardio\" ~ 0.3\n    ),\n    # Different intercepts for different programs\n    program_effect = case_when(\n      program == \"HIIT\" ~ 8.0,\n      program == \"Strength Training\" ~ 6.0,\n      program == \"Cardio\" ~ 4.0\n    ),\n    # Calculate outcome: Weight loss in pounds\n    weight_loss = program_effect + (fitness_effect * baseline_fitness_centered) + error\n  )\n\nLet’s fit a model to the simulated data.\n\nmodel &lt;- lm(weight_loss ~ program * baseline_fitness_centered, data = ix_data)\nmodel |&gt; tidy() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.1573036\n0.1628360\n25.530615\n0.0000000\n\n\nprogramHIIT\n3.7323100\n0.2285624\n16.329498\n0.0000000\n\n\nprogramStrength Training\n1.9190550\n0.2283159\n8.405262\n0.0000000\n\n\nbaseline_fitness_centered\n0.2963606\n0.0643186\n4.607698\n0.0000052\n\n\nprogramHIIT:baseline_fitness_centered\n0.4923781\n0.0884914\n5.564134\n0.0000000\n\n\nprogramStrength Training:baseline_fitness_centered\n0.2588035\n0.0920493\n2.811576\n0.0051260\n\n\n\n\n\nWe get results back. We have “main effects” for the program type and the baseline fitness level, and an interaction effect between the two.\nCentering helps: The intercept represents the expected weight loss when baseline fitness is at its mean.\nThe emmeans package can help us interpret. Let’s get it to give us the predicted weight loss at the average fitness level.\n\n# Plot the interaction\nemmeans(model, ~ program | baseline_fitness_centered)\n\nbaseline_fitness_centered = 1.28e-16:\n program           emmean    SE  df lower.CL upper.CL\n Cardio              4.16 0.163 494     3.84     4.48\n HIIT                7.89 0.160 494     7.57     8.20\n Strength Training   6.08 0.160 494     5.76     6.39\n\nConfidence level used: 0.95 \n\n\nWe see that this lines up with our model results:\n\nCardio: 4.16 lbs (intercept)\nHIIT: 7.89 lbs (4.16 + 3.73 = 7.89)\nStrength Training: 6.08 lbs (4.16 + 1.92 = 6.08)\n\nBut what if we want to see the relationship between program type and weight loss at different baseline fitness levels? Let’s create a plot that shows how weight loss changes across the full range of baseline fitness for each program.\nWe can use emmip() (expected means interaction plot)to plot the predicted weight loss by program type at different baseline fitness levels. We add some text to the plot to show the slopes for each program.\n\n# Create baseline fitness values in the original (uncentered) scale\n\n\n# Get centered values based on the observed data range\ncentered_range &lt;- range(ix_data$baseline_fitness - mean(ix_data$baseline_fitness))\ncentered_vals &lt;- seq(centered_range[1], centered_range[2], length.out = 100)\n\n# Create the plot\nemmip(\n  model,\n  program ~ baseline_fitness_centered,\n  at = list(baseline_fitness_centered = centered_vals),\n  type = \"response\"\n) +\n  labs(\n    title = \"Predicted Weight Loss by Program and Baseline Fitness\",\n    x = \"Baseline Fitness Level (Centered)\",\n    y = \"Predicted Weight Loss (pounds)\",\n    color = \"Program Type\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(-5, 5, by = 1),\n    labels = seq(-5, 5, by = 1)\n  ) +\n  annotate(\"text\", x = 3.5, y = 4.5, label = \"Cardio slope: 0.30\", color = \"#F8766D\", hjust = 0) +\n  annotate(\"text\", x = 3.5, y = 10.5, label = \"HIIT slope: 0.79\", color = \"#00BA38\", hjust = 0) +\n  annotate(\"text\", x = 3.5, y = 7.5, label = \"Strength slope: 0.56\", color = \"#619CFF\", hjust = 0) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nI find this a lot easier to interpret than the table of results. “Seeing is believing.”"
  },
  {
    "objectID": "applications_6_visualizing_correlations_and_models.html#more-complexity-categorical-by-continuous-by-continuous-interaction",
    "href": "applications_6_visualizing_correlations_and_models.html#more-complexity-categorical-by-continuous-by-continuous-interaction",
    "title": "Application 6: Visualizing Correlations and Models",
    "section": "More Complexity: Categorical by Continuous by Continuous Interaction",
    "text": "More Complexity: Categorical by Continuous by Continuous Interaction\nLet’s simulate data to get a sense of a complex set of interactions.\nWe will create a simulated dataset that explores how lung function is influenced by age, air pollution exposure, and smoking status. Both age and pollution independently reduce lung function, but the negative effect of pollution becomes more pronounced as people get older, so pollution is more damaging to lung function in older adults than in younger ones. Moreover, smoking increases the effect of pollution on lung function.\n\nfev1 = forced expiratory volume in one second\nage = age, which can be 30-80\npm25 = particulate matter exposure\nis_smoker = whether the person is a smoker\n\n\nset.seed(123)\nn &lt;- 300\nage &lt;- runif(n, 30, 80)              # Age 30–80\npm25 &lt;- runif(n, 5, 35)              # PM2.5 range\ninteraction_effect &lt;- -0.01          # Base moderation: steeper slope with age\n\n# Add smoking status (binary)\n# More likely to have smoking history as age increases\nsmoking_prob &lt;- 0.2 + (age - 30)/100  # Probability increases with age\nis_smoker &lt;- rbinom(n, 1, smoking_prob)\n\n# Create age-dependent error term (more variation with age)\nage_error &lt;- rnorm(n, 0, 0.2 * (1 + (age - 30)/50))  # Error increases with age\n\n# Outcome: base lung function + age effect + pollution effect + \n# two-way interactions + three-way interaction + smoking effect\n# Please note how are simulated data looks like the model we are going to estimate.\n# Simulating data is a great way to undrestand how models work.\n# In fact, if you uncenter the below model, you will see that the coefficients are very close to what we define below.\nfev1 &lt;- 4.5 - # intercept\n        0.02*age - # age effect\n        0.03*pm25 + # pollution effect\n        interaction_effect * age * pm25 -  # age × pollution\n        0.5*is_smoker +                    # smoking main effect\n        0.02*age*is_smoker +               # age × smoking\n        0.01*pm25*is_smoker +              # pollution × smoking\n        -0.005*age*pm25*is_smoker +        # three-way interaction\n        age_error\n\n# Create data frame\ndata &lt;- data.frame(age, pm25, fev1, is_smoker)\ndata |&gt; head() |&gt; kable()\n\n\n\n\nage\npm25\nfev1\nis_smoker\n\n\n\n\n44.37888\n28.537258\n-9.5398588\n0\n\n\n69.41526\n5.282897\n-0.3395903\n0\n\n\n50.44885\n28.371976\n-11.5508085\n0\n\n\n74.15087\n26.881720\n-26.1678392\n1\n\n\n77.02336\n23.903956\n-23.7395757\n1\n\n\n32.27782\n19.427325\n-6.3510524\n1\n\n\n\n\n\nA starting point for an interaction analysis is to color code by group and look at the trends. Let’s create age bins and plot out the trends, faceting by smoker status.\nWe clearly see a slope difference by age. We also see that all the slopes of smokers appear to be steeper than the slopes of non-smokers.\n\ndata |&gt; \n  mutate(\n    age_group = cut(age, breaks = c(30, 40, 50, 60, 70, 80), \n                   labels = c(\"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70+\")),\n    smoking_status = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\")\n  ) |&gt; \n  ggplot(aes(x = pm25, y = fev1, color = age_group)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ smoking_status) +\n  labs(\n    title = \"Lung Function by Air Pollution and Age Group\",\n    subtitle = \"Faceted by Smoking Status\",\n    x = \"PM2.5 Exposure\",\n    y = \"FEV1 (L)\",\n    color = \"Age Group\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Show variation by age group and smoking status\ndata |&gt;\n  mutate(\n    age_group = cut(age, breaks = c(30, 40, 50, 60, 70, 80), \n                   labels = c(\"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70+\")),\n    smoking_status = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\")\n  ) |&gt;\n  group_by(age_group, smoking_status) |&gt;\n  summarize(\n    mean_fev1 = mean(fev1),\n    sd_fev1 = sd(fev1),\n    min_fev1 = min(fev1),\n    max_fev1 = max(fev1),\n    n = n()\n  ) |&gt;\n  kable()\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage_group\nsmoking_status\nmean_fev1\nsd_fev1\nmin_fev1\nmax_fev1\nn\n\n\n\n\n30-39\nNon-smoker\n-3.868944\n3.220115\n-9.858047\n0.9199290\n40\n\n\n30-39\nSmoker\n-7.520161\n4.991676\n-15.763336\n0.4955185\n13\n\n\n40-49\nNon-smoker\n-6.433924\n4.233292\n-13.192217\n0.5287203\n43\n\n\n40-49\nSmoker\n-11.018059\n6.030700\n-20.044162\n-0.5861045\n25\n\n\n50-59\nNon-smoker\n-8.334104\n5.279194\n-17.175684\n0.4095706\n45\n\n\n50-59\nSmoker\n-13.701037\n8.796267\n-27.544445\n-0.4953899\n17\n\n\n60-69\nNon-smoker\n-9.396839\n6.462022\n-21.041806\n-0.3395903\n28\n\n\n60-69\nSmoker\n-16.031041\n8.681144\n-30.743716\n-1.9684067\n33\n\n\n70+\nNon-smoker\n-11.475683\n7.039974\n-20.919413\n-0.8792988\n19\n\n\n70+\nSmoker\n-16.826588\n9.214377\n-33.283871\n-1.8593214\n37\n\n\n\n\n\nNow let’s center our variables and fit a model with the three-way interaction.\n\ndata &lt;- data |&gt; \n  mutate(\n    pm25_centered = pm25 - mean(pm25),\n    age_centered = age - mean(age)\n  )\n\nmodel &lt;- lm(fev1 ~ pm25_centered * age_centered * is_smoker, data = data)\nmodel |&gt; tidy() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-8.1597223\n0.0230908\n-353.37484\n0\n\n\npm25_centered\n-0.5730489\n0.0025919\n-221.09554\n0\n\n\nage_centered\n-0.2173991\n0.0017101\n-127.12552\n0\n\n\nis_smoker\n-4.7052941\n0.0363237\n-129.53798\n0\n\n\npm25_centered:age_centered\n-0.0099022\n0.0001941\n-51.02526\n0\n\n\npm25_centered:is_smoker\n-0.2686729\n0.0041309\n-65.04034\n0\n\n\nage_centered:is_smoker\n-0.0783907\n0.0025621\n-30.59665\n0\n\n\npm25_centered:age_centered:is_smoker\n-0.0048553\n0.0003050\n-15.91835\n0\n\n\n\n\n\nWe see that the interaction terms in our regression model are statistically significant, but interpreting them directly from the table is very difficult. This is especially true in models with two or more continuous predictors interacting, and even more so when one of them is also interacting with a binary group.\nTake the main effects: pm25_centered and age_centered. These coefficients only describe the effect of PM2.5 or age at the average level of the other variable. For example, the coefficient for PM2.5 tells us how pollution affects FEV1 only at the mean age in our sample. That’s not very useful unless you already know what the mean is, and even then, it’s just a narrow slice of the story.\nNow add in interaction terms like pm25 × age or pm25 × age × is_smoker, and things get more abstract. What does a coefficient of –0.01 for pm25 × age actually mean? Technically, it means the effect of PM2.5 on lung function gets more negative as age increases, but:\n\nHow much more?\nIs that change clinically meaningful?\nDoes the slope actually flip direction at some point?\nIs the effect stronger in smokers than non-smokers?\n\nYou could try to work this out by multiplying coefficients in your head, but if you’re like me, then you’re not smart enough. Better to use visualization to help.\nemmeans can help us by generating predicted values and estimated trends. It also allows us to probe the model and “un-center” the data for interpretation.\nFor example, we can explore how the slope of PM2.5 on FEV1 changes at different ages, and see how those slopes differ by smoking status. Using emtrends(), we can even extract the actual slope estimates by subgroup, turning abstract interactions into tangible comparisons.\n\n# Get centering values\nmean_age &lt;- mean(data$age)\nmean_pm25 &lt;- mean(data$pm25)\n\n# Define values for uncentered age and PM2.5\nage_vals &lt;- c(40, 50, 60, 70, 80)\npm25_vals &lt;- seq(5, 35, length.out = 100)\n\n# Centered versions to use in emmip\nage_centered_vals &lt;- age_vals - mean_age\npm25_centered_vals &lt;- pm25_vals - mean_pm25\n\n# 1. Get predicted values with emmip, grouped by smoking status\npred_data &lt;- emmip(\n  model,\n  age_centered ~ pm25_centered | is_smoker,\n  at = list(\n    age_centered = age_centered_vals,\n    pm25_centered = pm25_centered_vals,\n    is_smoker = c(0, 1)\n  ),\n  type = \"response\",\n  plotit = FALSE\n)\n\n# Uncenter for plotting\npred_data &lt;- pred_data |&gt;\n  mutate(\n    age = age_centered + mean_age,\n    pm25 = pm25_centered + mean_pm25,\n    smoker_label = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\")\n  )\n\n# 2. Get slopes of PM2.5 at each age × smoking status\nslopes &lt;- emtrends(\n  model,\n  ~ age_centered * is_smoker,\n  var = \"pm25_centered\",\n  at = list(age_centered = age_centered_vals, is_smoker = c(0, 1))\n) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    age = age_centered + mean_age,\n    smoker_label = ifelse(is_smoker == 1, \"Smoker\", \"Non-smoker\"),\n    label = paste0(\"Slope: \", round(pm25_centered.trend, 3))\n  )\n\n# 3. Plot\nggplot(pred_data, aes(x = pm25, y = yvar, color = factor(age), group = interaction(age, smoker_label))) +\n  geom_line(linewidth = 1) +\n  geom_text(\n    data = pred_data |&gt;\n      group_by(age, smoker_label) |&gt;\n      slice_max(pm25) |&gt;\n      left_join(slopes, by = c(\"age\", \"smoker_label\")),\n    aes(x = pm25, y = yvar, label = label, color = factor(age)),\n    hjust = -0.1,\n    size = 3.5,\n    inherit.aes = FALSE,\n    show.legend = FALSE\n  ) +\n  facet_wrap(~ smoker_label) +\n  labs(\n    title = \"Predicted Lung Function by PM2.5 and Age, with Smoking Interaction\",\n    subtitle = \"Slopes of PM2.5 Effect Vary by Age and Smoking Status\",\n    x = \"PM2.5 (μg/m³)\",\n    y = \"Predicted FEV1\",\n    color = \"Age\"\n  ) +\n  xlim(min(pm25_vals), max(pm25_vals) + 5) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\",\n    strip.text = element_text(face = \"bold\")\n  )"
  },
  {
    "objectID": "colors_and_accessibility.html",
    "href": "colors_and_accessibility.html",
    "title": "Color Systems in R",
    "section": "",
    "text": "# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"ggplot2\", \"RColorBrewer\", \"viridis\", \"kableExtra\", \"tidyr\", \"ggridges\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(viridis)\nlibrary(kableExtra)\nlibrary(tidyr)\nlibrary(ggridges)"
  },
  {
    "objectID": "colors_and_accessibility.html#color-systems-in-r",
    "href": "colors_and_accessibility.html#color-systems-in-r",
    "title": "Color Systems in R",
    "section": "Color Systems in R",
    "text": "Color Systems in R\nWe can take several different approaches to color in data visualization, depending on our audience and our goals.\n\nData-focused colors: Designed for accessibility and clear data communication\nPerceptually uniform colors: Colorblind-friendly palettes with perceptual uniformity\nDesign system colors: For polished, consistent visual language"
  },
  {
    "objectID": "colors_and_accessibility.html#rcolorbrewer-accessible-data-visualization",
    "href": "colors_and_accessibility.html#rcolorbrewer-accessible-data-visualization",
    "title": "Color Systems in R",
    "section": "RColorBrewer: Accessible Data Visualization",
    "text": "RColorBrewer: Accessible Data Visualization\nRColorBrewer provides several types of color palettes designed for data visualization:\n\nSequential: for ordered data (e.g., “Blues”, “Greens”)\nDiverging: for data that deviates from a middle value (e.g., “RdBu”, “Spectral”)\nQualitative: for categorical data (e.g., “Set1”, “Set2”)\n\nHere are some examples of RColorBrewer palettes:\n\n# Create a function to display color palettes\ndisplay_brewer_pal &lt;- function(name, n) {\n  colors &lt;- brewer.pal(n, name)\n  plot(1:n, rep(1, n),\n       col = colors,\n       pch = 19,\n       cex = 5,\n       xlab = \"\",\n       ylab = \"\",\n       axes = FALSE)\n  title(name)\n}\n\n# Display some example palettes\npar(mfrow = c(2, 2))\ndisplay_brewer_pal(\"Blues\", 9)    # Sequential\ndisplay_brewer_pal(\"RdBu\", 9)     # Diverging\ndisplay_brewer_pal(\"Set1\", 9)     # Qualitative\ndisplay_brewer_pal(\"Spectral\", 9) # Diverging"
  },
  {
    "objectID": "colors_and_accessibility.html#using-rcolorbrewer-in-ggplot2",
    "href": "colors_and_accessibility.html#using-rcolorbrewer-in-ggplot2",
    "title": "Color Systems in R",
    "section": "Using RColorBrewer in ggplot2",
    "text": "Using RColorBrewer in ggplot2\nHere’s how RColorBrewer can be used in different visualization scenarios:\n\n# Create example data\ndf &lt;- data.frame(\n  category = letters[1:5],\n  value = c(10, 20, 15, 25, 30)\n)\n\n# Plot with qualitative colors (categorical data)\nggplot(df, aes(x = category, y = value, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Qualitative Colors (Set1)\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n# Create data for a heatmap\nset.seed(123)\nheatmap_data &lt;- expand.grid(\n  x = 1:10,\n  y = 1:10\n) |&gt;\n  mutate(\n    value = rnorm(100, mean = 50, sd = 15)\n  )\n\n# Plot with sequential colors (continuous data)\nggplot(heatmap_data, aes(x = x, y = y, fill = value)) +\n  geom_tile() +\n  scale_fill_distiller(palette = \"Blues\") +\n  labs(\n    title = \"Sequential Colors (Blues)\",\n    subtitle = \"Heatmap showing continuous data distribution\"\n  ) +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n# Create sample data with a moderate relationship and outliers\n\n# Create sample survey response data\nset.seed(789)\ncategories &lt;- c(\"Vaccination Requirements\", \"Mask Mandates\", \"Social Distancing\",\n               \"Contact Tracing\", \"Travel Restrictions\", \"Quarantine Rules\")\ndata &lt;- data.frame(\n  category = factor(categories, levels = categories),\n  support = c(90, 80, 70, 40, 30, 20),\n  oppose = c(10, 20, 30, 60, 70, 80)\n)\n\n# Calculate differences\ndata$difference &lt;- data$support - data$oppose\n\n# Reshape data for diverging bars\ndata_long &lt;- data |&gt;\n  pivot_longer(\n    cols = c(support, oppose),\n    names_to = \"response\",\n    values_to = \"percentage\"\n  ) |&gt;\n  mutate(\n    percentage = ifelse(response == \"oppose\", -percentage, percentage)\n  )\n\n# Create diverging bar chart\nggplot(data_long, aes(x = category, y = percentage, fill = percentage)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_distiller(\n    palette = \"RdBu\",\n    direction = 1,\n    limits = c(-100, 100),\n    breaks = c(-100, -50, 0, 50, 100),\n    labels = c(\"100%\", \"50%\", \"0%\", \"50%\", \"100%\")\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_line(color = \"gray90\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14)\n  ) +\n  labs(\n    title = \"Public Support for Health Measures\",\n    subtitle = \"Percentage support vs. oppose\",\n    x = \"\",\n    y = \"Percentage\",\n    fill = \"Support/Oppose\"\n  )\n\n\n\n\n\n\n\n# Create a lollipop chart version with annotations\nggplot(data_long, aes(x = category, y = percentage, color = percentage)) +\n  geom_segment(aes(xend = category, yend = 0), linewidth = 1) +\n  geom_point(size = 4) +\n  # Add difference annotations\n  geom_text(\n    data = data,\n    aes(x = category, y = 0, label = sprintf(\"%+d%%\", difference)),\n    hjust = 1.2,\n    vjust = -1.0,\n    size = 3.5,\n    inherit.aes = FALSE  # Don't inherit aesthetics from the main plot\n  ) +\n  scale_color_distiller(\n    palette = \"RdBu\",\n    direction = 1,\n    limits = c(-100, 100),\n    breaks = c(-100, -50, 0, 50, 100),\n    labels = c(\"100%\", \"50%\", \"0%\", \"50%\", \"100%\")\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_line(color = \"gray90\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14)\n  ) +\n  labs(\n    title = \"Public Support for Health Measures\",\n    subtitle = \"Percentage support vs. oppose (Lollipop Chart)\",\n    x = \"\",\n    y = \"Percentage\",\n    color = \"Support/Oppose\"\n  )"
  },
  {
    "objectID": "colors_and_accessibility.html#viridis-perceptually-uniform-colors",
    "href": "colors_and_accessibility.html#viridis-perceptually-uniform-colors",
    "title": "Color Systems in R",
    "section": "Viridis: Perceptually Uniform Colors",
    "text": "Viridis: Perceptually Uniform Colors\nPerceptually uniform colors are designed to:\n\nAllow humans to interpret magnitude differences using color (E.g., equal steps in data result in equal perceptual differences in color)\nEnsure that color transitions appear evenly spaced, so a change from 0 to 10 looks as distinct as 10 to 20\nRemain interpretable when converted to grayscale (useful for printing and accessibility)\n\nThe viridis package provides colorblind-friendly palettes that are perceptually uniform.\n\n# Create a function to display viridis palettes\ndisplay_viridis_pal &lt;- function(option, n) {\n  colors &lt;- viridis(n, option = option)\n  plot(1:n, rep(1, n),\n       col = colors,\n       pch = 19,\n       cex = 5,\n       xlab = \"\",\n       ylab = \"\",\n       axes = FALSE)\n  title(option)\n}\n\n# Display viridis palettes\npar(mfrow = c(2, 2))\ndisplay_viridis_pal(\"viridis\", 9)     # Default viridis\ndisplay_viridis_pal(\"magma\", 9)       # Magma\ndisplay_viridis_pal(\"plasma\", 9)      # Plasma\ndisplay_viridis_pal(\"inferno\", 9)     # Inferno\n\n\n\n\n\n\n\n\nHere’s how viridis can be used in ggplot2:\n\n# Create example data with more categories\ndf &lt;- data.frame(\n  category = letters[1:8],\n  value = c(10, 20, 15, 25, 30, 22, 18, 27)\n)\n\n# Plot with viridis colors\nggplot(df, aes(x = category, y = value, fill = value)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis() +\n  labs(title = \"Viridis Colors\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n## Plot with magma colors\n# Create simulated temperature data for different cities\nset.seed(789)\nn_samples &lt;- 1000\ncities &lt;- c(\"Phoenix\", \"Miami\", \"Los Angeles\", \"New York\", \"Chicago\", \"Seattle\")\nridge_data &lt;- data.frame(\n  city = rep(cities, each = n_samples),\n  temperature = c(\n    rnorm(n_samples, mean = 95, sd = 8),   # Phoenix\n    rnorm(n_samples, mean = 85, sd = 6),   # Miami\n    rnorm(n_samples, mean = 75, sd = 5),   # Los Angeles\n    rnorm(n_samples, mean = 65, sd = 7),   # New York\n    rnorm(n_samples, mean = 55, sd = 8),   # Chicago\n    rnorm(n_samples, mean = 45, sd = 6)    # Seattle\n  )\n)\n\n# Calculate mean temperatures and sort cities\ncity_means &lt;- ridge_data |&gt;\n  group_by(city) |&gt;\n  summarize(mean_temp = mean(temperature)) |&gt;\n  arrange(desc(mean_temp))\n\n# Reorder the factor levels of city based on mean temperature\nridge_data$city &lt;- factor(ridge_data$city, levels = city_means$city)\n\n# Create ridgeline plot of temperature distributions\nggplot(ridge_data, aes(x = temperature, y = city, fill = after_stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01,\n    gradient_lwd = 0.5\n  ) +\n  scale_fill_viridis_c(\n    option = \"plasma\",\n    direction = 1,\n    limits = c(20, 120),\n    breaks = seq(20, 120, 20),\n    labels = paste0(seq(20, 120, 20), \"°F\")\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_line(color = \"gray90\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14)\n  ) +\n  labs(\n    title = \"Summer Temperature Distributions\",\n    subtitle = \"Daily high temperatures by city (sorted by mean temperature)\",\n    x = \"Temperature (°F)\",\n    y = \"\",\n    fill = \"Temperature\"\n  )"
  },
  {
    "objectID": "colors_and_accessibility.html#tailwind-one-option-for-custom-design",
    "href": "colors_and_accessibility.html#tailwind-one-option-for-custom-design",
    "title": "Color Systems in R",
    "section": "Tailwind: One Option for Custom Design",
    "text": "Tailwind: One Option for Custom Design\nTailwind’s color palette offers another approach to visualization design. Here’s an example of how you might use it to create visual hierarchy and data representation:\n\n# Source the Tailwind colors\nsource(\"colors.R\")\n\n# Create example data with categories and subcategories\ndf &lt;- data.frame(\n  category = rep(c(\"Primary Care\", \"Emergency Care\", \"Specialty Care\"), each = 3),\n  subcategory = rep(c(\"Urban\", \"Suburban\", \"Rural\"), times = 3),\n  value = c(85, 75, 65, 90, 80, 70, 95, 85, 75)  # Healthcare access percentages\n)\n\n# One way to create visual hierarchy with grays\ncustom_theme &lt;- theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    plot.background = element_rect(fill = colors[[\"gray\"]][[\"50\"]], color = NA),\n    panel.background = element_rect(fill = colors[[\"gray\"]][[\"50\"]], color = NA),\n    axis.text = element_text(color = colors[[\"gray\"]][[\"700\"]]),\n    axis.title = element_text(color = colors[[\"gray\"]][[\"900\"]]),\n    plot.title = element_text(color = colors[[\"gray\"]][[\"900\"]], face = \"bold\"),\n    legend.text = element_text(color = colors[[\"gray\"]][[\"700\"]]),\n    legend.title = element_text(color = colors[[\"gray\"]][[\"900\"]])\n  )\n\n# One approach to using blues for data representation\nblue_palette &lt;- c(\n  colors[[\"blue\"]][[\"300\"]],\n  colors[[\"blue\"]][[\"500\"]],\n  colors[[\"blue\"]][[\"700\"]]\n)\n\n# The resulting visualization\nggplot(df, aes(x = category, y = value, fill = subcategory)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = blue_palette) +\n  labs(\n    title = \"Healthcare Access by Care Type and Location\",\n    subtitle = \"Percentage of population with access to healthcare services\",\n    x = NULL,\n    y = \"Access Rate (%)\",\n    fill = \"Location\"\n  ) +\n  custom_theme"
  },
  {
    "objectID": "colors_and_accessibility.html#resources",
    "href": "colors_and_accessibility.html#resources",
    "title": "Color Systems in R",
    "section": "Resources",
    "text": "Resources\n\nRColorBrewer Documentation\nColorBrewer Website\nViridis Package\nColor Accessibility Tools\nTailwind Colors"
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html",
    "href": "ggplot_themes_and_staying_dry.html",
    "title": "ggplot Themes and Staying DRY",
    "section": "",
    "text": "You may have noticed that as we create more and more notebooks and plots, we repeat a lot of code.\nYou might also notice that a lot of styling decisions in ggplot require being repeated across multiple plots.\nIn this example we will give you some techniques for making your code more DRY."
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#staying-dry-dont-repeat-yourself",
    "href": "ggplot_themes_and_staying_dry.html#staying-dry-dont-repeat-yourself",
    "title": "ggplot Themes and Staying DRY",
    "section": "",
    "text": "You may have noticed that as we create more and more notebooks and plots, we repeat a lot of code.\nYou might also notice that a lot of styling decisions in ggplot require being repeated across multiple plots.\nIn this example we will give you some techniques for making your code more DRY."
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#scaffolding",
    "href": "ggplot_themes_and_staying_dry.html#scaffolding",
    "title": "ggplot Themes and Staying DRY",
    "section": "Scaffolding",
    "text": "Scaffolding\n\nWhen you notice that you are repeating a lot of code, a nice and easy pattern is to create a “scaffold” file (e.g,. `scaffold.R) that contains the code you are repeating\nIn this file you could include your theme.R file, but also common library imports, functions, and so on.\nAll you then need to do is put something like source(\"scaffold.R\") in your setup block."
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#theme-file",
    "href": "ggplot_themes_and_staying_dry.html#theme-file",
    "title": "ggplot Themes and Staying DRY",
    "section": "Theme File",
    "text": "Theme File\n\nI have created a scaffold.R file and a theme.R file in the examples directory\nThe scaffold.R file loads libraries and sources the theme.R file\nRun it in a set up block, like below:\n\n\nsource(here::here(\"examples\", \"scaffold.R\"))\n\n\nNow you don’t need to repeat yourself at the top of every new notebook."
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#using-the-theme",
    "href": "ggplot_themes_and_staying_dry.html#using-the-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Using The Theme",
    "text": "Using The Theme\n\nThis file has a theme_jhu() functions\nIt extends theme_minimal() and adds a few customizations that accord with the https://brand.jhu.edu/visual-identity/colors/"
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#make-a-plot",
    "href": "ggplot_themes_and_staying_dry.html#make-a-plot",
    "title": "ggplot Themes and Staying DRY",
    "section": "Make A Plot",
    "text": "Make A Plot\nBelow we’ll make a simple plot and quickly encounter our usual issues with styling.\n\np &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Fuel Efficiency by Engine Size\",\n    caption = \"Source: mpg dataset\",\n    x = \"Engine Size (liters)\",\n    y = \"Fuel Efficiency (mpg)\"\n  )\n\np"
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#using-a-theme",
    "href": "ggplot_themes_and_staying_dry.html#using-a-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Using A Theme",
    "text": "Using A Theme\n\nIn our theme.R file we have a function called theme_jhu() that looks like this\nNotice how annoying it would be to include this in every plot.\n\n\ntheme_jhu &lt;- function() {\n  theme_minimal(base_family = \"Tahoma\") +\n    theme(\n      plot.title = element_text(\n        family = \"Georgia\", size = 18, hjust = 0.5, face = \"bold\",\n        margin = margin(b = 10)\n      ),\n      plot.subtitle = element_text(\n        hjust = 0.5, family = \"Tahoma\", size = 12,\n        margin = margin(t = 5, b = 15)\n      ),\n      plot.caption = element_text(\n        family = \"Tahoma\", size = 10, color = colors$Gray3,\n        margin = margin(t = 15)\n      ),\n      axis.text.y = element_text(\n        family = \"Tahoma\", size = 9, color = colors$Gray5\n      ),\n      axis.text.x = element_text(\n        family = \"Tahoma\", size = 9, color = colors$Gray5\n      ),\n      axis.title.y = element_text(\n        family = \"Tahoma\", size = 11,\n        margin = margin(r = 15),\n        face = \"bold\"\n      ),\n      axis.title.x = element_text(\n        family = \"Tahoma\", size = 11,\n        margin = margin(t = 15),\n        face = \"bold\"\n      ),\n      axis.line.x = element_line(color = colors$Gray1, linewidth = 0.5),\n      axis.line.y = element_line(color = colors$Gray1, linewidth = 0.5),\n      panel.grid.major.x = element_line(\n        color = colors$Gray1, linewidth = 0.5, linetype = \"solid\"\n      ),\n      panel.grid.minor = element_blank()\n    ) +\n    update_geom_defaults(\"smooth\", list(\n      color = colors$Blue,\n      fill = colors$Gray2\n    ))\n}"
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#applying-the-theme",
    "href": "ggplot_themes_and_staying_dry.html#applying-the-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Applying The Theme",
    "text": "Applying The Theme\n\nAll we need to do to apply the theme is add + theme_jhu() to the end of the plot code\nThis will apply the theme to the plot\nNotice how we now have JHU fonts, better spacing, etc.\n\n\np + theme_jhu()"
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#types-of-plots",
    "href": "ggplot_themes_and_staying_dry.html#types-of-plots",
    "title": "ggplot Themes and Staying DRY",
    "section": "Types of Plots",
    "text": "Types of Plots\n\nSometimes you want to apply a theme to a specific type of plot\nIn theme.R we have a function called theme_jhu_bar() that looks like this\n\n\ntheme_jhu_bar &lt;- function() {\n  theme_jhu() +\n    update_geom_defaults(\"bar\", list(fill = colors$HopkinsBlue)) +\n    theme(\n      axis.line.x = element_blank(),\n      axis.line.y = element_blank(),\n      axis.ticks = element_line(color = colors$Gray1, linewidth = 0.25),\n      axis.ticks.length = unit(0.2, \"cm\"),\n      panel.border = element_blank(),\n      panel.grid.major.x = element_blank(),\n      panel.grid.minor = element_blank()\n    )\n}"
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#make-a-bar-plot",
    "href": "ggplot_themes_and_staying_dry.html#make-a-bar-plot",
    "title": "ggplot Themes and Staying DRY",
    "section": "Make A Bar Plot",
    "text": "Make A Bar Plot\n\nLet’s make a bar plot\n\n\np_bar &lt;- ggplot(mpg, aes(x = class)) +\n  geom_bar() + \n  labs(\n    title = \"Fuel Efficiency by Engine Size\",\n    caption = \"Source: mpg dataset\",\n    x = \"Class of Vehicle\"\n  )\n\np_bar"
  },
  {
    "objectID": "ggplot_themes_and_staying_dry.html#applying-the-bar-theme",
    "href": "ggplot_themes_and_staying_dry.html#applying-the-bar-theme",
    "title": "ggplot Themes and Staying DRY",
    "section": "Applying The Bar Theme",
    "text": "Applying The Bar Theme\nAll we need to do to apply the bar theme is add + theme_jhu_bar() to the end of the plot code\n\np_bar + theme_jhu_bar()"
  },
  {
    "objectID": "git_troubleshooting.html",
    "href": "git_troubleshooting.html",
    "title": "Git Troubleshooting",
    "section": "",
    "text": "Several students had trouble pushing to GitHub with their work. Please try the following steps:"
  },
  {
    "objectID": "git_troubleshooting.html#github-issues",
    "href": "git_troubleshooting.html#github-issues",
    "title": "Git Troubleshooting",
    "section": "",
    "text": "Several students had trouble pushing to GitHub with their work. Please try the following steps:"
  },
  {
    "objectID": "git_troubleshooting.html#using-the-command-line",
    "href": "git_troubleshooting.html#using-the-command-line",
    "title": "Git Troubleshooting",
    "section": "Using The Command Line",
    "text": "Using The Command Line\nIf you having issues with RStudio’s Git functionality, you can try using the command line. Click “Terminal” in the bottom left pane or select “Tools” &gt; “Shell” &gt; “Terminal” from the menu.\nThere you can use the following commands:\ngit add .\ngit commit -m \"Your message\"\ngit push\nIn general, I find that using the command line is more reliable than using RStudio’s Git functionality. It is also faster."
  },
  {
    "objectID": "git_troubleshooting.html#authentication-issues",
    "href": "git_troubleshooting.html#authentication-issues",
    "title": "Git Troubleshooting",
    "section": "Authentication Issues",
    "text": "Authentication Issues\nIf you are having trouble getting GitHub to accept your push after providing it credentials, try the following:\n\nGo to your GitHub account settings.\nClick “Developer settings”\nClick “Personal access tokens”\nClick “Generate new token (Classic)””\nGive it a name and description\nSelect all boxes on “repo”\nClick “Generate token”\nCopy the token\nUse the token as your GitHub password when you push"
  },
  {
    "objectID": "git_troubleshooting.html#remote-issues",
    "href": "git_troubleshooting.html#remote-issues",
    "title": "Git Troubleshooting",
    "section": "Remote Issues",
    "text": "Remote Issues\nGitHub is a git remote. A remote is a copy of a repository that is stored on a different server. If GitHub is your primary remote, it is conventional to call it origin.\nEnsure your remote is set correctly.\nFirst, check out which if any remotes are set:\ngit remote -v\nIf you see nothing, then run:\ngit remote add origin https://github.com/YOUR-USERNAME/data-viz-summer-25.git\nIf you see an unfamiliar remote, you can remove it with:\ngit remote remove origin\nIf you see a remote, you can set it to the correct one with:\ngit remote set-url origin https://github.com/YOUR-USERNAME/data-viz-summer-25.git\nNote that the above call GitHub the origin remote."
  },
  {
    "objectID": "git_troubleshooting.html#branch-issues",
    "href": "git_troubleshooting.html#branch-issues",
    "title": "Git Troubleshooting",
    "section": "Branch Issues",
    "text": "Branch Issues\nSome installations will default to a master branch. Community standards are moving away from the term master branch in favor of a main branch. If you are using master, you can change it to main with:\ngit branch -m master main\nSome commands I have used assume an origin remote and a main branch. If you are using something else, you will need to adjust the commands accordingly.\nYou can then push or pull like so, assuming an origin remote is set:\ngit push origin main\ngit pull origin main"
  },
  {
    "objectID": "git_troubleshooting.html#errors-about-the-materials-in-your-repo-being-more-up-to-date",
    "href": "git_troubleshooting.html#errors-about-the-materials-in-your-repo-being-more-up-to-date",
    "title": "Git Troubleshooting",
    "section": "Errors About The Materials In Your Repo Being More Up To Date",
    "text": "Errors About The Materials In Your Repo Being More Up To Date\nIf you are receiving errors about the materials in your repo being more up to date than your local copy, you can try the following.\nIf you want to pull the latest materials from the repository, you can do so with the following command:\ngit pull origin main\nThis assumes you are using the main branch and an origin remote.\nIt is good to ensure you have saved your work before pulling.\nIf you cannot push your work up, and you are absolutely certain you’ve never pushed any work up, you can force the materials in on the command line:\ngit push origin main --force\nThis will overwrite the materials in the repository with your current work."
  },
  {
    "objectID": "making_dags.html",
    "href": "making_dags.html",
    "title": "Making DAGs with ggdag",
    "section": "",
    "text": "Directed Acyclic Graphs (DAGs) are visual representations of causal relationships. They help us:\n\nMake our assumptions about causality explicit\nIdentify which variables to control for\nUnderstand potential sources of bias\nCommunicate causal models clearly\n\nThe ggdag package makes it easy to create beautiful DAGs using familiar ggplot2 syntax."
  },
  {
    "objectID": "making_dags.html#introduction",
    "href": "making_dags.html#introduction",
    "title": "Making DAGs with ggdag",
    "section": "",
    "text": "Directed Acyclic Graphs (DAGs) are visual representations of causal relationships. They help us:\n\nMake our assumptions about causality explicit\nIdentify which variables to control for\nUnderstand potential sources of bias\nCommunicate causal models clearly\n\nThe ggdag package makes it easy to create beautiful DAGs using familiar ggplot2 syntax."
  },
  {
    "objectID": "making_dags.html#basic-dag-structure",
    "href": "making_dags.html#basic-dag-structure",
    "title": "Making DAGs with ggdag",
    "section": "Basic DAG Structure",
    "text": "Basic DAG Structure\n\nSimple DAG: X → Y\nThe simplest DAG shows one variable causing another.\n\n# Create a basic DAG: X causes Y\nsimple_dag &lt;- dagify(\n  Y ~ X\n)\n\n# Visualize it\nggdag(simple_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\nThe arrow shows that X causes Y (X → Y)."
  },
  {
    "objectID": "making_dags.html#the-three-elemental-confounds",
    "href": "making_dags.html#the-three-elemental-confounds",
    "title": "Making DAGs with ggdag",
    "section": "The Three Elemental Confounds",
    "text": "The Three Elemental Confounds\nRichard McElreath’s Statistical Rethinking (2nd ed., 2020) is a great resource on modeling in general, but also on causal inference. He breaks DAGs down into three fundamental causal structures that create confounding. Every complex DAG is built from combinations of these three patterns.\nThese examples follow McElreath’s framework, also covered in his excellent lecture series on YouTube.\n\n1. The Fork (Common Cause)\n\nfork_dag &lt;- dagify(\n  Y ~ Z,\n  X ~ Z,\n  coords = list(\n    x = c(X = 1, Y = 3, Z = 2),\n    y = c(X = 1, Y = 1, Z = 2)\n  )\n)\n\nggdag(fork_dag) +\n  theme_dag() +\n  labs(title = \"Fork: Z is a common cause of X and Y\")\n\n\n\n\n\n\n\n\nStrategy: Control for Z to block the non-causal path.\nZ is a confounder: it causes both X and Y. If you don’t control for Z, the association between X and Y includes both the direct causal effect AND the indirect path through Z.\n\n\n2. The Chain (Mediation)\n\nchain_dag &lt;- dagify(\n  Y ~ M,\n  M ~ X,\n  coords = list(\n    x = c(X = 1, M = 2, Y = 3),\n    y = c(X = 1, M = 1, Y = 1)\n  )\n)\n\nggdag(chain_dag) +\n  theme_dag() +\n  labs(title = \"Chain: M mediates the effect of X on Y\")\n\n\n\n\n\n\n\n\nStrategy: Don’t control for M if you want the total effect of X on Y.\nM is a mediator. It’s on the causal pathway from X to Y. Controlling for M blocks this path and gives you only the direct effect of X on Y, ignoring the indirect effect through M.\n\n\n3. The Inverted Fork (Collider)\n\ninverted_fork_dag &lt;- dagify(\n  C ~ X + Y,\n  coords = list(\n    x = c(X = 1, Y = 3, C = 2),\n    y = c(X = 2, Y = 2, C = 1)\n  )\n)\n\nggdag(inverted_fork_dag) +\n  theme_dag() +\n  labs(title = \"Inverted Fork: C is a collider\")\n\n\n\n\n\n\n\n\nStrategy: Never control for C. It introduces an association between X and Y that isn’t “real.”\nC is a collider: both X and Y cause it. Controlling for C creates a spurious association between X and Y even when there’s no causal relationship."
  },
  {
    "objectID": "making_dags.html#customizing-dag-appearance",
    "href": "making_dags.html#customizing-dag-appearance",
    "title": "Making DAGs with ggdag",
    "section": "Customizing DAG Appearance",
    "text": "Customizing DAG Appearance\n\nAdding Labels\nUse descriptive names instead of single letters:\n\n# Create DAG with meaningful labels\nlabeled_dag &lt;- dagify(\n  outcome ~ exposure + confounder,\n  exposure ~ confounder,\n  labels = c(\n    \"exposure\" = \"Treatment\",\n    \"outcome\" = \"Health\",\n    \"confounder\" = \"Age\"\n  )\n)\n\nggdag(labeled_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nCustom Positioning\nControl where nodes appear using coordinates:\n\n# Specify exact coordinates for nodes\ncoords &lt;- list(\n  x = c(exposure = 1, outcome = 3, confounder = 2),\n  y = c(exposure = 1, outcome = 1, confounder = 2)\n)\n\npositioned_dag &lt;- dagify(\n  outcome ~ exposure + confounder,\n  exposure ~ confounder,\n  coords = coords,\n  labels = c(\n    \"exposure\" = \"Treatment\",\n    \"outcome\" = \"Health\",\n    \"confounder\" = \"Age\"\n  )\n)\n\nggdag(positioned_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nStyling with ggplot2\nSince ggdag uses ggplot2, you can customize with familiar syntax:\n\nggdag(positioned_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  ) +\n  labs(title = \"Causal Model: Effect of Treatment on Health\")"
  },
  {
    "objectID": "making_dags.html#practical-example-healthcare-access",
    "href": "making_dags.html#practical-example-healthcare-access",
    "title": "Making DAGs with ggdag",
    "section": "Practical Example: Healthcare Access",
    "text": "Practical Example: Healthcare Access\nLet’s build a realistic DAG for healthcare access and outcomes:\n\nhealthcare_dag &lt;- dagify(\n  health_outcome ~ healthcare_access + age + comorbidities + insurance,\n  healthcare_access ~ income + insurance + distance + provider_quality,\n  insurance ~ income + job_type,\n  income ~ education + age,\n  comorbidities ~ age + income,\n  provider_quality ~ geography,\n  distance ~ geography,\n\n  coords = list(\n    x = c(\n      health_outcome = 5,\n      healthcare_access = 3,\n      age = 1,\n      comorbidities = 3,\n      insurance = 2,\n      income = 1,\n      education = 0.5,\n      job_type = 0.5,\n      distance = 2.5,\n      provider_quality = 2.5,\n      geography = 1.5\n    ),\n    y = c(\n      health_outcome = 3,\n      healthcare_access = 3,\n      age = 4,\n      comorbidities = 4,\n      insurance = 2,\n      income = 2,\n      education = 1,\n      job_type = 3,\n      distance = 1,\n      provider_quality = 2,\n      geography = 1\n    )\n  ),\n\n  labels = c(\n    \"health_outcome\" = \"Health\\nOutcome\",\n    \"healthcare_access\" = \"Healthcare\\nAccess\",\n    \"age\" = \"Age\",\n    \"comorbidities\" = \"Comorbidities\",\n    \"insurance\" = \"Insurance\",\n    \"income\" = \"Income\",\n    \"education\" = \"Education\",\n    \"job_type\" = \"Job Type\",\n    \"distance\" = \"Distance to\\nProvider\",\n    \"provider_quality\" = \"Provider\\nQuality\",\n    \"geography\" = \"Geography\"\n  ),\n\n  exposure = \"healthcare_access\",\n  outcome = \"health_outcome\"\n)\n\nggdag(healthcare_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11)\n  ) +\n  labs(\n    title = \"Causal Model: Healthcare Access and Health Outcomes\",\n    subtitle = \"Red = Exposure | Blue = Outcome\"\n  )"
  },
  {
    "objectID": "making_dags.html#identifying-adjustment-sets",
    "href": "making_dags.html#identifying-adjustment-sets",
    "title": "Making DAGs with ggdag",
    "section": "Identifying Adjustment Sets",
    "text": "Identifying Adjustment Sets\nThe ggdag package can automatically identify which variables to control for:\n\n# Show what to adjust for\nggdag_adjustment_set(healthcare_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  labs(title = \"What variables should we control for?\")\n\n\n\n\n\n\n\n\nGreen variables are in the adjustment set - control for these to estimate the causal effect."
  },
  {
    "objectID": "making_dags.html#dag-analysis-paths",
    "href": "making_dags.html#dag-analysis-paths",
    "title": "Making DAGs with ggdag",
    "section": "DAG Analysis: Paths",
    "text": "DAG Analysis: Paths\nVisualize all paths from exposure to outcome:\n\n# Show all paths\nggdag_paths(healthcare_dag, text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  labs(title = \"All Paths from Healthcare Access to Health Outcome\")"
  },
  {
    "objectID": "making_dags.html#dag-analysis-backdoor-paths",
    "href": "making_dags.html#dag-analysis-backdoor-paths",
    "title": "Making DAGs with ggdag",
    "section": "DAG Analysis: Backdoor Paths",
    "text": "DAG Analysis: Backdoor Paths\nIdentify confounding (backdoor) paths:\n\n# Show backdoor paths that create confounding\nggdag_parents(healthcare_dag, \"health_outcome\", text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  labs(title = \"What directly affects Health Outcome?\")"
  },
  {
    "objectID": "making_dags.html#tips-for-making-dags",
    "href": "making_dags.html#tips-for-making-dags",
    "title": "Making DAGs with ggdag",
    "section": "Tips for Making DAGs",
    "text": "Tips for Making DAGs\n\nStart simple: Begin with just exposure -&gt; outcome\nAdd confounders: What affects both?\nConsider mediators: What’s on the causal pathway?\nWatch for colliders: Don’t condition on them!\nGet feedback: Show your DAG to domain experts\nIterate: DAGs evolve as you learn more\n\nBooks:\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press.\n\nLecture videos on YouTube\n\nHernán, M. A., & Robins, J. M. Causal Inference: What If. Free online\n\nSoftware:\n\nggdag documentation - R package for drawing DAGs\ndagitty.net - Browser-based interactive DAG tool"
  },
  {
    "objectID": "prams_1_data_prep.html",
    "href": "prams_1_data_prep.html",
    "title": "Data Preparation Example",
    "section": "",
    "text": "We use CDC’s PRAMStat data for 2011."
  },
  {
    "objectID": "prams_1_data_prep.html#data-source",
    "href": "prams_1_data_prep.html#data-source",
    "title": "Data Preparation Example",
    "section": "",
    "text": "We use CDC’s PRAMStat data for 2011."
  },
  {
    "objectID": "prams_1_data_prep.html#note",
    "href": "prams_1_data_prep.html#note",
    "title": "Data Preparation Example",
    "section": "Note",
    "text": "Note\nIn this file, I use exposition to explain what I am doing and what I am thinking. In this context, this is an unknown data set we’re working with and I am trying to teach others.\nFor a project orreport, you would want to first read the data file documentation and codebook first to understand the data. Your commentary in the notebook should be targeted to the audience, conveying everything required for people to understand the steps you took and why you took them.\nThis data is aggregated at the location level and comes to us in “long” format."
  },
  {
    "objectID": "prams_1_data_prep.html#loading-required-libraries",
    "href": "prams_1_data_prep.html#loading-required-libraries",
    "title": "Data Preparation Example",
    "section": "Loading Required Libraries",
    "text": "Loading Required Libraries\nWe first need to load in the libraries we’ll use.\nWe will use:\n\nreadr to load in the data ( I prefer readr over the built-in read.csv function because it is faster and has better default settings for avoiding common issues with CSVs.)\ndplyr to clean data.\n\nkableExtra is used to make tables look nice.\nDT to make tables interactive and get nice pagination on long tables.\ntidyr to pivot the data to wide format.\n\n\n# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"DT\", \"kableExtra\", \"readr\", \"tidyr\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(DT)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(tidyr)\n\nTo hide this code from the output, I typically prepend the above code with.\n#| echo: false\n#| message: false\nHere, for illustration, I hide messages but not the code block."
  },
  {
    "objectID": "prams_1_data_prep.html#read-and-explore-the-data",
    "href": "prams_1_data_prep.html#read-and-explore-the-data",
    "title": "Data Preparation Example",
    "section": "Read and Explore the Data",
    "text": "Read and Explore the Data\nHere we read the data using the readr::read_csv function. Note that we use here::here to ensure the path to the data is correct relative to the project root.\nWe then use the glimpse function to take a look at the data. Glimpse tells us the number of rows and columns in the data, the names of the columns, the type of each column, and the first few rows of the data.\nI create a new object df_clean to store the cleaned data. For now, we’ll just copy the original data to this object. This is a good habit to get into, as it allows you to keep the original data for reference and to easily revert to it if needed.\n\n# Load the data using here::here to ensure correct path resolution\ndf &lt;- read_csv(here::here(\"data\", \"raw\", \"cdc_PRAMStat_Data_for_2011_20250610.csv\"))\n\n# Take a look at the data structure\ndf |&gt; glimpse()\n\nRows: 520,381\nColumns: 27\n$ Year                       &lt;dbl&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2…\n$ LocationAbbr               &lt;chr&gt; \"AR\", \"AR\", \"CO\", \"CO\", \"DE\", \"DE\", \"GA\", \"…\n$ LocationDesc               &lt;chr&gt; \"Arkansas\", \"Arkansas\", \"Colorado\", \"Colora…\n$ Class                      &lt;chr&gt; \"Prenatal Care\", \"Prenatal Care\", \"Prenatal…\n$ Topic                      &lt;chr&gt; \"Prenatal Care - Content\", \"Prenatal Care -…\n$ Question                   &lt;chr&gt; \"During any of your prenatal care visits  d…\n$ DataSource                 &lt;chr&gt; \"PRAMS\", \"PRAMS\", \"PRAMS\", \"PRAMS\", \"PRAMS\"…\n$ Response                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Data_Value_Unit            &lt;chr&gt; \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\"…\n$ Data_Value_Type            &lt;chr&gt; \"Percentage\", \"Percentage\", \"Percentage\", \"…\n$ Data_Value                 &lt;dbl&gt; 52.0, 52.0, 39.0, 35.0, 22.0, 19.0, 71.0, 3…\n$ Data_Value_Footnote_Symbol &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ Data_Value_Footnote        &lt;chr&gt; \"Missing includes not applicable, don't kno…\n$ Data_Value_Std_Err         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Low_Confidence_Limit       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ High_Confidence_Limit      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Sample_Size                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Break_Out                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Break_Out_Category         &lt;chr&gt; \"Maternal Age - 18 to 44 years in groupings…\n$ Geolocation                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ClassId                    &lt;chr&gt; \"CLA11\", \"CLA11\", \"CLA11\", \"CLA11\", \"CLA11\"…\n$ TopicId                    &lt;chr&gt; \"TOP19\", \"TOP19\", \"TOP19\", \"TOP19\", \"TOP19\"…\n$ QuestionId                 &lt;chr&gt; \"QUO333\", \"QUO67\", \"QUO333\", \"QUO67\", \"QUO3…\n$ LocationId                 &lt;dbl&gt; 5, 5, 8, 8, 10, 10, 13, 15, 15, 25, 25, 24,…\n$ BreakOutId                 &lt;chr&gt; \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\"…\n$ BreakOutCategoryid         &lt;chr&gt; \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\", \"BOC17\"…\n$ ResponseId                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\ndf_clean &lt;- df"
  },
  {
    "objectID": "prams_1_data_prep.html#tabbing-out-data",
    "href": "prams_1_data_prep.html#tabbing-out-data",
    "title": "Data Preparation Example",
    "section": "Tabbing Out Data",
    "text": "Tabbing Out Data\nAlways read the codebook first to understand the data. Nevertheless, it’s still helpful to tab out the data to get a sense of the variables and their values.\nLet’s first look at the topics availble\n\ntable(df$Topic) |&gt; kable()\n\n\n\n\nVar1\nFreq\n\n\n\n\nAbuse - Physical\n6799\n\n\nAlcohol Use\n17875\n\n\nAssisted Reproduction\n1964\n\n\nBreastfeeding\n8829\n\n\nContraception - Conception\n32460\n\n\nContraception - Postpartum\n26716\n\n\nDelivery - Method\n999\n\n\nDelivery - Payment\n14802\n\n\nHIV Test\n9975\n\n\nHospital Length of Stay\n9564\n\n\nHousehold Characteristics\n3469\n\n\nIncome\n8143\n\n\nInfant Health Care\n6377\n\n\nInjury Prevention\n1692\n\n\nInsurance Coverage\n2997\n\n\nMaternal Health Care\n2220\n\n\nMedicaid\n11770\n\n\nMental Health\n5994\n\n\nMorbidity - Infant\n5697\n\n\nMorbidity - Maternal\n35790\n\n\nMultivitamin Use\n12185\n\n\nObesity\n11517\n\n\nOral Health\n10543\n\n\nPreconception Health\n21977\n\n\nPreconception Morbidity\n7142\n\n\nPregnancy History\n12875\n\n\nPregnancy Intention\n14249\n\n\nPregnancy Outcome\n2928\n\n\nPregnancy Recognition\n2996\n\n\nPrenatal Care  Provider\n999\n\n\nPrenatal Care - Content\n36688\n\n\nPrenatal Care - Initiation\n8991\n\n\nPrenatal Care - Location\n1692\n\n\nPrenatal Care - Payment\n14846\n\n\nPrenatal Care - Visits\n7930\n\n\nSleep Behaviors\n19536\n\n\nSmoke Exposure\n2986\n\n\nStress\n56867\n\n\nTobacco Use\n56440\n\n\nWIC\n2862\n\n\n\n\n\nSuppose we’re intersted in the relationship between Alcohol Use and Mental Health.\nLet’s filter the data to include only those topics, and then look at the questions available.\n\ndf |&gt;\n  filter(Topic %in% c(\"Alcohol Use\", \"Mental Health\")) |&gt;\n  select(QuestionId, Topic, Question) |&gt;\n  distinct() |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\nQuestionId\nTopic\nQuestion\n\n\n\n\nQUO100\nAlcohol Use\nIndicator of drinking any alcohol during the past two years\n\n\nQUO271\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\n\n\nQUO8\nAlcohol Use\n(*PCH) Indicator of drinking alcohol during the three months before pregnancy\n\n\nQUO9\nAlcohol Use\nIndicator of whether mother reported having any alcoholic drinks during the last 3 months of pregnancy\n\n\nQUO99\nAlcohol Use\nChange in drinking between three months before pregnancy and last three months of pregnancy\n\n\nQUO133\nMental Health\nDuring the 3 months before you got pregnant with your new baby did you have depression?\n\n\nQUO219\nMental Health\n(*PCH) Indicator of whether mother reported frequent postpartum depressive symptoms (years 2009 - 2011)\n\n\nQUO232\nMental Health\nDuring the 3 months before you got pregnant with your new baby did you have anxiety?\n\n\nQUO97\nMental Health\nDid a doctor nurse or other health care worker talk with you about baby blues or postpartum depression during pregnancy or after your delivery?\n\n\nQUO133\nMental Health\nDuring the 3 months before you got pregnant with your new baby, did you have depression?\n\n\nQUO232\nMental Health\nDuring the 3 months before you got pregnant with your new baby, did you have anxiety?\n\n\nQUO97\nMental Health\nDid a doctor, nurse, or other health care worker talk with you about baby blues or postpartum depression during pregnancy or after your delivery?"
  },
  {
    "objectID": "prams_1_data_prep.html#filtering-data",
    "href": "prams_1_data_prep.html#filtering-data",
    "title": "Data Preparation Example",
    "section": "Filtering Data",
    "text": "Filtering Data\nSuppose we’re interested in the relationship between drinking in the first three months before pregnancy and reporting depression and anxiety. H\n\nquestion_ids_of_interest &lt;- c(\n  \"QUO271\", # Binge drinking in the first three months before pregnancy\n  \"QUO8\",   # Any alcohol use in the first three months before pregnancy\n  \"QUO133\", # Depression reported in the first three months before pregnancy\n  \"QUO232\"  # Anxiety reported in the first three months before pregnancy\n)\n\ndf_clean &lt;- df_clean |&gt;\n  filter(QuestionId %in% question_ids_of_interest)\n\ndf_clean |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nLocationAbbr\nLocationDesc\nClass\nTopic\nQuestion\nDataSource\nResponse\nData_Value_Unit\nData_Value_Type\nData_Value\nData_Value_Footnote_Symbol\nData_Value_Footnote\nData_Value_Std_Err\nLow_Confidence_Limit\nHigh_Confidence_Limit\nSample_Size\nBreak_Out\nBreak_Out_Category\nGeolocation\nClassId\nTopicId\nQuestionId\nLocationId\nBreakOutId\nBreakOutCategoryid\nResponseId\n\n\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNO\n%\nPercentage\n78.4\nNA\nNA\nNA\n76.7\n79.9\n7089\nLBW (&lt;=2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT1\nBOC1\nRES23\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nYES\n%\nPercentage\n21.6\nNA\nNA\nNA\n20.1\n23.3\n1832\nLBW (&lt;=2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT1\nBOC1\nRES40\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNO\n%\nPercentage\n76.8\nNA\nNA\nNA\n76.0\n77.5\n20588\nNBW (&gt;2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT2\nBOC1\nRES23\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nYES\n%\nPercentage\n23.2\nNA\nNA\nNA\n22.5\n24.0\n6099\nNBW (&gt;2500g)\nBirth Weight\nNA\nCLA9\nTOP2\nQUO271\n59\nBWT2\nBOC1\nRES40\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNA\n%\nPercentage\n877.0\n1\nMissing includes not applicable, don’t know, not recorded, no responses, and legitimate skips\nNA\nNA\nNA\nNA\nNA\nOn WIC during Pregnancy\nNA\nCLA9\nTOP2\nQUO271\n59\nBOC10\nBOC10\nNA\n\n\n2011\nPRAMS Total\nPRAMS Total\nMaternal Behavior/Health\nAlcohol Use\n(*PCH) Indicator of binge drinking (4+ drinks) during 3 months before pregnancy\nPRAMS\nNO\n%\nPercentage\n74.5\nNA\nNA\nNA\n73.5\n75.4\n13716\nNon-WIC\nOn WIC during Pregnancy\nNA\nCLA9\nTOP2\nQUO271\n59\nWIC1\nBOC10\nRES23\n\n\n\n\n\nI personally prefer to remove values from the data set we do not need. We can always add them back in. By using select we also reorder the columns to make it easier to take in.\n\ndf_clean &lt;- df_clean |&gt;\n  select(\n    QuestionId,\n    Topic,\n    Question,\n    Response,\n    Data_Value,\n    Data_Value_Unit,\n    Data_Value_Type,\n    LocationAbbr,\n    LocationDesc,\n    Break_Out,\n    Break_Out_Category\n  )"
  },
  {
    "objectID": "prams_1_data_prep.html#renaming-variables",
    "href": "prams_1_data_prep.html#renaming-variables",
    "title": "Data Preparation Example",
    "section": "Renaming Variables",
    "text": "Renaming Variables\nI prefer to rename variables to follow a consistent style and naming convention (e.g., lowercase, snake_case, noun first). Here I also rename variables to be more descriptive, such as with subgroups. Some would prefer keeping the original names, but I prefer the ergonomics of this approach.\nWe can use the DT::datatable function to view the data in our document in an interactive, filterable way. You’ll want to be careful with inline this data, but with public data, this is a good way to share data (or a selection of it) with others.\n\ndf_clean &lt;- df_clean |&gt;\n  rename_with(tolower) |&gt;\n  rename(\n    question_id = questionid,\n    value = data_value,\n    unit = data_value_unit,\n    type = data_value_type,\n    location_abbr = locationabbr,\n    location = locationdesc,\n    subgroup = break_out,\n    subgroup_cat = break_out_category\n  )\n\nDT::datatable(df_clean)"
  },
  {
    "objectID": "prams_1_data_prep.html#understanding-the-data",
    "href": "prams_1_data_prep.html#understanding-the-data",
    "title": "Data Preparation Example",
    "section": "Understanding The Data",
    "text": "Understanding The Data\nFirst note that this data still needs work. Even without the codebook, we can seee:\n\nThe response/value has “yes,” “no”, and NA values.\nThe location_abbr has both aggregations and location values.\nThe meaning of each row depends on the subgroup and subgroup_cat variables.\n\nFirst, let’s get a sense of the locations in the data\n\ndf_clean |&gt; \n  group_by(location) |&gt;\n  summarise(\n    location_abbr = first(location_abbr),\n    n = n()\n  ) |&gt;\nkable()\n\n\n\n\nlocation\nlocation_abbr\nn\n\n\n\n\nArkansas\nAR\n210\n\n\nColorado\nCO\n216\n\n\nDelaware\nDE\n438\n\n\nGeorgia\nGA\n210\n\n\nHawaii\nHI\n438\n\n\nMaine\nME\n216\n\n\nMaryland\nMD\n438\n\n\nMassachusetts\nMA\n216\n\n\nMichigan\nMI\n438\n\n\nMinnesota\nMN\n438\n\n\nMissouri\nMO\n432\n\n\nNebraska\nNE\n210\n\n\nNew Jersey\nNJ\n216\n\n\nNew Mexico\nNM\n210\n\n\nNew York (excluding NYC)\nNY\n210\n\n\nNew York City\nYC\n216\n\n\nOklahoma\nOK\n216\n\n\nOregon\nOR\n210\n\n\nPRAMS Total\nPRAMS Total\n432\n\n\nPennsylvania\nPA\n210\n\n\nRhode Island\nRI\n216\n\n\nUtah\nUT\n438\n\n\nVermont\nVT\n216\n\n\nWashington\nWA\n216\n\n\nWest Virginia\nWV\n438\n\n\nWisconsin\nWI\n438\n\n\nWyoming\nWY\n438\n\n\n\n\n\nWe see that some non-standard choices are made here: New York and New York City are both listed as locations. Also, aggregations are included under PRAMS Total.\nLets get an overview of the subgroup categories:\n\ndf_clean |&gt; \n  group_by(subgroup_cat, subgroup) |&gt;\n  summarise(n = n()) |&gt;\n  kable()\n\n\n\n\nsubgroup_cat\nsubgroup\nn\n\n\n\n\nAdequacy of Prenatal care\nADEQUATE PNC\n152\n\n\nAdequacy of Prenatal care\nINADEQUATE PNC\n152\n\n\nAdequacy of Prenatal care\nINTERMEDIATE PNC\n152\n\n\nAdequacy of Prenatal care\nUNKNOWN PNC\n152\n\n\nAdequacy of Prenatal care\nNA\n49\n\n\nBirth Weight\nLBW (&lt;=2500g)\n152\n\n\nBirth Weight\nNBW (&gt;2500g)\n152\n\n\nBirth Weight\nNA\n67\n\n\nIncome (years 2004 and beyond)\n$10,000 to $24,999\n152\n\n\nIncome (years 2004 and beyond)\n$25,000 to $49,999\n152\n\n\nIncome (years 2004 and beyond)\n$50,000 or more\n152\n\n\nIncome (years 2004 and beyond)\nLess than $10,000\n152\n\n\nIncome (years 2004 and beyond)\nNA\n49\n\n\nMarital Status\nMARRIED\n152\n\n\nMarital Status\nOTHER\n152\n\n\nMarital Status\nNA\n67\n\n\nMaternal Age (3 Levels)\n20-29 yrs\n152\n\n\nMaternal Age (3 Levels)\n30+ yrs\n152\n\n\nMaternal Age (3 Levels)\n&lt;20 yrs\n152\n\n\nMaternal Age (3 Levels)\nNA\n49\n\n\nMaternal Age (4 Levels)\n20-24 yrs\n152\n\n\nMaternal Age (4 Levels)\n25-34 yrs\n152\n\n\nMaternal Age (4 Levels)\n35+ yrs\n152\n\n\nMaternal Age (4 Levels)\n&lt;20 yrs\n152\n\n\nMaternal Age (4 Levels)\nNA\n67\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 18 - 24\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 25 - 29\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 30 - 44\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 45+\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nAge &lt; 18\n152\n\n\nMaternal Age - 18 to 44 years in groupings\nNA\n76\n\n\nMaternal Age - 18 to 44 years only\nAge 18 - 44\n152\n\n\nMaternal Age - 18 to 44 years only\nNA\n49\n\n\nMaternal Education\n12 yrs\n152\n\n\nMaternal Education\n&lt; 12 yrs\n152\n\n\nMaternal Education\n&gt;12 yrs\n152\n\n\nMaternal Education\nNA\n76\n\n\nMaternal Race/Ethnicity\nBlack, non-Hispanic\n152\n\n\nMaternal Race/Ethnicity\nHispanic\n152\n\n\nMaternal Race/Ethnicity\nOther non-Hispanic\n152\n\n\nMaternal Race/Ethnicity\nWhite, non-Hispanic\n152\n\n\nMaternal Race/Ethnicity\nNA\n67\n\n\nMedicaid Recipient\nMedicaid\n152\n\n\nMedicaid Recipient\nNon-Medicaid\n152\n\n\nMedicaid Recipient\nNA\n67\n\n\nMother Hispanic\nHispanic\n152\n\n\nMother Hispanic\nNon-Hispanic\n152\n\n\nMother Hispanic\nNA\n67\n\n\nNone\nNone\n152\n\n\nNumber of Previous Live Births\n0\n152\n\n\nNumber of Previous Live Births\n1 or more\n152\n\n\nNumber of Previous Live Births\nNA\n76\n\n\nOn WIC during Pregnancy\nNon-WIC\n152\n\n\nOn WIC during Pregnancy\nWIC\n152\n\n\nOn WIC during Pregnancy\nNA\n76\n\n\nPregnancy Intendedness\nIntended\n152\n\n\nPregnancy Intendedness\nUnintended\n152\n\n\nPregnancy Intendedness\nNA\n49\n\n\nSmoked 3 months before Pregnancy\nNon-Smoker\n152\n\n\nSmoked 3 months before Pregnancy\nSmoker\n152\n\n\nSmoked 3 months before Pregnancy\nNA\n76\n\n\nSmoked last 3 months of Pregnancy\nNon-Smoker\n152\n\n\nSmoked last 3 months of Pregnancy\nSmoker\n152\n\n\nSmoked last 3 months of Pregnancy\nNA\n49\n\n\n\n\n\nWe now have a sense of the structure of the entire data file.\nLet’s finally use the group_by and summarise functions to try narrow down groups to Ns of 1.\n\ndf_summary &lt;- df_clean |&gt;\n  group_by(question_id, location_abbr, subgroup_cat, subgroup, response) |&gt;\n  summarise(\n    n = n(),\n    question = first(question),\n    mean_value = mean(value)\n  )\n\nDT::datatable(df_summary)\n\n\n\n\n\nWe now know that the following data combine to form a “base” row:\n\nquestion_id\nlocation_abbr\nsubgroup_cat\nsubgroup\nresponse\n\nWith this in mind, we can now see that the following data combine to form a “base” row."
  },
  {
    "objectID": "prams_1_data_prep.html#preparing-the-data-for-visualizaiton",
    "href": "prams_1_data_prep.html#preparing-the-data-for-visualizaiton",
    "title": "Data Preparation Example",
    "section": "Preparing The Data For Visualizaiton",
    "text": "Preparing The Data For Visualizaiton\nYou’ll notice how the data is in a kind of long format: there are yes, no, and NA values for each question. We want to get all values per question per location/subgroup/subgroup_cat.\nSteps:\n\nFilter out the aggregate (non-location specific) values\nFilter out NAs on the subroup (break out) category: of course, you would need to identify WHY these are NAs and address this issue if there is a reason.\n\nWe can then use the tidyr package to pivot the data to wide format.\n\ndf_final &lt;- df_clean |&gt;\n  filter(location_abbr != \"PRAMS Total\") |&gt;\n  filter(!is.na(subgroup)) |&gt;\n  filter(response == \"YES\") |&gt;  # Only keep YES responses\n  pivot_wider(\n    id_cols = c(location_abbr, subgroup_cat, subgroup),\n    names_from = question_id,\n    values_from = value,\n    names_prefix = \"q_\"  # Add prefix to question ID columns\n  ) |&gt;\n  rename(\n    depression_within_3_months_birth = q_QUO133,\n    anxiety_within_3_months_birth = q_QUO232,\n    alcohol_use_within_3_months_birth = q_QUO8,\n    binge_drinking_within_3_months_birth = q_QUO271\n  ) |&gt; \n  arrange(location_abbr, subgroup_cat, subgroup)\n  \n\nDT::datatable(df_final)"
  },
  {
    "objectID": "prams_1_data_prep.html#data-diagnostics",
    "href": "prams_1_data_prep.html#data-diagnostics",
    "title": "Data Preparation Example",
    "section": "Data Diagnostics",
    "text": "Data Diagnostics\nLet’s do a quick analysis of missing values.\n\ndf_final |&gt; \n  summarise(\n    across(everything(), ~ 100 * sum(is.na(.)) / n())\n  ) |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation_abbr\nsubgroup_cat\nsubgroup\ndepression_within_3_months_birth\nanxiety_within_3_months_birth\nbinge_drinking_within_3_months_birth\nalcohol_use_within_3_months_birth\n\n\n\n\n0\n0\n0\n63.42062\n63.42062\n5.07365\n5.07365\n\n\n\n\n\nLet’s examine missing data across all measures, by location.\n\ndf_final |&gt;\n  group_by(location_abbr) |&gt;\n  summarise(\n    depression_missing = 100 * mean(is.na(depression_within_3_months_birth)),\n    anxiety_missing = 100 * mean(is.na(anxiety_within_3_months_birth)),\n    alcohol_missing = 100 * mean(is.na(alcohol_use_within_3_months_birth)),\n    binge_missing = 100 * mean(is.na(binge_drinking_within_3_months_birth))\n  ) |&gt;\n  arrange(location_abbr) |&gt;\n  kable(digits = 1)\n\n\n\n\n\n\n\n\n\n\n\nlocation_abbr\ndepression_missing\nanxiety_missing\nalcohol_missing\nbinge_missing\n\n\n\n\nAR\n100.0\n100.0\n4.3\n4.3\n\n\nCO\n100.0\n100.0\n2.1\n2.1\n\n\nDE\n4.3\n4.3\n4.3\n4.3\n\n\nGA\n100.0\n100.0\n2.1\n2.1\n\n\nHI\n4.3\n4.3\n4.3\n4.3\n\n\nMA\n100.0\n100.0\n4.3\n4.3\n\n\nMD\n4.3\n4.3\n4.3\n4.3\n\n\nME\n100.0\n100.0\n14.9\n14.9\n\n\nMI\n2.1\n2.1\n2.1\n2.1\n\n\nMN\n4.3\n4.3\n4.3\n4.3\n\n\nMO\n4.3\n4.3\n4.3\n4.3\n\n\nNE\n100.0\n100.0\n2.1\n2.1\n\n\nNJ\n100.0\n100.0\n6.4\n6.4\n\n\nNM\n100.0\n100.0\n4.3\n4.3\n\n\nNY\n100.0\n100.0\n4.3\n4.3\n\n\nOK\n100.0\n100.0\n2.1\n2.1\n\n\nOR\n100.0\n100.0\n2.1\n2.1\n\n\nPA\n100.0\n100.0\n4.3\n4.3\n\n\nRI\n100.0\n100.0\n6.4\n6.4\n\n\nUT\n6.4\n6.4\n6.4\n6.4\n\n\nVT\n100.0\n100.0\n12.8\n12.8\n\n\nWA\n100.0\n100.0\n4.3\n4.3\n\n\nWI\n2.1\n2.1\n2.1\n2.1\n\n\nWV\n10.6\n10.6\n10.6\n10.6\n\n\nWY\n6.4\n6.4\n6.4\n6.4\n\n\nYC\n100.0\n100.0\n6.4\n6.4\n\n\n\n\n\nData on alcohol use is more comprehensive than on anxiety and depression. Please note that just because we’re focused on data visualization does not mean our concerns over missing data are any less important than, say, when we are doing staitstical modeling."
  },
  {
    "objectID": "prams_1_data_prep.html#save-the-data",
    "href": "prams_1_data_prep.html#save-the-data",
    "title": "Data Preparation Example",
    "section": "Save The Data",
    "text": "Save The Data\nNow, let’s save this processed data to an RDS file. Before we do, let’s merge back in our full location for reference and turn our location variables into factors.\n\ndf_final &lt;- df_final |&gt;\n  left_join(\n    df_clean |&gt;\n      select(location_abbr, location) |&gt;\n      distinct(),\n    by = \"location_abbr\"\n  ) |&gt;\n  mutate(\n    location_abbr = factor(location_abbr),\n    location = factor(location)\n  )\n\nsaveRDS(df_final, here::here(\"data\", \"processed\", \"cdc_prams_df_final.rds\"))"
  },
  {
    "objectID": "prams_2_ggplot_concepts.html",
    "href": "prams_2_ggplot_concepts.html",
    "title": "ggplot2 Fundamentals Example",
    "section": "",
    "text": "# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"ggplot2\", \"forcats\", \"kableExtra\", \"readr\", \"ggtext\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(ggtext)  # Add ggtext for markdown support"
  },
  {
    "objectID": "prams_2_ggplot_concepts.html#research-question",
    "href": "prams_2_ggplot_concepts.html#research-question",
    "title": "ggplot2 Fundamentals Example",
    "section": "Research Question",
    "text": "Research Question\nWe had substantial missing data on depression/anxiety questions. For this reason, let’s focus on the relationship between location and binge drinking."
  },
  {
    "objectID": "prams_2_ggplot_concepts.html#workflow-with-ggplot2",
    "href": "prams_2_ggplot_concepts.html#workflow-with-ggplot2",
    "title": "ggplot2 Fundamentals Example",
    "section": "Workflow with ggplot2:",
    "text": "Workflow with ggplot2:\n\nStart with data\nPick an aesthetic mapping\nChoose a geometric object\nAdd statistical transformations\nAdjust finer details: scales, coordinate systems, faceting, etc.\n\nBuild this up, layer by layer."
  },
  {
    "objectID": "prams_2_ggplot_concepts.html#step-1-data",
    "href": "prams_2_ggplot_concepts.html#step-1-data",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 1: Data",
    "text": "Step 1: Data\nLet’s load the data.\n\ndf_final &lt;- readRDS(here::here(\"data\", \"processed\", \"cdc_prams_df_final.rds\"))\n\ndf_final |&gt; \n  glimpse()\n\nRows: 1,222\nColumns: 8\n$ location_abbr                        &lt;fct&gt; AR, AR, AR, AR, AR, AR, AR, AR, A…\n$ subgroup_cat                         &lt;chr&gt; \"Adequacy of Prenatal care\", \"Ade…\n$ subgroup                             &lt;chr&gt; \"ADEQUATE PNC\", \"INADEQUATE PNC\",…\n$ depression_within_3_months_birth     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ anxiety_within_3_months_birth        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ binge_drinking_within_3_months_birth &lt;dbl&gt; 24.6, 12.6, 22.2, 36.0, 21.5, 24.…\n$ alcohol_use_within_3_months_birth    &lt;dbl&gt; 53.5, 28.7, 40.9, 54.1, 45.8, 49.…\n$ location                             &lt;fct&gt; Arkansas, Arkansas, Arkansas, Ark…\n\n\nLet’s filter out missing data now to avoid warnings later.\n\ndf_binge_location &lt;- df_final |&gt;\n  filter(!is.na(binge_drinking_within_3_months_birth)) |&gt; \n  select(location_abbr, subgroup_cat, subgroup, location, binge_drinking_within_3_months_birth)\n\nThis isn’t actually enough. Remember, the data has sub groupings:\n\ndf_binge_location |&gt;\n  select(subgroup_cat, subgroup) |&gt;\n  distinct() |&gt; \n  kable()\n\n\n\n\nsubgroup_cat\nsubgroup\n\n\n\n\nAdequacy of Prenatal care\nADEQUATE PNC\n\n\nAdequacy of Prenatal care\nINADEQUATE PNC\n\n\nAdequacy of Prenatal care\nINTERMEDIATE PNC\n\n\nAdequacy of Prenatal care\nUNKNOWN PNC\n\n\nBirth Weight\nLBW (&lt;=2500g)\n\n\nBirth Weight\nNBW (&gt;2500g)\n\n\nIncome (years 2004 and beyond)\n$10,000 to $24,999\n\n\nIncome (years 2004 and beyond)\n$25,000 to $49,999\n\n\nIncome (years 2004 and beyond)\n$50,000 or more\n\n\nIncome (years 2004 and beyond)\nLess than $10,000\n\n\nMarital Status\nMARRIED\n\n\nMarital Status\nOTHER\n\n\nMaternal Age (3 Levels)\n20-29 yrs\n\n\nMaternal Age (3 Levels)\n30+ yrs\n\n\nMaternal Age (3 Levels)\n&lt;20 yrs\n\n\nMaternal Age (4 Levels)\n20-24 yrs\n\n\nMaternal Age (4 Levels)\n25-34 yrs\n\n\nMaternal Age (4 Levels)\n35+ yrs\n\n\nMaternal Age (4 Levels)\n&lt;20 yrs\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 18 - 24\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 25 - 29\n\n\nMaternal Age - 18 to 44 years in groupings\nAge 30 - 44\n\n\nMaternal Age - 18 to 44 years in groupings\nAge &lt; 18\n\n\nMaternal Age - 18 to 44 years only\nAge 18 - 44\n\n\nMaternal Education\n12 yrs\n\n\nMaternal Education\n&lt; 12 yrs\n\n\nMaternal Education\n&gt;12 yrs\n\n\nMaternal Race/Ethnicity\nBlack, non-Hispanic\n\n\nMaternal Race/Ethnicity\nHispanic\n\n\nMaternal Race/Ethnicity\nWhite, non-Hispanic\n\n\nMedicaid Recipient\nMedicaid\n\n\nMedicaid Recipient\nNon-Medicaid\n\n\nMother Hispanic\nHispanic\n\n\nMother Hispanic\nNon-Hispanic\n\n\nNone\nNone\n\n\nNumber of Previous Live Births\n0\n\n\nNumber of Previous Live Births\n1 or more\n\n\nOn WIC during Pregnancy\nNon-WIC\n\n\nOn WIC during Pregnancy\nWIC\n\n\nPregnancy Intendedness\nIntended\n\n\nPregnancy Intendedness\nUnintended\n\n\nSmoked 3 months before Pregnancy\nNon-Smoker\n\n\nSmoked 3 months before Pregnancy\nSmoker\n\n\nSmoked last 3 months of Pregnancy\nNon-Smoker\n\n\nSmoked last 3 months of Pregnancy\nSmoker\n\n\nMaternal Race/Ethnicity\nOther non-Hispanic\n\n\n\n\n\nFor now, let’s just filter on the “None” subgroup.\n\ndf_binge_location &lt;- df_binge_location |&gt;\n  filter(subgroup_cat == \"None\")"
  },
  {
    "objectID": "prams_2_ggplot_concepts.html#step-2.-aesthetic-mappings",
    "href": "prams_2_ggplot_concepts.html#step-2.-aesthetic-mappings",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 2. Aesthetic Mappings",
    "text": "Step 2. Aesthetic Mappings\n\nWhat is your X (independent) variable?\nWhat is your Y (dependent) variable?\n\nHere:\n\nx = location\ny = binge_drinking_within_3_months_birth\n\n\np1 &lt;- df_binge_location |&gt;\n    ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) \n\np1\n\n\n\n\n\n\n\n\nWe can see already we’re likely going to want to do a coordinate flip, but let’s save that for later.\nNote how we save our plot as an object, p1.\nIt’s often nice to build plots up, step by step, with p{n} where n is the step number."
  },
  {
    "objectID": "prams_2_ggplot_concepts.html#step-3.-geometric-object",
    "href": "prams_2_ggplot_concepts.html#step-3.-geometric-object",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 3. Geometric Object",
    "text": "Step 3. Geometric Object\n\nOur x is a categorical variable\nOur y is a continuous variable\n\nIt makes sense to use a bar chart.\n\np2 &lt;- p1 + geom_bar(stat = \"identity\")\n\np2"
  },
  {
    "objectID": "prams_2_ggplot_concepts.html#step-4.-statistical-transformations",
    "href": "prams_2_ggplot_concepts.html#step-4.-statistical-transformations",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 4. Statistical Transformations",
    "text": "Step 4. Statistical Transformations\n\nThe public 2011 PRAMS data we are using is aggregated at the location level.\nFor this reason, we don’t need to do any statistical transformations.\n\nYou can imagine data where we have data at the individual level, and we want to aggregate it to the location level, we’d need to use an aggregation function. I’ll show this below with simulated data."
  },
  {
    "objectID": "prams_2_ggplot_concepts.html#step-5.-adjust-fine-details",
    "href": "prams_2_ggplot_concepts.html#step-5.-adjust-fine-details",
    "title": "ggplot2 Fundamentals Example",
    "section": "Step 5. Adjust Fine Details",
    "text": "Step 5. Adjust Fine Details\n\nCoordinate Flip\nFirst, let’s flip the coordinates.\n\np3 &lt;- p2 + coord_flip()\n\np3\n\n\n\n\n\n\n\n\nThat looks a lot better.\n\n\nFixing Labels\nRight now, the labels are reverse alphabetical. Let’s fix that.\n\np4 &lt;- p3 + \n  scale_x_discrete(limits = rev(levels(df_binge_location$location)))\n\np4\n\n\n\n\n\n\n\n\nWe might also consider sorting by the rate of binge drinking.\n\np5 &lt;- df_binge_location |&gt;\n  mutate(location = fct_reorder(location, binge_drinking_within_3_months_birth)) |&gt;\n  ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()\n\np5\n\n\n\n\n\n\n\n\nAh – something of an insight!\n\n\nLabels\nOur labels are not great. We actually do not need the location names, but we need to label the x-axis and give it a title.\n\np6 &lt;- p5 +\n  labs(\n    x = NULL,\n    y = \"Percent\",\n    title = \"Binge Drinking Before Pregnancy\",\n    subtitle = \"Percentage of mothers who reported binging drinking in the 3 months before pregnancy\"\n)\n\np6\n\n\n\n\n\n\n\n\nYou can consider where to put the necessary description. Possible options:\n\ntitle\nsubtitle\ncaption\nx-axis or y-axis labels\n\nAbove, I opted for the subtitle.\n\n\nTheme\nI find the minimal theme to be a good starting point.\n\np7 &lt;- p6 +\n  theme_minimal()\n\np7\n\n\n\n\n\n\n\n\n\n\nGridlines\nHere, the horizontal gridlines are not helpful. (Remember, we used coord_flip() above, so we need to “flip” the gridlines.)\n\np8 &lt;- p7 +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\np8\n\n\n\n\n\n\n\n\nLet’s make the title bold:\n\np9 &lt;- p8 +\n  theme(plot.title = element_text(face = \"bold\"))\n\np9\n\n\n\n\n\n\n\n\n\n\nEmphasizing Certain Values\nImagine we were interested in comparing New York with everyone else. We have two New York bars.\nWe could reduce the opacity of the bars, color the New York bars, and then annotate the specific values.\nLet’s start by removing the fill.\nLet’s also reset our entire plot so we can see all the steps we took\n\np_ny &lt;- df_binge_location |&gt;\n  mutate(location = fct_reorder(location, binge_drinking_within_3_months_birth)) |&gt;\n  ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    aes(fill = location %in% c(\"New York (excluding NYC)\", \"New York City\"))\n  ) +\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"lightgray\")) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"none\" # Remove the legend since we don't need it\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percent\",\n    title = \"Binge Drinking Before Pregnancy\",\n    subtitle = \"Percentage of mothers who reported binging drinking in the 3 months before pregnancy\"\n  ) +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\np_ny\n\n\n\n\n\n\n\n\n\n\nConvert to Lollipop Chart\nI feel the gray is too overwhelming and the red is too bright. We can convert it to a so-called lollipop chart. The information is the same, but for many bars this can look cleaner.\n\np_ny2 &lt;- df_binge_location |&gt;\n    mutate(\n      location = fct_reorder(location, binge_drinking_within_3_months_birth),\n      is_ny = location %in% c(\"New York (excluding NYC)\", \"New York City\")\n    ) |&gt;\n    ggplot(aes(x = location, y = binge_drinking_within_3_months_birth)) +\n    geom_segment(aes(xend = location, yend = 0,\n                    color = is_ny),\n                linewidth = ifelse(df_binge_location$location %in% c(\"New York (excluding NYC)\", \"New York City\"), 1.5, 0.8)) +\n    geom_point(aes(color = is_ny),\n              size = ifelse(df_binge_location$location %in% c(\"New York (excluding NYC)\", \"New York City\"), 4.5, 2.2)) +\n    scale_color_manual(\n      values = c(\"TRUE\" = \"#E63946\", \"FALSE\" = \"grey\")  # Darker red\n    ) +\n    scale_x_discrete(\n      labels = function(x) ifelse(x %in% c(\"New York (excluding NYC)\", \"New York City\"), \n                                 paste0(\"**\", x, \"**\"), \n                                 x)\n    ) +\n    coord_flip() +\n    theme_minimal() +\n    theme(\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.position = \"none\",  # Remove legend\n      axis.text.y = element_markdown(size = 11)  # Use markdown for labels\n    ) +\n    labs(\n      x = NULL,\n      y = \"Percent\",\n      title = \"Binge Drinking Before Pregnancy in New York vs. Other States\",\n      subtitle = \"Percentage of mothers who reported binging drinking in the 3 months before pregnancy\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\"))\n\n\np_ny2\n\n\n\n\n\n\n\n\n\n\nAdd Text Annotations\n\np_ny3 &lt;- p_ny2 +\n  geom_text(data = df_binge_location |&gt;\n              filter(location %in% c(\"New York (excluding NYC)\", \"New York City\")) |&gt;\n              mutate(location = fct_reorder(location, binge_drinking_within_3_months_birth)),\n            aes(x = location, y = binge_drinking_within_3_months_birth,\n                label = paste0(round(binge_drinking_within_3_months_birth, 1), \"%\")),\n            hjust = -0.5,\n            vjust = 0.5,\n            size = 4,\n            fontface = \"bold\")\n\np_ny3\n\n\n\n\n\n\n\n\nThis isn’t perfect, but I think it’s a vast improvement and conveys the message well.\nWe can also force the scale to 100%; this can sometimes help viewers realize the actual rates.\n\n\nForce Scale to 100%\n\np_ny4 &lt;- p_ny3 +\n  scale_y_continuous(limits = c(0, 100))\n\np_ny4\n\n\n\n\n\n\n\n\n\n\nWhite Space Adjustments\nAnother final touch: I do not think there is enough vertical space between the title/subtitle and the lines. Likewise, the x-axis labels are a bit too close to the bars.\n\np_ny5 &lt;- p_ny4 +\n  theme(\n    plot.subtitle = element_text(margin = margin(b = 12)),\n    axis.title.x = element_text(margin = margin(t = 10))\n  )\n\np_ny5"
  },
  {
    "objectID": "prams_3_iteration_aggregation.html",
    "href": "prams_3_iteration_aggregation.html",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "",
    "text": "# Install required packages if not already installed\nrequired_packages &lt;- c(\"dplyr\", \"ggplot2\", \"forcats\", \"kableExtra\", \"readr\", \"ggtext\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(ggtext)\n\nsource(here::here(\"examples\", \"colors.R\"))"
  },
  {
    "objectID": "prams_3_iteration_aggregation.html#goal",
    "href": "prams_3_iteration_aggregation.html#goal",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Goal",
    "text": "Goal\nIn this file, we’ll show:\n\nHow to iterate on a figure to make it more informative and visually appealing.\nThe difference between using stat = \"identity\" and other statistical transformations."
  },
  {
    "objectID": "prams_3_iteration_aggregation.html#research-question",
    "href": "prams_3_iteration_aggregation.html#research-question",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Research Question",
    "text": "Research Question\nLet’s examine the relationship between depression in the 3 months before birth and the number of previous live births, focusing specifically on Wisconsin."
  },
  {
    "objectID": "prams_3_iteration_aggregation.html#data-preparation",
    "href": "prams_3_iteration_aggregation.html#data-preparation",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, let’s load and prepare our data:\n\ndf_final &lt;- readRDS(here::here(\"data\", \"processed\", \"cdc_prams_df_final.rds\"))\n\n# Filter for Wisconsin and relevant variables\ndf_wi &lt;- df_final |&gt;\n  filter(\n    location_abbr == \"WI\",\n    subgroup_cat == \"Number of Previous Live Births\"\n  ) |&gt;\n  select(\n    location_abbr,\n    subgroup,\n    depression_within_3_months_birth\n  )\n\n# Let's look at the structure of our filtered data\ndf_wi |&gt;\n  glimpse()\n\nRows: 2\nColumns: 3\n$ location_abbr                    &lt;fct&gt; WI, WI\n$ subgroup                         &lt;chr&gt; \"0\", \"1 or more\"\n$ depression_within_3_months_birth &lt;dbl&gt; 13.8, 13.0\n\n\nLet’s examine the unique values in our key variables:\n\n# Check unique values in depression variable\ndf_wi |&gt;\n  select(depression_within_3_months_birth) |&gt;\n  distinct() |&gt;\n  kable()\n\n\n\n\ndepression_within_3_months_birth\n\n\n\n\n13.8\n\n\n13.0\n\n\n\n\n# Check unique values in subgroup (number of previous live births)\ndf_wi |&gt;\n  select(subgroup) |&gt;\n  distinct() |&gt;\n  kable()\n\n\n\n\nsubgroup\n\n\n\n\n0\n\n\n1 or more\n\n\n\n\n\nNow let’s create our first visualization:\n\np1 &lt;- df_wi |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    fill = colors$blue[['100']],\n    color = colors$blue[['400']],\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"Wisconsin PRAMS Data\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\np1"
  },
  {
    "objectID": "prams_3_iteration_aggregation.html#faceting-by-small-multiples",
    "href": "prams_3_iteration_aggregation.html#faceting-by-small-multiples",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Faceting By Small Multiples",
    "text": "Faceting By Small Multiples\nA great strategy for showing patterns by groups is to facet by small multiples.\nLet’s start again with our data, not filtering by Wisconsin, but showing little bar charts for each state.\n\np2 &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    fill = colors$blue[['100']],\n    color = colors$blue[['400']],\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  facet_wrap(~ location_abbr) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"By State\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.text = element_text(face = \"bold\")\n  )\n\np2\n\nWarning: Removed 32 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\nWarning: Removed 32 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nThis a start. it has issues, though. First, we have a lot of missing states. Let’s fix that first.\n\np3 &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    fill = colors$blue[['100']],\n    color = colors$blue[['400']],\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  facet_wrap(~ location_abbr) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"By State\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.text = element_text(face = \"bold\")\n  )\n\np3\n\n\n\n\n\n\n\n\nThis is an improvement, but we see some issues. First, our y-axis is too short, and annotations are getting cut off. We can fix our y axis to go to 20\n\np4 &lt;- p3 +\n  scale_y_continuous(limits = c(0, 20))\n\np4\n\n\n\n\n\n\n\n\nThat’s better. Some stylistic adjustments:\n\nNeed more space between the labels and the plot:\n\n\np5 &lt;- p4 +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\np5\n\n\n\n\n\n\n\n\nNow, let’s sort it by the difference between the birth. This requires data transformation, so we’ll start from scratch.\n\n# Calculate differences and sort states\nstate_diffs &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  group_by(location_abbr) |&gt;\n  summarize(\n    first_birth = depression_within_3_months_birth[subgroup == \"0\"],\n    later_births = mean(depression_within_3_months_birth[subgroup != \"0\"]),\n    diff = first_birth - later_births\n  ) |&gt;\n  arrange(desc(diff))\n\n# Create ordered factor for states\nstate_order &lt;- state_diffs$location_abbr\n\n# Create plot with sorted states\np6 &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  mutate(location_abbr = factor(location_abbr, levels = state_order)) |&gt;\n  left_join(\n    state_diffs |&gt; select(location_abbr, diff),\n    by = \"location_abbr\"\n  ) |&gt;\n  ggplot(aes(x = subgroup, y = depression_within_3_months_birth)) +\n  geom_bar(\n    stat = \"identity\",\n    aes(fill = diff &gt; 0, color = diff &gt; 0),\n    linewidth = 0.5,\n    linetype = \"solid\"\n  ) +\n  scale_fill_manual(\n    name = \"Difference\",\n    values = c(\n      \"TRUE\" = colors$blue[['100']],\n      \"FALSE\" = colors$red[['100']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_color_manual(\n    name = \"Difference\",\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", depression_within_3_months_birth)),\n    vjust = -1.2,\n    size = 3.5\n  ) +\n  facet_wrap(~ location_abbr) +\n  scale_y_continuous(limits = c(0, 20)) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    subtitle = \"States ordered by difference between first birth and subsequent births\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.text = element_text(face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10)),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np6\n\n\n\n\n\n\n\n\nThis is interesting, but I wonder if we’re using the right graph.\nIf we’re interested in the difference, we can do a forest plot by state in a single plot. Let’s start with a basic version.\n\n# Calculate differences for forest plot\nforest_data &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  group_by(location_abbr) |&gt;\n  summarize(\n    first_birth = depression_within_3_months_birth[subgroup == \"0\"],\n    later_births = mean(depression_within_3_months_birth[subgroup != \"0\"]),\n    diff = first_birth - later_births\n  ) |&gt;\n  arrange(diff)  # Sort by difference\n\n# Create ordered factor for states\nstate_order &lt;- forest_data$location_abbr\n\n# Create forest plot\np7 &lt;- forest_data |&gt;\n  mutate(location_abbr = factor(location_abbr, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location_abbr)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location_abbr,\n      yend = location_abbr,\n      color = diff &gt; 0\n    ),\n    linewidth = 1\n  ) +\n  scale_color_manual(\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np7\n\n\n\n\n\n\n\n\nThis is a decent start. Let’s add text annotations to the plot. Let’s also use the location rather than the abbrevaition.\n\n# Calculate differences for forest plot\nforest_data &lt;- df_final |&gt;\n  filter(subgroup_cat == \"Number of Previous Live Births\") |&gt;\n  filter(!is.na(depression_within_3_months_birth)) |&gt;\n  group_by(location_abbr, location) |&gt;\n  summarize(\n    first_birth = depression_within_3_months_birth[subgroup == \"0\"],\n    later_births = mean(depression_within_3_months_birth[subgroup != \"0\"]),\n    diff = first_birth - later_births,\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(diff)  # Sort by difference\n\n# Create ordered factor for states\nstate_order &lt;- forest_data$location\n\n# Create forest plot\np8 &lt;- forest_data |&gt;\n  mutate(location = factor(location, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location,\n      yend = location,\n      color = diff &gt; 0\n    ),\n    linewidth = 1\n  ) +\n  geom_text(\n    aes(\n      x = diff,\n      label = sprintf(\"%+.1f%%\", diff)\n    ),\n    hjust = ifelse(forest_data$diff &gt; 0, -0.3, 1.3),\n    size = 3.5,\n    fontface = \"bold\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np8\n\n\n\n\n\n\n\n\nThe spacing is a little wide between the annotations. Let’s also make them bold.\n\np9 &lt;- forest_data |&gt;\n  mutate(location = factor(location, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location,\n      yend = location,\n      color = diff &gt; 0\n    ),\n    linewidth = 1\n  ) +\n  geom_text(\n    aes(\n      x = diff,\n      label = sprintf(\"%+.1f%%\", diff)\n    ),\n    hjust = ifelse(forest_data$diff &gt; 0, -0.3, 1.3),\n    size = 3.5,\n    fontface = \"bold\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"TRUE\" = colors$blue[['400']],\n      \"FALSE\" = colors$red[['400']]\n    ),\n    labels = c(\n      \"TRUE\" = \"Depression Improves With Extra Births\",\n      \"FALSE\" = \"Depression Worsens With Extra Births\"\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\np9\n\n\n\n\n\n\n\n\nLet’s step back and think.\n\nThe subtitle does the work of the caption. We can remove it and simplify the code.\nThe bold is too much. Let’s remove it.\nWe need more top margin on our y axis.\n\n\np10 &lt;- forest_data |&gt;\n  mutate(location = factor(location, levels = state_order)) |&gt;\n  ggplot(aes(x = diff, y = location)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(\n    aes(\n      x = 0,\n      xend = diff,\n      y = location,\n      yend = location,\n      color = ifelse(diff &gt; 0, \"positive\", \"negative\")\n    ),\n    linewidth = 1\n  ) +\n  geom_text(\n    aes(\n      x = diff,\n      label = sprintf(\"%+.1f%%\", diff)\n    ),\n    hjust = ifelse(forest_data$diff &gt; 0, -0.3, 1.3),\n    size = 3.5\n  ) +\n  scale_color_manual(\n    values = c(\n      \"positive\" = colors$blue[['400']],\n      \"negative\" = colors$red[['400']]\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(min(forest_data$diff) - 2, max(forest_data$diff) + 2),\n    breaks = seq(-10, 10, 2)\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Difference in Depression Rates: First Birth vs. Subsequent Births\",\n    subtitle = \"Negative values indicate higher depression rates for second births\",\n    x = \"Difference in Depression Rate (First Birth - Subsequent Births)\",\n    y = NULL\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"none\",\n    axis.text.y = element_text(margin = margin(r = 10))\n  )\n\np10"
  },
  {
    "objectID": "prams_3_iteration_aggregation.html#thinking-about-aggregations-transformations",
    "href": "prams_3_iteration_aggregation.html#thinking-about-aggregations-transformations",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Thinking About Aggregations & Transformations",
    "text": "Thinking About Aggregations & Transformations\nLet’s create a simulated dataset to illustrate how we were using stat = \"identity\" above. We’ll create individual-level data that mimics our PRAMS data structure.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create simulated data\nsim_data &lt;- tibble(\n  state = rep(state.name, each = 100)  # 100 people per state\n) |&gt;\n  mutate(\n    person_id = row_number(),  # Generate unique IDs\n    # Generate number of previous births using Poisson distribution (mean = 1)\n    previous_births_count = rpois(n(), lambda = 1),\n    # Create factor for 0 vs 1+ births\n    previous_births = factor(\n      ifelse(previous_births_count == 0, \"0\", \"1+\"),\n      levels = c(\"0\", \"1+\")\n    ),\n\n    # Generate depression status with different probabilities based on previous births\n    # Higher probability of depression for first births (0) than subsequent births (1+)\n    depression_prob = ifelse(previous_births == \"0\", 0.15, 0.10),  # 15% vs 10% base rates\n    depression = rbinom(n(), 1, depression_prob) == 1  # Convert to TRUE/FALSE\n  ) |&gt;\n  select(-depression_prob)\n\n# Let's look at the structure\nsim_data |&gt;\n  glimpse()\n\nRows: 5,000\nColumns: 5\n$ state                 &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala…\n$ person_id             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ previous_births_count &lt;int&gt; 0, 2, 1, 2, 3, 0, 1, 2, 1, 1, 3, 1, 1, 1, 0, 2, …\n$ previous_births       &lt;fct&gt; 0, 1+, 1+, 1+, 1+, 0, 1+, 1+, 1+, 1+, 1+, 1+, 1+…\n$ depression            &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n\nsim_data |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nstate\nperson_id\nprevious_births_count\nprevious_births\ndepression\n\n\n\n\nAlabama\n1\n0\n0\nFALSE\n\n\nAlabama\n2\n2\n1+\nTRUE\n\n\nAlabama\n3\n1\n1+\nFALSE\n\n\nAlabama\n4\n2\n1+\nFALSE\n\n\nAlabama\n5\n3\n1+\nFALSE\n\n\nAlabama\n6\n0\n0\nFALSE\n\n\n\n\n# Quick summary to verify our data\nsim_data |&gt;\n  group_by(state, previous_births) |&gt;\n  summarize(\n    n = n(),\n    depression_rate = mean(depression) * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\nstate\nprevious_births\nn\ndepression_rate\n\n\n\n\nAlabama\n0\n35\n20.000000\n\n\nAlabama\n1+\n65\n12.307692\n\n\nAlaska\n0\n33\n18.181818\n\n\nAlaska\n1+\n67\n5.970149\n\n\nArizona\n0\n39\n15.384615\n\n\nArizona\n1+\n61\n6.557377"
  },
  {
    "objectID": "prams_3_iteration_aggregation.html#recreating-visualizations-with-simulated-data",
    "href": "prams_3_iteration_aggregation.html#recreating-visualizations-with-simulated-data",
    "title": "PRAMS Data Analysis: Iteration, Aggregation, & Transformation",
    "section": "Recreating Visualizations with Simulated Data",
    "text": "Recreating Visualizations with Simulated Data\nLet’s recreate our visualizations using the simulated data, starting with Wisconsin.\nBelow we are using the stat = \"summary\" to aggregate the data and the scales functionality to format the y axis as a percentage.\n\n# Filter for Wisconsin\nwi_data &lt;- sim_data |&gt;\n  filter(state == \"Wisconsin\") |&gt;\n  mutate(depression = as.numeric(depression))\n\n# Approach 1: Using scales::percent_format()\np1_sim &lt;- wi_data |&gt;\n  ggplot(aes(x = previous_births, y = depression)) +\n  geom_bar(\n    stat = \"summary\",  # We let ggplot2 do the aggregation for us\n    fun = \"mean\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  )\n\np1_sim\n\n\n\n\n\n\n\n\nAn alternative approach is – similar to above – is to do the aggregation ourselves and use identity.\n\n# Approach 2: Multiplying by 100 in the aggregation\np1_sim_self_aggregate &lt;- wi_data |&gt;\n  group_by(previous_births) |&gt;\n  summarize(\n    depression_rate = mean(depression) * 100,  # Convert to percentage\n    .groups = \"drop\"\n  ) |&gt;\n  ggplot(aes(x = previous_births, y = depression_rate)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(\n    title = \"Depression Before Birth by Number of Previous Live Births\",\n    x = \"Number of Previous Live Births\",\n    y = \"Percentage Reporting Depression\"\n  )\n\n# Display both plots\n\np1_sim_self_aggregate\n\n\n\n\n\n\n\n\nLife tends to be easier when you summarize the data yourself. I find it more explicit and it avoids confusion from trying to figure out exactly what ggplot2 is doing."
  },
  {
    "objectID": "saving_visualizations.html",
    "href": "saving_visualizations.html",
    "title": "Application 7: Saving Visualizations",
    "section": "",
    "text": "There are several ways to save visualizations in R, each with its own advantages. Here we’ll cover the most common approaches.\n\n\nThe ggsave() function is the most straightforward way to save ggplot2 visualizations. It automatically detects the file type from the extension and saves with appropriate settings.\n\n# Basic usage - saves last plot\nggsave(\"figures/correlation_plot.png\", width = 10, height = 8, dpi = 300)\n\n# Save a specific plot\np &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggsave(\"figures/custom_plot.png\", p, width = 10, height = 8, dpi = 300)\n\n# Save with different formats\nggsave(\"figures/plot.pdf\", p, width = 10, height = 8)  # PDF\nggsave(\"figures/plot.svg\", p, width = 10, height = 8)  # SVG\nggsave(\"figures/plot.jpg\", p, width = 10, height = 8, quality = 0.9)  # JPEG\n\n\n\n\nFor base R graphics, you can use functions like png(), pdf(), jpeg(), etc.:\n\n# Save a base R plot\npng(\"figures/base_plot.png\", width = 1000, height = 800, res = 100)\nplot(1:10, 1:10)\ndev.off()\n\n# Save multiple plots to PDF\npdf(\"figures/multiple_plots.pdf\", width = 10, height = 8)\nplot(1:10, 1:10)\nplot(1:10, 10:1)\ndev.off()\n\n\n\n\nThe Cairo package provides high-quality graphics with better font rendering:\n\n# Install and load Cairo\nif (!require(Cairo)) install.packages(\"Cairo\")\nlibrary(Cairo)\n\n# Save with Cairo\nCairoPNG(\"figures/cairo_plot.png\", width = 1000, height = 800, dpi = 100)\nplot(1:10, 1:10)\ndev.off()\n\n\n\n\nAbove all, consider your audience. For example, read the submission guidelines for the academic journal you are targeting. (It’s worth also considering the audience before you overpolish a figure!)\nHere are some best practices for saving visuals:\n\nResolution and Size:\n\nFor web: 72-96 DPI\nFor print: 300-600 DPI\nFor presentations: 150-200 DPI\n\nFile Formats:\n\nPNG: Best for web, supports transparency\nPDF: Best for print, scalable\nSVG (vector graphics): Best for web, when it works\nJPEG: Best for photographs, smaller file size\n\n\n\n\n\nTo save multiple plots efficiently:\n\n# Save multiple plots to a single PDF\npdf(\"figures/all_plots.pdf\", width = 10, height = 8)\nprint(p)  # First plot\nprint(p + theme_dark())  # Second plot\ndev.off()\n\n# Save multiple plots to separate files\nplots &lt;- list(p1 = p, p2 = p + theme_dark())\nfor (i in seq_along(plots)) {\n  ggsave(\n    sprintf(\"figures/plot_%d.png\", i),\n    plots[[i]],\n    width = 10,\n    height = 8,\n    dpi = 300\n  )\n}"
  },
  {
    "objectID": "saving_visualizations.html#saving-visualizations",
    "href": "saving_visualizations.html#saving-visualizations",
    "title": "Application 7: Saving Visualizations",
    "section": "",
    "text": "There are several ways to save visualizations in R, each with its own advantages. Here we’ll cover the most common approaches.\n\n\nThe ggsave() function is the most straightforward way to save ggplot2 visualizations. It automatically detects the file type from the extension and saves with appropriate settings.\n\n# Basic usage - saves last plot\nggsave(\"figures/correlation_plot.png\", width = 10, height = 8, dpi = 300)\n\n# Save a specific plot\np &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggsave(\"figures/custom_plot.png\", p, width = 10, height = 8, dpi = 300)\n\n# Save with different formats\nggsave(\"figures/plot.pdf\", p, width = 10, height = 8)  # PDF\nggsave(\"figures/plot.svg\", p, width = 10, height = 8)  # SVG\nggsave(\"figures/plot.jpg\", p, width = 10, height = 8, quality = 0.9)  # JPEG\n\n\n\n\nFor base R graphics, you can use functions like png(), pdf(), jpeg(), etc.:\n\n# Save a base R plot\npng(\"figures/base_plot.png\", width = 1000, height = 800, res = 100)\nplot(1:10, 1:10)\ndev.off()\n\n# Save multiple plots to PDF\npdf(\"figures/multiple_plots.pdf\", width = 10, height = 8)\nplot(1:10, 1:10)\nplot(1:10, 10:1)\ndev.off()\n\n\n\n\nThe Cairo package provides high-quality graphics with better font rendering:\n\n# Install and load Cairo\nif (!require(Cairo)) install.packages(\"Cairo\")\nlibrary(Cairo)\n\n# Save with Cairo\nCairoPNG(\"figures/cairo_plot.png\", width = 1000, height = 800, dpi = 100)\nplot(1:10, 1:10)\ndev.off()\n\n\n\n\nAbove all, consider your audience. For example, read the submission guidelines for the academic journal you are targeting. (It’s worth also considering the audience before you overpolish a figure!)\nHere are some best practices for saving visuals:\n\nResolution and Size:\n\nFor web: 72-96 DPI\nFor print: 300-600 DPI\nFor presentations: 150-200 DPI\n\nFile Formats:\n\nPNG: Best for web, supports transparency\nPDF: Best for print, scalable\nSVG (vector graphics): Best for web, when it works\nJPEG: Best for photographs, smaller file size\n\n\n\n\n\nTo save multiple plots efficiently:\n\n# Save multiple plots to a single PDF\npdf(\"figures/all_plots.pdf\", width = 10, height = 8)\nprint(p)  # First plot\nprint(p + theme_dark())  # Second plot\ndev.off()\n\n# Save multiple plots to separate files\nplots &lt;- list(p1 = p, p2 = p + theme_dark())\nfor (i in seq_along(plots)) {\n  ggsave(\n    sprintf(\"figures/plot_%d.png\", i),\n    plots[[i]],\n    width = 10,\n    height = 8,\n    dpi = 300\n  )\n}"
  }
]